<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xml" href="https://jonnyzzz.com/feed.xslt.xml"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="https://jonnyzzz.com/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://jonnyzzz.com/" rel="alternate" type="text/html" />
  <updated>2026-02-06T10:57:51+00:00</updated>
  <id>/</id>

  
  <title type="html">Eugene Petrenko</title>
  

  
  <subtitle>Founding Engineering Leader | Agentic AI DevTools &amp; Experience</subtitle>
  

  

  
  
  <entry>
    <title type="html">Git Fork Pattern: Full Checkouts Without the Bloat</title>
    <link href="https://jonnyzzz.com/blog/2026/02/02/git-fork-pattern/" rel="alternate" type="text/html" title="Git Fork Pattern: Full Checkouts Without the Bloat" />
    <published>2026-02-02T00:00:00+00:00</published>
    <updated>2026-02-02T00:00:00+00:00</updated>
    <id>/blog/2026/02/02/git-fork-pattern</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/02/02/git-fork-pattern/">&lt;p&gt;I’ve been working with multiple AI agents on the same codebase, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git worktree&lt;/code&gt; kept getting in the way.
The same branch can’t be checked out in multiple worktrees. Some git operations don’t work well with worktrees.
&lt;strong&gt;So I stopped using worktrees entirely.&lt;/strong&gt; Instead, I’m using full git checkouts that share objects with the
original repository. &lt;strong&gt;Full git functionality, minimal disk usage.&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;the-problem-with-git-worktree&quot;&gt;The Problem with Git Worktree&lt;/h2&gt;

&lt;p&gt;Git worktree is useful for quick parallel work, but it has limitations that become painful when you’re
orchestrating multiple AI agents or just need full git flexibility:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Can’t checkout the same branch twice&lt;/strong&gt; - Want two agents working on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main&lt;/code&gt;? Can’t do it with worktrees.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Limited git operations&lt;/strong&gt; - Some rebase operations, submodules, and other features behave differently.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tool confusion&lt;/strong&gt; - IDEs and git tools sometimes don’t handle worktrees well.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tied to main repo&lt;/strong&gt; - Worktrees are dependent on the main repository structure.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When you’re running &lt;a href=&quot;https://jonnyzzz.com/MULTI-AGENT.md&quot;&gt;multi-agent workflows&lt;/a&gt; or just need multiple
independent checkouts, these constraints become blockers.&lt;/p&gt;

&lt;h2 id=&quot;the-solution-git-alternates&quot;&gt;The Solution: Git Alternates&lt;/h2&gt;

&lt;p&gt;Git has a lesser-known feature called &lt;strong&gt;alternates&lt;/strong&gt; - you can tell one repository to use another
repository’s object database. This means:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create as many full checkouts as you want&lt;/li&gt;
  &lt;li&gt;Each checkout is a complete, independent git repository&lt;/li&gt;
  &lt;li&gt;Objects (commits, trees, blobs) are shared via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.git/objects/info/alternates&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;New commits are stored locally, existing objects are read from the source&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Full git functionality, no worktree limitations&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I tested this approach with our 25-year-old monorepo, and it works perfectly. Multiple agents can now
work independently, each in their own full checkout, sharing the same object database.&lt;/p&gt;

&lt;h2 id=&quot;how-it-works&quot;&gt;How It Works&lt;/h2&gt;

&lt;p&gt;The implementation is straightforward. Here’s what happens:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Original repository&lt;/span&gt;
~/projects/myrepo/.git/objects  &lt;span class=&quot;c&quot;&gt;# Contains all git objects&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# New fork&lt;/span&gt;
~/projects/myrepo-fork/.git/objects/info/alternates
&lt;span class=&quot;c&quot;&gt;# Contains: /Users/username/projects/myrepo/.git/objects&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Result:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# - Fork reads objects from original&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# - Fork writes new objects locally&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# - Both are independent repositories&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When git needs an object, it checks the local &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.git/objects&lt;/code&gt; first, then checks the paths listed
in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alternates&lt;/code&gt; file. It’s transparent and works with all git operations.&lt;/p&gt;

&lt;h2 id=&quot;the-implementation&quot;&gt;The Implementation&lt;/h2&gt;

&lt;p&gt;The implementation is just 5 steps:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 1. Create and initialize new repository&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /path/to/fork &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /path/to/fork &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; git init

&lt;span class=&quot;c&quot;&gt;# 2. Set up object sharing (use absolute path)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; .git/objects/info
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;/absolute/path/to/source/.git/objects&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; .git/objects/info/alternates

&lt;span class=&quot;c&quot;&gt;# 3. Copy git config (inherits remotes)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; /absolute/path/to/source/.git/config .git/config

&lt;span class=&quot;c&quot;&gt;# 4. Add parent remote and fetch&lt;/span&gt;
git remote add parent /absolute/path/to/source
git fetch parent

&lt;span class=&quot;c&quot;&gt;# 5. Checkout branch (track from parent)&lt;/span&gt;
git checkout &lt;span class=&quot;nt&quot;&gt;-b&lt;/span&gt; main parent/main
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Done. You now have &lt;strong&gt;two remotes&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;origin&lt;/code&gt; (from config) pointing to the real remote, and
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parent&lt;/code&gt; pointing to your local source repository.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;https://jonnyzzz.com/GIT-FORK.md&quot;&gt;GIT-FORK.md&lt;/a&gt; for complete documentation.&lt;/p&gt;

&lt;h2 id=&quot;two-remotes-pattern&quot;&gt;Two Remotes Pattern&lt;/h2&gt;

&lt;p&gt;When you copy &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.git/config&lt;/code&gt;, you inherit the original repository’s remotes. Then you add a second
remote for the local parent:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;origin&lt;/strong&gt; (from config) → Real remote (GitHub, etc.) - push, pull, create PRs&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;parent&lt;/strong&gt; (added manually) → Local source repo - sync uncommitted changes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is powerful for multi-agent workflows:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Working in fork&lt;/span&gt;
git commit &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;feature work&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Get latest from parent (before it&apos;s pushed anywhere)&lt;/span&gt;
git fetch parent
git merge parent/main

&lt;span class=&quot;c&quot;&gt;# Push to real remote&lt;/span&gt;
git push origin feature-branch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Agents can share work through the parent repository, then independently push to origin when ready.&lt;/p&gt;

&lt;h2 id=&quot;disk-usage&quot;&gt;Disk Usage&lt;/h2&gt;

&lt;p&gt;Actual numbers from testing:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Original repo:     150 MB
Normal clone:      150 MB
Git fork:          ~5 MB (only new commits)
Git worktree:      ~5 MB (but with limitations)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The git fork gives you the disk efficiency of worktrees &lt;strong&gt;plus&lt;/strong&gt; the full functionality of a
complete repository, &lt;strong&gt;plus&lt;/strong&gt; access to both local and remote changes.&lt;/p&gt;

&lt;h2 id=&quot;ai-agent-integration&quot;&gt;AI Agent Integration&lt;/h2&gt;

&lt;p&gt;This pattern works exceptionally well with AI agents. When I orchestrate multiple agents on the
same codebase, I just tell them:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Agent, fork my git repository as suggested in GIT-FORK.md
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Agents understand this pattern and implement the 5 steps automatically. Each agent gets its own
fork with:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Full git access - create branches, rebase, merge, everything works&lt;/li&gt;
  &lt;li&gt;Shared objects - no disk space wasted&lt;/li&gt;
  &lt;li&gt;True independence - complete repository, no worktree constraints&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I added &lt;a href=&quot;https://jonnyzzz.com/GIT-FORK.md&quot;&gt;GIT-FORK.md&lt;/a&gt; to my skill files alongside
&lt;a href=&quot;https://jonnyzzz.com/RLM.md&quot;&gt;RLM.md&lt;/a&gt; and &lt;a href=&quot;https://jonnyzzz.com/MULTI-AGENT.md&quot;&gt;MULTI-AGENT.md&lt;/a&gt;.
Agents read it and implement the pattern without additional instructions.&lt;/p&gt;

&lt;h2 id=&quot;testing-and-validation&quot;&gt;Testing and Validation&lt;/h2&gt;

&lt;p&gt;I tested this extensively:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Multiple forks&lt;/strong&gt; - Created 5+ forks from the same source, all working independently&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;All git operations&lt;/strong&gt; - Commit, branch, merge, rebase, cherry-pick - everything works&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Object sharing&lt;/strong&gt; - Verified that shared objects aren’t duplicated (checked with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;du -sh .git/objects&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tool compatibility&lt;/strong&gt; - IntelliJ, VSCode, git CLI, git GUI tools - all work normally&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;25-year-old monorepo&lt;/strong&gt; - Works with large, complex repositories&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The key insight: &lt;strong&gt;this is just a normal git repository with an optimization&lt;/strong&gt;. Tools don’t need
to know about alternates, they just work.&lt;/p&gt;

&lt;h2 id=&quot;important-notes&quot;&gt;Important Notes&lt;/h2&gt;

&lt;p&gt;A few things to keep in mind:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Absolute paths&lt;/strong&gt; - The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alternates&lt;/code&gt; file must contain absolute paths, not relative paths&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Source availability&lt;/strong&gt; - The source repository must remain available. If you delete it, forks lose shared objects&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Two remotes&lt;/strong&gt; - Copying config gives you &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;origin&lt;/code&gt;, then you add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parent&lt;/code&gt; manually&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Track parent&lt;/strong&gt; - Checkout from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parent/branch&lt;/code&gt;, not &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;origin/branch&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Garbage collection&lt;/strong&gt; - Each repository runs &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git gc&lt;/code&gt; independently, which is usually fine&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Not for distribution&lt;/strong&gt; - This is for local development, not for sharing repos with others&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The two-remote pattern means you can:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git fetch parent      &lt;span class=&quot;c&quot;&gt;# Get local uncommitted changes&lt;/span&gt;
git fetch origin      &lt;span class=&quot;c&quot;&gt;# Get pushed changes from team&lt;/span&gt;
git push origin main  &lt;span class=&quot;c&quot;&gt;# Push your work to team&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;the-pattern-in-practice&quot;&gt;The Pattern in Practice&lt;/h2&gt;

&lt;p&gt;Here’s my typical workflow now:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Main repository&lt;/strong&gt; - My primary working copy at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/projects/myrepo&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tell agent&lt;/strong&gt; - “Fork my git as suggested in GIT-FORK.md”&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Agent works&lt;/strong&gt; - Creates fork, does work, full git capabilities&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Cleanup&lt;/strong&gt; - Delete fork when done, no impact on main repository&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Some forks last minutes, some last days. The flexibility is what matters - forks are cheap (5MB),
fully functional, and agents know how to create them.&lt;/p&gt;

&lt;h2 id=&quot;why-this-matters&quot;&gt;Why This Matters&lt;/h2&gt;

&lt;p&gt;The git fork pattern enables true parallel development with AI agents. Instead of carefully
orchestrating which agent works where and managing worktree constraints, I just:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Spin up agent with its own fork&lt;/li&gt;
  &lt;li&gt;Agent works independently&lt;/li&gt;
  &lt;li&gt;Agent completes task&lt;/li&gt;
  &lt;li&gt;Results are integrated back&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The pattern scales to any number of agents. I’ve had 5+ agents working simultaneously, each in
their own fork, no conflicts, no limitations.&lt;/p&gt;

&lt;p&gt;For solo development, this is useful too. Want to try a risky refactoring without branching? Fork
it. Want to test something while keeping your main checkout clean? Fork it. Each fork is cheap and
fully functional.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;try-it-yourself&quot;&gt;Try It Yourself&lt;/h2&gt;

&lt;p&gt;The complete pattern is in &lt;a href=&quot;https://jonnyzzz.com/GIT-FORK.md&quot;&gt;GIT-FORK.md&lt;/a&gt; - just 5 steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create and initialize fork&lt;/li&gt;
  &lt;li&gt;Set up alternates file (absolute path)&lt;/li&gt;
  &lt;li&gt;Copy git config&lt;/li&gt;
  &lt;li&gt;Add remote and fetch&lt;/li&gt;
  &lt;li&gt;Checkout branch&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you’re working with AI agents, just reference GIT-FORK.md and they’ll implement it. If you’re
doing it manually, the commands are straightforward - see the doc for exact syntax.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next&lt;/h2&gt;

&lt;p&gt;I’m now exploring how to chain forks - creating a fork from a fork. This could enable interesting
multi-level agent hierarchies where parent agents spawn child agents, each with their own workspace.&lt;/p&gt;

&lt;p&gt;The git alternates mechanism has been in git for years, but it’s not widely used. With AI agents
becoming primary users of development tools, patterns like this become more important. &lt;strong&gt;Tools must
now be optimized for agentic consumption.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you’re orchestrating AI agents or just want more flexibility than git worktree provides, try the
git fork pattern. It’s what I use every day now.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Questions? Experiments? Let me know how the git fork pattern works for you.&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/jonnyzzz/&quot;&gt;Follow me on LinkedIn&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/jonnyzzz&quot;&gt;Follow me on X (Twitter)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jonnyzzz.com/ai/&quot;&gt;Check out more AI agent patterns&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="git" />
  
    <category term="dev-tools" />
  
    <category term="ai-agents" />
  
    <category term="workflow" />
  
    <summary type="html">Why I stopped using git worktree and switched to full checkouts with shared objects</summary>
  
  </entry>
  
  <entry>
    <title type="html">Russian Loto with AI Celebrity Voices</title>
    <link href="https://jonnyzzz.com/blog/2026/02/01/russian-loto-ai-voices/" rel="alternate" type="text/html" title="Russian Loto with AI Celebrity Voices" />
    <published>2026-02-01T00:00:00+00:00</published>
    <updated>2026-02-01T00:00:00+00:00</updated>
    <id>/blog/2026/02/01/russian-loto-ai-voices</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/02/01/russian-loto-ai-voices/">&lt;p&gt;Hey Siri, can you count from 1 to 90 in random order? Siri could not. But Russian Loto absolutely
needs it: a caller shuffles 1-90 and announces each number as people mark their tickets.&lt;/p&gt;

&lt;p&gt;If you never played Loto/Bingo, this explainer shows the call-and-mark loop:
&lt;a href=&quot;https://www.youtube.com/watch?v=H1tQs2vXnQg&quot;&gt;Learn Numbers Playing Lotto (Bingo) (Games in Russian for complete beginners)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I challenged Claude to build the game in 10 minutes. Then, in the next 20 minutes, I had Qwen TTS
running locally on my Mac with AI celebrity voices. It was fast to invent, and it turned into
great family fun.&lt;/p&gt;

&lt;p&gt;The result: &lt;a href=&quot;https://github.com/jonnyzzz/jonnyzzz-loto&quot;&gt;jonnyzzz-loto&lt;/a&gt; — a Russian Loto game where
12 AI-generated celebrity voices announce numbers with character-specific jokes.&lt;/p&gt;

&lt;h2 id=&quot;the-problem-with-real-loto&quot;&gt;The Problem with Real Loto&lt;/h2&gt;

&lt;p&gt;Family gatherings with Russian Loto have a problem: the person calling numbers gets bored. They
read monotonously. The game becomes a chore. What if the announcer was actually entertaining?&lt;/p&gt;

&lt;p&gt;The obvious solution: record celebrity-like voice samples and use voice cloning. But that requires
finding clean audio samples, dealing with licensing concerns, and building a pipeline for each
celebrity. Too much work for a weekend prototype.&lt;/p&gt;

&lt;h2 id=&quot;qwen3-tts-and-voicedesign&quot;&gt;Qwen3-TTS and VoiceDesign&lt;/h2&gt;

&lt;p&gt;Alibaba’s &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-Omni-3B&quot;&gt;Qwen3-TTS&lt;/a&gt; changed everything. This
open-source model (Apache 2.0) has a fascinating capability called VoiceDesign — instead of
cloning a voice from audio samples, you describe the voice in text and the model generates it.
That is exactly what rapid prototyping needs: a prompt, a quick run, a new character.&lt;/p&gt;

&lt;p&gt;No samples needed. Just a text description like:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“A deep elderly male Russian voice, slow deliberate speech with long pauses, authoritative
Soviet leader tone, slight speech impediment”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And outcomes something that sounds like Brezhnev. Not a clone — but a convincing character voice
that captures the essence.&lt;/p&gt;

&lt;h2 id=&quot;the-tech-stack&quot;&gt;The Tech Stack&lt;/h2&gt;

&lt;p&gt;The project runs entirely locally on Mac with Apple Silicon, optimized for short iteration loops:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Setup (creates venv, installs dependencies)&lt;/span&gt;
./setup_qwen_mac.sh

&lt;span class=&quot;c&quot;&gt;# Run with AI voices&lt;/span&gt;
uv run python loto.py &lt;span class=&quot;nt&quot;&gt;--qwen&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Stack:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Python with &lt;a href=&quot;https://github.com/astral-sh/uv&quot;&gt;uv&lt;/a&gt; package manager&lt;/li&gt;
  &lt;li&gt;Qwen3-TTS for speech synthesis&lt;/li&gt;
  &lt;li&gt;MPS (Metal Performance Shaders) for GPU acceleration on Apple Silicon&lt;/li&gt;
  &lt;li&gt;pygame for audio playback&lt;/li&gt;
  &lt;li&gt;VoiceDesign model for text-to-voice-description synthesis&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The setup script handles PyTorch with MPS support, the transformers library, and audio
dependencies. First run downloads the model (~3GB).&lt;/p&gt;

&lt;h2 id=&quot;12-celebrity-voices-random-pick-claude-generated&quot;&gt;12 Celebrity Voices Random Pick (Claude-Generated)&lt;/h2&gt;

&lt;p&gt;Each character has a unique voice description and personality:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Character&lt;/th&gt;
      &lt;th&gt;Voice Description&lt;/th&gt;
      &lt;th&gt;Style&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Leonid Brezhnev&lt;/td&gt;
      &lt;td&gt;Deep elderly male, slow deliberate speech, Soviet gravitas&lt;/td&gt;
      &lt;td&gt;Turns everything into five-year plan references&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Maxim Galkin&lt;/td&gt;
      &lt;td&gt;Theatrical, energetic showman, perfect diction&lt;/td&gt;
      &lt;td&gt;Game show host enthusiasm&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Alla Pugacheva&lt;/td&gt;
      &lt;td&gt;Mature female, pop diva, dramatic pauses&lt;/td&gt;
      &lt;td&gt;Primadonna drama for every number&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Vinni-Puh (Evgeny Leonov)&lt;/td&gt;
      &lt;td&gt;Warm, friendly, childlike wonder&lt;/td&gt;
      &lt;td&gt;Honey and friendship references&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Vladimir Zhirinovsky&lt;/td&gt;
      &lt;td&gt;Passionate, loud, political fervor&lt;/td&gt;
      &lt;td&gt;Makes everything a political statement&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Monetochka&lt;/td&gt;
      &lt;td&gt;Young female, indie pop, Gen-Z slang&lt;/td&gt;
      &lt;td&gt;“Это вайб!” for random numbers&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Korol i Shut (Gorshok)&lt;/td&gt;
      &lt;td&gt;Theatrical punk rock, horror imagery&lt;/td&gt;
      &lt;td&gt;Gothic humor, Friday the 13th&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Vladimir Vysotsky&lt;/td&gt;
      &lt;td&gt;Raspy, intense, poetic bard&lt;/td&gt;
      &lt;td&gt;Every number becomes existential&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Sergey Shnurov (Leningrad)&lt;/td&gt;
      &lt;td&gt;Rock provocateur, irreverent, mocking&lt;/td&gt;
      &lt;td&gt;Makes fun of the game itself&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Filipp Kirkorov&lt;/td&gt;
      &lt;td&gt;Grandiose pop king, over-the-top&lt;/td&gt;
      &lt;td&gt;Everything is a grand celebration&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Grigory Leps&lt;/td&gt;
      &lt;td&gt;Rock ballad, emotional, gravelly&lt;/td&gt;
      &lt;td&gt;Dramatic weight to each number&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Nikolay Baskov&lt;/td&gt;
      &lt;td&gt;Operatic tenor, polished, charming&lt;/td&gt;
      &lt;td&gt;Treats Loto like opera&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;character-specific-jokes&quot;&gt;Character-Specific Jokes&lt;/h2&gt;

&lt;p&gt;The magic isn’t just in the voices — each character has jokes tailored to specific numbers. Here
are some examples:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Brezhnev on 5:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Пять... Пятилетка! Выполним и перевыполним план!
(Five... Five-year plan! We will fulfill and exceed the plan!)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Gorshok (Korol i Shut) on 13:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Тринадцать! Моё любимое число! Ха-ха-ха! Пятница тринадцатое!
(Thirteen! My favorite number! Ha-ha-ha! Friday the 13th!)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Zhirinovsky on any number:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Это число... это символ нашей великой страны!
(This number... this is a symbol of our great country!)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Monetochka on 69:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Ой, это тот самый мем... Найс!
(Oh, that&apos;s that meme... Nice!)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The character definitions live in Python dataclasses with voice descriptions, typical phrases,
and number-specific joke mappings.&lt;/p&gt;

&lt;h2 id=&quot;how-voicedesign-actually-works&quot;&gt;How VoiceDesign Actually Works&lt;/h2&gt;

&lt;p&gt;Traditional TTS voice cloning needs audio samples. VoiceDesign takes a different approach:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;voice_description&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;
A deep elderly male Russian voice speaking with
slow deliberate speech and long dramatic pauses.
Authoritative Soviet leader tone with slight
speech impediment characteristic of late period.
&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The model generates voice parameters from the description
# No audio samples required
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The model was trained to understand voice characteristics from text and synthesize matching
speech. It’s not perfect — sometimes the voices drift across utterances. But for a game where
each number is a separate audio clip, it works well.&lt;/p&gt;

&lt;p&gt;The Russian language support is surprisingly good. Qwen handles Cyrillic text natively, and the
pronunciation is close enough that native speakers recognize the character archetypes.&lt;/p&gt;

&lt;h2 id=&quot;noise-detection-making-it-interactive&quot;&gt;Noise Detection: Making It Interactive&lt;/h2&gt;

&lt;p&gt;The game includes an experimental feature: ambient noise detection. It listens to the room and
reacts:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# When the room gets quiet, repeat the last number
# Uses exponential backoff to get attention
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ambient_noise&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;repeat_last_number&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;backoff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This makes the game feel more alive — when people are talking and not paying attention, the AI
announcer waits. When it gets quiet, it calls the next number or repeats. Like a patient game
master.&lt;/p&gt;

&lt;h2 id=&quot;running-the-game&quot;&gt;Running the Game&lt;/h2&gt;

&lt;p&gt;Basic usage:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Install dependencies&lt;/span&gt;
./setup_qwen_mac.sh

&lt;span class=&quot;c&quot;&gt;# Run with AI voices (first run generates all audio clips)&lt;/span&gt;
uv run python loto.py &lt;span class=&quot;nt&quot;&gt;--qwen&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Run with pre-generated voices (faster startup)&lt;/span&gt;
uv run python loto.py &lt;span class=&quot;nt&quot;&gt;--qwen&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--cache&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Select specific character&lt;/span&gt;
uv run python loto.py &lt;span class=&quot;nt&quot;&gt;--qwen&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--character&lt;/span&gt; brezhnev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;First run takes a while — it generates audio for all numbers (1-90) for the selected character.
Subsequent runs with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--cache&lt;/code&gt; start instantly.&lt;/p&gt;

&lt;h2 id=&quot;performance-on-apple-silicon&quot;&gt;Performance on Apple Silicon&lt;/h2&gt;

&lt;p&gt;On M1/M2/M3 Macs, the MPS backend provides decent performance:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Audio generation: ~2-3 seconds per number&lt;/li&gt;
  &lt;li&gt;First run (90 numbers): ~3-4 minutes per character&lt;/li&gt;
  &lt;li&gt;Cached playback: instant&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For comparison, CPU-only inference is 5-10x slower. The MPS optimization in PyTorch makes local
AI TTS practical.&lt;/p&gt;

&lt;h2 id=&quot;lessons-learned&quot;&gt;Lessons Learned&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Family Fun&lt;/strong&gt; is easy to achieve. Just a suggent idea gets much easier to implement.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;VoiceDesign is powerful but inconsistent.&lt;/strong&gt; The same voice description can produce slightly
different voices across runs. For a game, this adds variety. For production TTS, you’d want more
control.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Character design matters more than voice quality.&lt;/strong&gt; A mediocre voice with great jokes is more
entertaining than a perfect voice reading numbers monotonously. The character personalities
carry the experience.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;#LocalAI for fun projects is here.&lt;/strong&gt; No API keys, no cloud costs, no latency. Models like
Qwen3-TTS make creative voice projects accessible to anyone with a decent Mac.&lt;/p&gt;

&lt;h2 id=&quot;try-it&quot;&gt;Try It&lt;/h2&gt;

&lt;p&gt;The code is at &lt;a href=&quot;https://github.com/jonnyzzz/jonnyzzz-loto&quot;&gt;github.com/jonnyzzz/jonnyzzz-loto&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Add your own characters. Improve the jokes. Make Brezhnev more authentic. The voice description
format is simple enough that you can experiment without ML expertise.&lt;/p&gt;

&lt;p&gt;And if you have a better joke for number 77 (семьдесят семь) — submit a PR.&lt;/p&gt;

&lt;p&gt;Find me on &lt;a href=&quot;https://www.linkedin.com/in/jonnyzzz/&quot;&gt;LinkedIn&lt;/a&gt; or
&lt;a href=&quot;https://twitter.com/jonnyzzz&quot;&gt;Twitter&lt;/a&gt; if you build something fun with AI voices.&lt;/p&gt;

&lt;p&gt;Stay tuned, I will prepare something similar with NVIDIA DGX Spart setup for some conferences in the furure!&lt;/p&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="ai" />
  
    <category term="tts" />
  
    <category term="python" />
  
    <category term="qwen" />
  
    <category term="voice-cloning" />
  
    <summary type="html">Building a Russian Loto announcer with 12 celebrity AI voices using Qwen3-TTS</summary>
  
  </entry>
  
  <entry>
    <title type="html">Orchestrating AI Fleets: When Agents Manage Agents</title>
    <link href="https://jonnyzzz.com/blog/2026/01/30/orchestrating-ai-fleets/" rel="alternate" type="text/html" title="Orchestrating AI Fleets: When Agents Manage Agents" />
    <published>2026-01-30T00:00:00+00:00</published>
    <updated>2026-01-30T00:00:00+00:00</updated>
    <id>/blog/2026/01/30/orchestrating-ai-fleets</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/01/30/orchestrating-ai-fleets/">&lt;p&gt;For the last several days, I have made my AI Agents call each other. Claude Code, Codex, and Gemini. 
After several agentic improvements, the root prompt does the following steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Instructs an agent to copy and adjust the prompt for their task&lt;/li&gt;
  &lt;li&gt;Instructs to use the standard &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run-agent.sh&lt;/code&gt; start agents, log outputs&lt;/li&gt;
  &lt;li&gt;Clearly states that the AI agent should use the script, not the embedded feature&lt;/li&gt;
  &lt;li&gt;Starts the agent at any working folder&lt;/li&gt;
  &lt;li&gt;One more secret ingredient to make it work, or two&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One AI Agent controls the flow of a fleet of other agents. It is much better than just a bash loop, 
and I like how it performs and adapts to the task given.&lt;/p&gt;

&lt;p&gt;Today:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Deep research on a product topic was conducted by 16+ agents&lt;/li&gt;
  &lt;li&gt;Code reading of a big monorepo, so the agent could understand how to implement the REST API client&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Claude Code calling multiple agents on the JetBrains Space reposiotry to mine knowledge from my repositories
&lt;img src=&quot;/images/posts/2026-01-30-orchestrating-ai-fleets-example.png&quot; alt=&quot;Claude Code orchestrating multiple agents&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is more&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Agent processes never ask me, and bother much less&lt;/li&gt;
  &lt;li&gt;I manage to make a root AI Agent run for hours unattended&lt;/li&gt;
  &lt;li&gt;It keeps the context small for each and avoid context rot&lt;/li&gt;
  &lt;li&gt;One can manage 3-5 such sessions, and I want to grow this number&lt;/li&gt;
  &lt;li&gt;The baseline is still the &lt;a href=&quot;https://jonnyzzz.com/RLM.md&quot;&gt;https://jonnyzzz.com/RLM.md&lt;/a&gt;,
which I based on the outcomes of my multiple experiments&lt;/li&gt;
  &lt;li&gt;Look what I’ve done &lt;a href=&quot;https://mcp-steroid.jonnyzzz.com&quot;&gt;MCP Steroid&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What’s next? One part say – do measurements. I will keep experimenting at the first place, 
and I need a partnership to set it up in a more scientific manner.&lt;/p&gt;

&lt;p&gt;This work builds directly on the success of my previous experiment 
where &lt;a href=&quot;/blog/2026/01/24/16-ai-agents-documentation-refactor/&quot;&gt;16 AI Agents Fixed Our Documentation Problem&lt;/a&gt;
and relies heavily on the &lt;a href=&quot;/blog/2026/01/05/rlm-multi-agent-orchestration/&quot;&gt;Recursive Language Model (RLM)&lt;/a&gt;
methodology I established earlier this year.&lt;/p&gt;

&lt;p&gt;The fleet is busy workin’
&lt;img src=&quot;/images/posts/2026-01-30-orchestrating-ai-fleets-busy-working.png&quot; alt=&quot;AI Agent fleet busy working&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Happy Friday!&lt;/p&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="ai-agents" />
  
    <category term="multi-agent" />
  
    <category term="orchestration" />
  
    <category term="automation" />
  
    <category term="llm" />
  
    <summary type="html">AI Agents call each other by themselves: One controls the flow, others work on the short task</summary>
  
  </entry>
  
  <entry>
    <title type="html">Coding in English with AI</title>
    <link href="https://jonnyzzz.com/blog/2026/01/27/coding-in-english-with-ai/" rel="alternate" type="text/html" title="Coding in English with AI" />
    <published>2026-01-27T00:00:00+00:00</published>
    <updated>2026-01-27T00:00:00+00:00</updated>
    <id>/blog/2026/01/27/coding-in-english-with-ai</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/01/27/coding-in-english-with-ai/">&lt;p&gt;About a year ago, I was wondering when programming would move to natural languages, and what it would look like.
&lt;strong&gt;Now I know&lt;/strong&gt;. For the last two months, English has become my main coding language to instruct AI agents,
build pipelines, and architect systems. &lt;strong&gt;I’m coding in English to make AI Agents code in code.&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Running sub-agents is the new trend. We move from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ralph&lt;/code&gt; towards a multiple-agent swarm, 
where an agent calls sub-agents and monitors the outcomes.
ClaudeCode and Codex (and other vendors) recently included sub-agent support, too.
You make bigger tasks smaller by running sub-agents, and only if the whole chain of command can start sub-agents 
do we have the ability to split tasks, iterate, and so on.&lt;/p&gt;

&lt;p&gt;My main challenge is to check whether an agent can control sub-agents, 
so instead of a bash or Python loop, we put a managing agent at the top who will review 
the outcomes and decide on the next step.&lt;/p&gt;

&lt;p&gt;The whole agentic development process starts to look like an agentic waterflow workflow of many activities, each of which
is done my various AI agents, or even multiple agents, with multiple various 
agentic roles, repeats, and cycles. Should something go wrong, it should restart. 
I created a ~10-step instruction locally for a 25-year-old monorepo and keep testing it to see how
it goes. Agents tend to use the native sub-agent implementation instead of running the console 
script, so I need to explain more carefully that I need processes created. Running sub agent
as sub process gives the sub agent more power, another working directory, another &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CLAUDE.md/AGENTS.md&lt;/code&gt;,
different MCP Servers, and much more fresh context. And we can switch between different agents
within the same waterflow.&lt;/p&gt;

&lt;p&gt;After all iterations and tuning, it looks like Codex can do that better. Sometimes it falls back
to native sub-agent implementation. And my goal is to make agentic swarm function by itself
most of the time. Agents should cooperate to decide on the complex problems, research for solutions.&lt;/p&gt;

&lt;p&gt;Agents can fix your prompts, that is how I improved mine as well. Just ask it to start all 
supported agents to interview them for problems and suggestions. Process outcomes with agents.&lt;/p&gt;

&lt;p&gt;With that, I came to the next question: how can one monitor the graph of agents running? Not that I 
want to interact, just out of curiosity. Agents helped me create the monitoring scripts for 
that, so now I can see all agents started. Such code
is cheap, and experimentation is not blocked by that.&lt;/p&gt;

&lt;p&gt;Today, I have multiple code repositories of &lt;strong&gt;prompts&lt;/strong&gt;. I task selected agents to write prompts 
for their sub-agents. I version-control and back-up kilobytes of prompts. I ask agents
to interview agents on selected agentic integration topics. And of course some agents
will code, run tests in IntelliJ via the &lt;a href=&quot;https://mcp-steroid.jonnyzzz.com/&quot;&gt;MCP Steroid&lt;/a&gt;
plugin, debug the code and so on.&lt;/p&gt;

&lt;p&gt;We are approaching a shift where AI Agents become the primary users of software development tools.
The ‘Buying Persona’ remains human, but the ‘User Persona’ is becoming digital. Tools must
now be optimized for agentic consumption — APIs first, GUIs second.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;About a year ago, I was wondering when programming would move to natural languages, and what it would look like.
&lt;strong&gt;Now I know&lt;/strong&gt;. For the last two months, English has become my main coding language to instruct AI agents,
build pipelines, and architect systems. &lt;strong&gt;I’m coding in English to make AI Agents code in code.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I’m coding in English to make AI Agents code in code.&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;instead-of-conclusion&quot;&gt;Instead of Conclusion&lt;/h2&gt;
&lt;p&gt;Below is what Gemini 3 told me&lt;/p&gt;

&lt;p&gt;The landscape of software development is undergoing a seismic shift. For decades, learning to code 
meant mastering syntax, understanding memory management, and debugging cryptic error messages. Today, 
with the rise of powerful AI models, the most important programming language might just be English.&lt;/p&gt;

&lt;h3 id=&quot;the-shift-from-syntax-to-semantics&quot;&gt;The Shift from Syntax to Semantics&lt;/h3&gt;

&lt;p&gt;Traditionally, developers acted as translators, converting human intent into machine-readable 
code. AI agents are now taking over this translation layer.&lt;/p&gt;

&lt;h3 id=&quot;prompt-engineering-as-the-new-coding&quot;&gt;Prompt Engineering as the New Coding&lt;/h3&gt;

&lt;p&gt;Writing clear, unambiguous instructions—prompt engineering—is becoming a critical skill. It 
shares many principles with traditional coding: precision, modularity, and iterative refinement.&lt;/p&gt;

&lt;h3 id=&quot;the-future-of-development&quot;&gt;The Future of Development&lt;/h3&gt;

&lt;p&gt;Does this mean the end of programmers? Unlikely. Instead, it elevates the role of the 
developer from a bricklayer to an architect. We spend less time on boilerplate and more 
time on system design and problem-solving.&lt;/p&gt;

&lt;h3 id=&quot;conclusion-by-ai&quot;&gt;Conclusion by AI&lt;/h3&gt;

&lt;p&gt;Coding in English doesn’t mean coding without thinking. It requires a different kind of discipline—one 
focused on clarity of thought and structural understanding.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;I’m coding in English to make AI Agents code in code.&lt;/strong&gt;&lt;/p&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="ai" />
  
    <category term="coding" />
  
    <category term="future" />
  
    <category term="ai-agents" />
  
    <category term="multi-agent" />
  
    <category term="sub-agent" />
  
    <summary type="html">Coding became natural language</summary>
  
  </entry>
  
  <entry>
    <title type="html">How 16 AI Agents Fixed Our Documentation Problem</title>
    <link href="https://jonnyzzz.com/blog/2026/01/24/16-ai-agents-documentation-refactor/" rel="alternate" type="text/html" title="How 16 AI Agents Fixed Our Documentation Problem" />
    <published>2026-01-24T00:00:00+00:00</published>
    <updated>2026-01-24T00:00:00+00:00</updated>
    <id>/blog/2026/01/24/16-ai-agents-documentation-refactor</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/01/24/16-ai-agents-documentation-refactor/">&lt;p&gt;I’ve been documenting multi-agent orchestration patterns for months, and I had no idea how bad our docs had become
until I asked our customers — AI agents, and they were brutally honest.&lt;/p&gt;

&lt;p&gt;Our documentation had grown to 2,648 lines across four files. The most important commands were buried at line 91 of
CLAUDE-CODE.md, after 90 lines of introductions, explanations, and edge cases. We’d duplicated entire sections across
three CLI docs. And when AI agents tried to use these docs to spawn other AI agents, they struggled.&lt;/p&gt;

&lt;p&gt;Here’s the recursive twist: I sent agents to interview agents about documentation for spawning agents. We used
multi-agent orchestration to fix documentation about multi-agent orchestration. The tools improved themselves.&lt;/p&gt;

&lt;p&gt;This is the story of how 16 AI agents helped us refactor documentation that was both about them and for them. This was
attempt #3, and we finally got it right by doing something simple: we stopped guessing and started asking.&lt;/p&gt;

&lt;h2 id=&quot;the-bigger-picture&quot;&gt;The Bigger Picture&lt;/h2&gt;

&lt;p&gt;This wasn’t just about fixing documentation. It was about proving something I’d suspected but couldn’t demonstrate:
multi-agent orchestration works for real engineering problems, not just toy examples.&lt;/p&gt;

&lt;p&gt;I’ve been experimenting with multi-agent patterns for months, documenting the RLM methodology, building orchestration
guides. But until this project, I hadn’t stress-tested it on something complex and messy. Documentation refactoring is
exactly that - subjective, iterative, full of judgment calls.&lt;/p&gt;

&lt;p&gt;Here’s where it gets meta-recursive: we used agents to spawn agents to interview agents about documentation that
teaches agents how to spawn agents. The documentation problem was its own solution. We needed better docs for
multi-agent orchestration, so we used multi-agent orchestration to improve those docs. The methodology ate its own
dogfood and came out stronger.&lt;/p&gt;

&lt;p&gt;The fact that 16 AI agents could analyze, interview, implement, and validate better than I could alone? That changes
how I think about engineering workflows. But more importantly, the agents-spawning-agents-to-fix-agent-docs approach
proved the recursive viability of the entire pattern.&lt;/p&gt;

&lt;p&gt;We spawned 16 agents across four distinct phases. We ran them in parallel when possible, sequentially when necessary.
We treated AI agents as actual customers and conducted structured interviews to understand their needs. We validated
the results with fresh agents who had no prior context. Some of those interview agents spawned their own sub-agents to
dig deeper into specific sections.&lt;/p&gt;

&lt;p&gt;The outcome? Measurable improvements: 39% shorter documentation, 15% higher quality ratings, 5x faster navigation to
key commands. Production-ready output that both humans and AI agents can use effectively.&lt;/p&gt;

&lt;p&gt;But the bigger lesson is about treating AI agents as customers. They can’t be polite out of social obligation. They
won’t pretend to understand confusing documentation. They’ll tell you exactly where you buried the lead, what’s
missing, and what’s duplicated. They’re brutally honest feedback machines. And when they give feedback about their own
operational instructions? That’s when the recursion gets really interesting.&lt;/p&gt;

&lt;h2 id=&quot;why-this-mattered&quot;&gt;Why This Mattered&lt;/h2&gt;

&lt;h3 id=&quot;the-dual-role-problem&quot;&gt;The Dual Role Problem&lt;/h3&gt;

&lt;p&gt;We’d written 2,648 lines of documentation across four files: CLAUDE-CODE.md, CODEX.md, GEMINI.md, and MULTI-AGENT.md. 
Standard developer docs, right? Not quite.&lt;/p&gt;

&lt;p&gt;The twist: this documentation was both &lt;strong&gt;about&lt;/strong&gt; AI agents and &lt;strong&gt;for&lt;/strong&gt; AI agents. Human developers would read it to 
learn how to spawn sub-agents. But more importantly, the AI agents themselves would read these docs when they needed to 
spawn other AI agents. An agent working on your behalf would need to parse these instructions, understand the patterns, 
and correctly invoke another agent with the right commands and flags.&lt;/p&gt;

&lt;p&gt;If the documentation was confusing, ambiguous, or poorly structured, the AI wouldn’t just get frustrated and ask for 
clarification. It would hallucinate. It would guess. It would burn the context and tokens to figure out the right
parameters. It would use the wrong flags or miss critical setup steps. The 
documentation quality directly impacted whether agent-to-agent orchestration actually worked.&lt;/p&gt;

&lt;h3 id=&quot;why-ai-agents-care-about-documentation-quality&quot;&gt;Why AI Agents Care About Documentation Quality&lt;/h3&gt;

&lt;p&gt;AI agents aren’t just slower readers than humans - they have fundamentally different constraints:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Token costs are real.&lt;/strong&gt; Reading 2,648 lines of documentation translates to roughly 40,000 tokens. Every time an agent 
needed to spawn a sub-agent, it would load these docs into context. That’s not just slow, it’s expensive. Token costs 
compound across every operation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Context limits matter.&lt;/strong&gt; Even with 200K token context windows, shorter is better. The more documentation an agent has 
to process, the slower it responds and the more likely it is to miss crucial details buried in the middle.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Precision matters more than style.&lt;/strong&gt; Ambiguity that humans can resolve through intuition becomes a hallucination risk 
for AI. When documentation says three commands are “RECOMMENDED” with no clear priority, humans might make an educated 
guess. AI agents will pick the first one they see, or worse, combine flags in creative ways that don’t work.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Navigation is harder.&lt;/strong&gt; Humans can Ctrl+F for keywords or skim section headers. AI agents must parse the document 
sequentially, building a mental model as they go. If the most important information is at line 91, they’ve already 
spent 90 lines of processing budget on preliminaries.&lt;/p&gt;

&lt;h3 id=&quot;the-meta-challenge&quot;&gt;The Meta-Challenge&lt;/h3&gt;

&lt;p&gt;Here’s what made this interesting: we needed documentation good enough that an AI reading it could correctly spawn 
another AI, which might then need to spawn yet another AI. The clarity requirements weren’t just high - they were 
recursive. Every ambiguity would compound across agent hierarchies.&lt;/p&gt;

&lt;p&gt;One validator put it perfectly:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Strong operational doc. With recommended command moved to top and error handling added, would be 9/10.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That quote came from an AI agent evaluating the documentation. And it nailed exactly what was missing: quick access to 
the essential commands and proper error handling patterns.&lt;/p&gt;

&lt;p&gt;This was our third attempt. The first two? We tested on agents, and improved with agents with various prompts. 
Each next iteration creates a more improved outcome. But I would not run it too many times to avoid a drift
from the main focus. I’d optimized for completeness over usability, documenting every option and edge case
while forgetting to answer the only question that mattered: “What command do I run right now?”&lt;/p&gt;

&lt;p&gt;What changed? We stopped assuming we knew what good documentation looked like and started treating AI agents as actual 
customers who could tell us. They knew what they needed better than I did.&lt;/p&gt;

&lt;h2 id=&quot;what-the-interviews-revealed&quot;&gt;What the Interviews Revealed&lt;/h2&gt;

&lt;p&gt;The interview data surprised the root agent — seven agents, completely independent, and they all complained 
about the same five things. That kind of consistency doesn’t lie.&lt;/p&gt;

&lt;h3 id=&quot;pain-point-1-buried-the-lead&quot;&gt;Pain Point #1: Buried the Lead&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Severity: HIGH (reported by 5/7 interviews)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This was the killer. I’d buried our most important command at line 91 in CLAUDE-CODE.md, after 90 lines of 
introductions, prerequisites, and edge cases.&lt;/p&gt;

&lt;p&gt;From Interview #1:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Most important command (line 91) hidden after 90 lines of setup. Users want copy-paste immediately.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Looking back, I see exactly what went wrong. I wrote the docs the way I’d learned to write academic papers: 
introduction, background, methodology, results. But documentation isn’t a paper. Users want the answer immediately.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Before:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Introduction]
[Background]
[Architecture overview]
[Installation]
[Configuration]
[Prerequisites]
...
Line 91: # RECOMMENDED: Use this command...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;After:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## Purpose
Command-line interface for spawning Claude Code sub-agents.

### Quick Start
# THIS IS THE ONE COMMAND YOU NEED
claude -p --tools default --permission-mode dontAsk &quot;prompt&quot; 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Impact:&lt;/strong&gt; Time-to-command dropped from 60-90 seconds to 10-15 seconds. For an AI agent reading sequentially, that’s a 
5x improvement. For a human developer scanning the page, it’s the difference between finding what you need and giving 
up.&lt;/p&gt;

&lt;h3 id=&quot;pain-point-2-error-handling-completely-absent&quot;&gt;Pain Point #2: Error Handling Completely Absent&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Severity: HIGH (4/7 interviews)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Not a single retry example. No timeout recommendations. Zero guidance on detecting token limits or API errors. No exit 
code documentation. Our docs assumed success on first try, which is approximately never how real agent orchestration 
works.&lt;/p&gt;

&lt;p&gt;Agent feedback from Interview #4:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“No guidance on detecting token limits/API errors. No concrete retry example with exit code checking.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This wasn’t an edge case concern. Four separate agents independently identified it as a blocker. When your sub-agent 
hits a rate limit at 3 AM, you need to know: retry or fail? How long to wait? What exit code means “try again” vs “give 
up”?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The fix:&lt;/strong&gt; Added Part 9 to MULTI-AGENT.md with production-ready error handling patterns:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;MAX_RETRIES&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3
&lt;span class=&quot;nv&quot;&gt;RETRY_COUNT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0
&lt;span class=&quot;nv&quot;&gt;TIMEOUT_SEC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;900

&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$RETRY_COUNT&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-lt&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$MAX_RETRIES&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do
  if &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;timeout&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$TIMEOUT_SEC&lt;/span&gt; claude &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tools&lt;/span&gt; default &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
     &lt;span class=&quot;nt&quot;&gt;--permission-mode&lt;/span&gt; dontAsk &lt;span class=&quot;s2&quot;&gt;&quot;prompt&quot;&lt;/span&gt; 2&amp;gt;&amp;amp;1&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;then
    &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;SUCCESS&quot;&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;break
  &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;fi
  &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;EXIT_CODE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$?&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Attempt &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$((&lt;/span&gt;RETRY_COUNT+1&lt;span class=&quot;k&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$MAX_RETRIES&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; failed (exit &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$EXIT_CODE&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;)&quot;&lt;/span&gt;
  &lt;span class=&quot;nv&quot;&gt;RETRY_COUNT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$((&lt;/span&gt;RETRY_COUNT &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;))&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;sleep&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;$((&lt;/span&gt;RETRY_COUNT &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;))&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# Exponential backoff: 2s, 4s, 6s&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The new section includes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Retry logic with exponential backoff (2s, 4s, 6s delays)&lt;/li&gt;
  &lt;li&gt;Timeout recommendations by task type (30s for simple queries, 600s for complex analysis)&lt;/li&gt;
  &lt;li&gt;Exit code meanings (0 = success, 1 = general error, 41 = rate limit, 124 = timeout)&lt;/li&gt;
  &lt;li&gt;Token limit detection patterns (grep for specific error messages)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pain-point-3-authentication-steps-missing&quot;&gt;Pain Point #3: Authentication Steps Missing&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Severity: HIGH (3/7 interviews)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;CODEX.md referenced &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.codex/auth.json&lt;/code&gt; in multiple places but never explained how to create it. I’d assumed you’d 
already authenticated before reading the docs.&lt;/p&gt;

&lt;p&gt;Interview #6 agent feedback:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Document never explains how auth.json is created. Assumes user already authenticated.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is the classic curse of knowledge. As the person who wrote the docs, I already had &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auth.json&lt;/code&gt; on my machine. I’d 
logged in months ago. The auth step was invisible to me, but catastrophic for new users.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The fix:&lt;/strong&gt; Added Prerequisites sections to all three CLI docs with step-by-step auth flow:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Verify installation: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;codex --version&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Authenticate: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;codex login&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Confirm: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ls ~/.codex/auth.json&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Test: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;codex exec &quot;hello&quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Plus a troubleshooting table for common issues:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Symptom&lt;/th&gt;
      &lt;th&gt;Diagnosis&lt;/th&gt;
      &lt;th&gt;Fix&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auth.json not found&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Not logged in yet&lt;/td&gt;
      &lt;td&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;codex login&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;invalid credentials&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Token expired&lt;/td&gt;
      &lt;td&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;codex login&lt;/code&gt; again&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;permission denied&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Wrong file permissions&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;chmod 600 ~/.codex/auth.json&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;pain-point-4-20-token-overhead-from-duplication&quot;&gt;Pain Point #4: 20% Token Overhead from Duplication&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Severity: HIGH (reported by 2/7 interviews, measured by our team)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We’d copy-pasted entire sections across CLAUDE-CODE.md, CODEX.md, and GEMINI.md:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;MCP visibility tables (nearly identical, 80+ lines each)&lt;/li&gt;
  &lt;li&gt;Parallel execution patterns (minimal CLI differences, 100+ lines)&lt;/li&gt;
  &lt;li&gt;DO/DON’T best practices lists (word-for-word duplicates, 50+ lines)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This created 795 lines of duplicated content - content that every agent reading any CLI doc had to parse.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The fix:&lt;/strong&gt; Extract to MULTI-AGENT.md as single source of truth. CLI docs now reference rather than duplicate.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Total lines: 2,648 → 1,618 (-39%)&lt;/li&gt;
  &lt;li&gt;Estimated tokens: ~40,000 → ~32,000 (-20%)&lt;/li&gt;
  &lt;li&gt;Maintenance burden: Update 1 file instead of 3&lt;/li&gt;
  &lt;li&gt;Consistency: No more drift between docs&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pain-point-5-mcp-troubleshooting-confusion&quot;&gt;Pain Point #5: MCP Troubleshooting Confusion&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Severity: MEDIUM (2/7 interviews)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Users didn’t understand when Model Context Protocol servers were available to sub-agents, or how to diagnose visibility 
issues. The docs mentioned MCP but never explained the inheritance rules or debugging steps.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The fix:&lt;/strong&gt; Added MCP Troubleshooting section with diagnostic table:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Symptom&lt;/th&gt;
      &lt;th&gt;Diagnosis&lt;/th&gt;
      &lt;th&gt;Fix&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Sub-agent doesn’t see MCP&lt;/td&gt;
      &lt;td&gt;Not registered globally&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;claude mcp add &amp;lt;name&amp;gt; &amp;lt;cmd&amp;gt;&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MCP listed but unavailable&lt;/td&gt;
      &lt;td&gt;Restrictive tool flags&lt;/td&gt;
      &lt;td&gt;Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--tools default&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--tools none&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Connection errors&lt;/td&gt;
      &lt;td&gt;Server not running&lt;/td&gt;
      &lt;td&gt;Check IntelliJ IDE is open&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Works in parent, not in child&lt;/td&gt;
      &lt;td&gt;Command-line args block inheritance&lt;/td&gt;
      &lt;td&gt;Remove explicit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--mcp&lt;/code&gt; flags&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Validation score:&lt;/strong&gt; One agent rated this 9/10, commenting: “diagnostic table format is perfect - covers the exact 
confusion I had.”&lt;/p&gt;

&lt;h3 id=&quot;the-one-command-problem&quot;&gt;The “ONE Command” Problem&lt;/h3&gt;

&lt;p&gt;This wasn’t in our original analysis, but three interviews independently mentioned it: CODEX.md had THREE commands 
marked “RECOMMENDED” with no clear priority.&lt;/p&gt;

&lt;p&gt;Agent feedback:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Ambiguity about which command to use. Mark ONE as primary for 95% of cases.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I thought I was being helpful by showing options. I was actually creating decision paralysis.&lt;/p&gt;

&lt;p&gt;The problem was real. We had:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# RECOMMENDED: Minimal non-interactive&lt;/span&gt;
codex &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;prompt&quot;&lt;/span&gt; 2&amp;gt;&amp;amp;1

&lt;span class=&quot;c&quot;&gt;# RECOMMENDED: Low-friction auto mode&lt;/span&gt;
codex &lt;span class=&quot;nt&quot;&gt;--full-auto&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;prompt&quot;&lt;/span&gt; 2&amp;gt;&amp;amp;1

&lt;span class=&quot;c&quot;&gt;# RECOMMENDED: Maximum tool access&lt;/span&gt;
codex &lt;span class=&quot;nt&quot;&gt;--tools&lt;/span&gt; default &lt;span class=&quot;nt&quot;&gt;--full-auto&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;prompt&quot;&lt;/span&gt; 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Three “recommended” commands means zero recommended commands. An AI agent parsing this has to guess which one we 
actually want them to use.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The fix in CODEX.md:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;### Quick Start&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# ✅ THIS IS THE ONE COMMAND YOU NEED&lt;/span&gt;
codex &lt;span class=&quot;nt&quot;&gt;--full-auto&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;prompt&quot;&lt;/span&gt; 2&amp;gt;&amp;amp;1

Don&lt;span class=&quot;s1&quot;&gt;&apos;t use alternative flag combinations unless you have
a specific reason (debugging, restricting tool access, etc.)
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Validation result:&lt;/strong&gt; 10/10 rating from independent reviewer: “Clear and effective. Zero ambiguity.”&lt;/p&gt;

&lt;p&gt;This became our guiding principle for all three CLI docs: users need ONE command to copy-paste. Not three options. Not 
a menu of flags. One command that works 95% of the time, with a separate “Advanced Usage” section for the 5% edge cases.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;the-implementation-rlm-in-action&quot;&gt;The Implementation: RLM in Action&lt;/h2&gt;

&lt;h3 id=&quot;what-is-rlm&quot;&gt;What Is RLM?&lt;/h3&gt;

&lt;p&gt;Before I explain how we fixed everything, let me introduce the pattern that made this possible.&lt;/p&gt;

&lt;p&gt;RLM, &lt;a href=&quot;https://arxiv.org/abs/2512.24601&quot;&gt;Recursive Language Model&lt;/a&gt;, is a workflow for breaking large 
tasks into parallel agent work. If you’ve used 
MapReduce for data processing, RLM is the same concept applied to AI agents:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PARTITION:&lt;/strong&gt; Divide work into independent tasks with clear boundaries. The rule: no inter-task dependencies. Each 
agent must be able to complete its work without waiting for another agent’s output. For this project, we partitioned by 
file: 5 agents, 5 files to refactor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MAP:&lt;/strong&gt; Execute all tasks simultaneously. Use shell background jobs (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;amp;&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt;) to run agents in parallel. Watch 
for rate limits - most APIs allow 3-5 concurrent requests. Each agent writes to its own output file to avoid conflicts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;REDUCE:&lt;/strong&gt; Manual synthesis of all outputs. This is where the human comes in. Check that cross-references align (no 
broken links between files). Verify terminology is consistent (don’t mix “sub-agent” and “child agent”). Ensure version 
numbers match. Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;diff&lt;/code&gt; to spot conflicts, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; to validate references, and manual review for quality.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;When RLM works:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Tasks are independent (refactoring separate files)&lt;/li&gt;
  &lt;li&gt;Output format is structured (markdown, JSON)&lt;/li&gt;
  &lt;li&gt;Human can validate and merge results&lt;/li&gt;
  &lt;li&gt;Time savings justify coordination cost&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;When RLM fails:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Tasks require sequential coordination (Agent 2 needs Agent 1’s output first)&lt;/li&gt;
  &lt;li&gt;Outputs conflict (both agents edit the same section differently)&lt;/li&gt;
  &lt;li&gt;API rate limits block parallelism entirely&lt;/li&gt;
  &lt;li&gt;Coordination overhead exceeds time savings&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For this project: perfect RLM fit. Five files, five agents, independent work. Parallelism cut execution time from 2+ 
hours to 30 minutes for the implementation phase.&lt;/p&gt;

&lt;p&gt;Full methodology (chopped for agents by agents) with error handling patterns:
&lt;a href=&quot;https://jonnyzzz.com/RLM.md&quot;&gt;jonnyzzz.com/RLM.md&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;working-in-parallel&quot;&gt;Working in Parallel&lt;/h3&gt;

&lt;p&gt;Once we had customer feedback, I had a decision: fix these issues sequentially (safe, slow) or spawn parallel agents 
(risky, fast). I chose fast.&lt;/p&gt;

&lt;p&gt;We spawned 5 agents in parallel:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Agent 1: Refactor CLAUDE-CODE.md (Quick Start, error handling, auth setup)&lt;/li&gt;
  &lt;li&gt;Agent 2: Refactor CODEX.md (single “THE ONE command”, prerequisites, MCP troubleshooting)&lt;/li&gt;
  &lt;li&gt;Agent 3: Refactor GEMINI.md (consistency with other CLIs)&lt;/li&gt;
  &lt;li&gt;Agent 4: Add Part 9 to MULTI-AGENT.md (error handling patterns with retry logic)&lt;/li&gt;
  &lt;li&gt;Agent 5: Add Part 11 to MULTI-AGENT.md (cost management and token optimization)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This was the most nerve-wracking phase. Five agents working simultaneously meant five potential points of failure. If 
two agents edited the same section differently, we’d have merge conflicts. If one agent misunderstood the requirements, 
we’d ship broken docs. But the interviews were so specific - line numbers, severity ratings, concrete examples - that 
the risk felt manageable.&lt;/p&gt;

&lt;p&gt;Each agent received:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Specific pain points from interviews&lt;/li&gt;
  &lt;li&gt;Target file to modify&lt;/li&gt;
  &lt;li&gt;Success criteria (what “done” looks like)&lt;/li&gt;
  &lt;li&gt;Examples of desired patterns&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;timeline-and-reality-check&quot;&gt;Timeline and Reality Check&lt;/h3&gt;

&lt;p&gt;Here’s how the complete project actually played out:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase 1 - Discovery (30 minutes):&lt;/strong&gt;
4 analysis agents examined the existing documentation structure, identified duplication, and mapped pain points. 
Parallelism cut what would have been 2 hours of sequential reading down to 30 minutes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase 2 - Customer Interviews (45 minutes):&lt;/strong&gt;
9 interview agents spawned, though only 7 completed successfully (2 hit API rate limits). Most ran in parallel, with 
some sequential execution when we hit connection limits. This was the most valuable phase - getting honest feedback 
from agents who would actually use these docs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase 3 - Implementation (30 minutes execution, 2 hours human coordination):&lt;/strong&gt;
5 implementation agents working simultaneously on the actual file changes. Agent execution: 30 minutes. Human 
coordination in the reduce phase (checking cross-references, resolving version number mismatches, validating changes): 
2 hours. This is where the RLM pattern requires human oversight.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase 4 - Validation (30 minutes):&lt;/strong&gt;
3 fresh validator agents reviewed the updated documentation. No prior context, just “read this and tell us if it’s 
better.” I was genuinely nervous. What if the refactor made things worse? What if we’d optimized for the wrong thing?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Total: ~4 hours including human coordination, 2 hours of agent execution time&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Without parallelism? I’d still be refactoring. Twelve to fifteen hours of sequential work, minimum. Parallelism cut 
that to 4 hours total. That’s why this matters.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What went wrong:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;2 of 9 interview agents hit API rate limits and failed silently&lt;/li&gt;
  &lt;li&gt;Agent 4’s first attempt at error handling patterns was too generic - had to re-run with specific examples&lt;/li&gt;
  &lt;li&gt;Manual coordination in the Reduce phase took longer than expected because version numbers didn’t match across files&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;command-examples-how-to-interview-ai-agents&quot;&gt;Command Examples: How to Interview AI Agents&lt;/h3&gt;

&lt;p&gt;Here’s the actual command we used to spawn customer interview agents. These templates are copy-paste ready:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Template 1: Accessibility Audit&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;You are auditing CLAUDE-CODE.md for accessibility.

Answer these questions:
1. Where does the Quick Start section appear? (line number)
2. Can you find the most important command in 15 seconds? (yes/no + why)
3. What information is repeated unnecessarily? (quote sections)
4. Rate accessibility: 1-10

Max 300 words, be specific with line numbers.&quot;&lt;/span&gt; | &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  claude &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tools&lt;/span&gt; default &lt;span class=&quot;nt&quot;&gt;--permission-mode&lt;/span&gt; dontAsk 2&amp;gt;&amp;amp;1 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; interview-accessibility.md
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Template 2: Completeness Check&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;You are checking CODEX.md for completeness.

Identify what&apos;s missing:
1. Prerequisites (installation, auth, dependencies)
2. Error handling (retry logic, timeouts, exit codes)
3. Examples (are they copy-paste ready?)
4. Rate completeness: 1-10

Max 300 words, cite line numbers.&quot;&lt;/span&gt; | &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  claude &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tools&lt;/span&gt; default &lt;span class=&quot;nt&quot;&gt;--permission-mode&lt;/span&gt; dontAsk 2&amp;gt;&amp;amp;1 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; interview-completeness.md
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Template 3: Ambiguity Detection&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;You are identifying ambiguous documentation in GEMINI.md.

Find these issues:
1. Sections where the recommended action is unclear
2. Conflicting instructions (if any)
3. Jargon or undefined terms
4. Rate clarity: 1-10

Max 300 words, quote ambiguous phrases.&quot;&lt;/span&gt; | &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  claude &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tools&lt;/span&gt; default &lt;span class=&quot;nt&quot;&gt;--permission-mode&lt;/span&gt; dontAsk 2&amp;gt;&amp;amp;1 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; interview-clarity.md
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The key flags:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-p&lt;/code&gt; (proactive mode): Let the agent use tools without asking&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--tools default&lt;/code&gt;: Give access to standard toolset (Read, Grep, etc.)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--permission-mode dontAsk&lt;/code&gt;: No interactive prompts, fully automated&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2&amp;gt;&amp;amp;1&lt;/code&gt;: Capture both stdout and stderr&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For parallel execution, we used background jobs:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Run 3 interviews in parallel&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Interview 1 prompt&quot;&lt;/span&gt; | claude &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tools&lt;/span&gt; default &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--permission-mode&lt;/span&gt; dontAsk 2&amp;gt;&amp;amp;1 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; interview-1.md&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &amp;amp;

&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Interview 2 prompt&quot;&lt;/span&gt; | claude &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tools&lt;/span&gt; default &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--permission-mode&lt;/span&gt; dontAsk 2&amp;gt;&amp;amp;1 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; interview-2.md&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &amp;amp;

&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Interview 3 prompt&quot;&lt;/span&gt; | claude &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tools&lt;/span&gt; default &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--permission-mode&lt;/span&gt; dontAsk 2&amp;gt;&amp;amp;1 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; interview-3.md&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &amp;amp;

&lt;span class=&quot;nb&quot;&gt;wait
echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;All interviews complete&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt; command is critical - it ensures all background processes finish before proceeding to the next phase.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;validation-did-it-actually-work&quot;&gt;Validation: Did It Actually Work?&lt;/h2&gt;

&lt;h3 id=&quot;the-proof-version-108-testing&quot;&gt;The Proof: Version 1.0.8 Testing&lt;/h3&gt;

&lt;p&gt;After implementation, we had a problem: how do we know the refactored documentation is actually better? We couldn’t 
grade our own work. So we brought in external reviewers.&lt;/p&gt;

&lt;p&gt;Except our external reviewers were AI agents.&lt;/p&gt;

&lt;p&gt;We spawned 3 brand new agents with zero context. No memory of the interviews. No knowledge of what changed. Just: 
“Here’s the documentation. Evaluate it.”&lt;/p&gt;

&lt;p&gt;The first validator came back with an 8.5/10. I exhaled. The second: 9.0/10. I started to relax. The third validator’s 
report opened with: “This is exactly what I needed. Why didn’t it look like this before?”&lt;/p&gt;

&lt;p&gt;That question hit hard. Because I’d written the original docs. I’d thought they were good. I’d been wrong.&lt;/p&gt;

&lt;h3 id=&quot;beforeafter-scores&quot;&gt;Before/After Scores&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Document&lt;/th&gt;
      &lt;th&gt;v1.0.7 Score&lt;/th&gt;
      &lt;th&gt;v1.0.8 Score&lt;/th&gt;
      &lt;th&gt;Improvement&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;CLAUDE-CODE.md&lt;/td&gt;
      &lt;td&gt;8.0/10&lt;/td&gt;
      &lt;td&gt;8.5/10&lt;/td&gt;
      &lt;td&gt;+6%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CODEX.md&lt;/td&gt;
      &lt;td&gt;7.0/10&lt;/td&gt;
      &lt;td&gt;9.0/10&lt;/td&gt;
      &lt;td&gt;+29%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MULTI-AGENT.md&lt;/td&gt;
      &lt;td&gt;8.0/10&lt;/td&gt;
      &lt;td&gt;9.0/10&lt;/td&gt;
      &lt;td&gt;+13%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Average&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;7.67/10&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;8.83/10&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;+15%&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The biggest improvement was CODEX.md - from 7.0 to 9.0. That makes sense: it had the most severe issues (3 
“recommended” commands, buried essentials, missing auth setup). The fixes were dramatic.&lt;/p&gt;

&lt;p&gt;CLAUDE-CODE.md saw a smaller gain because it was already the cleanest of the three. But even there, moving the Quick 
Start section to the top and adding error handling examples pushed it from “good” to “very good.”&lt;/p&gt;

&lt;h3 id=&quot;validator-quotes&quot;&gt;Validator Quotes&lt;/h3&gt;

&lt;p&gt;From VALIDATION-REPORT-v1.0.8.md:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Quick Start sections score 9-10/10 for accessibility”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The validators loved the new structure. Quick Start at the top meant they could find and execute the most important 
command in 15 seconds instead of scrolling through 90 lines of setup.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“‘THE ONE command’ messaging is clear and effective” (10/10)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CODEX.md went from 3 ambiguous “recommended” options to one crystal-clear primary command with explicit guidance on 
when to deviate. No more decision paralysis.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Diagnostic table format is perfect” (9/10)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The MCP troubleshooting table gave validators a quick reference for debugging. Symptom → Diagnosis → Fix. Exactly
what they needed.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Production-ready guidance with measurable metrics” (9/10)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Adding error handling patterns with concrete examples (retry logic, exit codes, timeouts) transformed the docs from 
“here’s what you can do” to “here’s how to do it reliably.”&lt;/p&gt;

&lt;h3 id=&quot;the-numbers&quot;&gt;The Numbers&lt;/h3&gt;

&lt;p&gt;Let’s look at the concrete efficiency metrics:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Documentation efficiency:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Total lines: 2,648 → 1,618 (-39%)&lt;/li&gt;
  &lt;li&gt;Duplication: 795 lines repeated → 0 lines repeated&lt;/li&gt;
  &lt;li&gt;Distance to Quick Start: Line 90+ → Line 10-15 (80% closer)&lt;/li&gt;
  &lt;li&gt;Error handling patterns: 0 → 15+ examples added&lt;/li&gt;
  &lt;li&gt;Cost guidance: None → Comprehensive (Part 11 added)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Customer satisfaction:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Critical issues resolved: 5 out of 5 top pain points (100%)&lt;/li&gt;
  &lt;li&gt;Quality improvement: 7.67/10 → 8.83/10 (+15%)&lt;/li&gt;
  &lt;li&gt;Production ready: ✅ All 3 validators approved for production use&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Time savings:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Time to find recommended command: 60-90 seconds → 10-15 seconds (5x faster)&lt;/li&gt;
  &lt;li&gt;Time to understand error handling: ∞ (didn’t exist) → 2-3 minutes&lt;/li&gt;
  &lt;li&gt;Time to set up authentication: Failed often → Documented with Prerequisites section&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-surprise&quot;&gt;The Surprise&lt;/h3&gt;

&lt;p&gt;Here’s what I didn’t expect: the validators gave us higher scores (8.83/10) than the interviews did (7.67/10). Why?&lt;/p&gt;

&lt;p&gt;The interviewers were looking at broken documentation with a critical eye. The validators were looking at fixed 
documentation with no prior context. They didn’t know it had been worse. They just saw working, clear documentation and 
rated it highly.&lt;/p&gt;

&lt;p&gt;This taught us something about user research: users who experience the pain give you better feedback than users who 
only see the solution. The interview phase was more valuable than the validation phase, even though validation gave us 
better scores.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;what-i-learned&quot;&gt;What I Learned&lt;/h2&gt;

&lt;h3 id=&quot;interview-your-users-even-if-theyre-ais&quot;&gt;Interview Your Users (Even If They’re AIs)&lt;/h3&gt;

&lt;p&gt;I spent two full versions guessing what was wrong. Version 1.0.6? Not idea. Version 1.0.7? Still not good enough. 
One round of structured interviews revealed what users actually needed better than months of my assumptions.&lt;/p&gt;

&lt;p&gt;AI agents are surprisingly good at articulating what makes documentation effective. They can’t be polite out of social 
obligation — if it’s confusing, they’ll tell you.&lt;/p&gt;

&lt;p&gt;The practical takeaway: don’t assume, ask. Spawn 5-10 agents with structured questions, collect ratings, prioritize by 
frequency. That’s it.&lt;/p&gt;

&lt;h3 id=&quot;recommended-is-not-enough&quot;&gt;“Recommended” Is Not Enough&lt;/h3&gt;

&lt;p&gt;CODEX.md had three commands marked “RECOMMENDED” with no clear priority. As one validator put it: “Ambiguity about 
which command to use.” I thought I was being helpful by showing options. I was actually creating decision paralysis.&lt;/p&gt;

&lt;p&gt;Having three recommended commands equals having none. The fix: mark ONE primary command for 95% of use cases, then 
explain when to deviate. After this change, validators gave it a 10/10.&lt;/p&gt;

&lt;p&gt;Your users shouldn’t have to think about which tool to reach for.&lt;/p&gt;

&lt;h3 id=&quot;duplication-has-hidden-costs&quot;&gt;Duplication Has Hidden Costs&lt;/h3&gt;

&lt;p&gt;Twenty percent token overhead seems small until you multiply by every agent reading the docs, every spawned sub-agent, 
every validation test, and every future update requiring 3x maintenance work.&lt;/p&gt;

&lt;p&gt;We removed 795 lines of duplication, saving 8,000 tokens per agent spawn. The ROI calculation: 4 hours to remove 
duplication, 2x time saved per future update, break-even after 2-3 updates.&lt;/p&gt;

&lt;p&gt;Small percentages compound fast.&lt;/p&gt;

&lt;h3 id=&quot;error-handling-is-not-optional&quot;&gt;Error Handling Is Not Optional&lt;/h3&gt;

&lt;p&gt;Zero interviews complained about lack of advanced features. Four out of seven complained about missing error handling.&lt;/p&gt;

&lt;p&gt;Users need to know:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;What to do when it fails (retry logic)&lt;/li&gt;
  &lt;li&gt;How long to wait (timeouts)&lt;/li&gt;
  &lt;li&gt;How to interpret errors (exit codes)&lt;/li&gt;
  &lt;li&gt;When to give up (max retries)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We added 15+ error handling patterns based on this feedback. The lesson: error states are not edge cases, they’re the 
primary use case for production systems.&lt;/p&gt;

&lt;h3 id=&quot;multi-agent-orchestration-works&quot;&gt;Multi-Agent Orchestration Works&lt;/h3&gt;

&lt;p&gt;Sixteen agents spawned across four phases - analysis, interviews, implementation, validation. What made it work:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Clear task boundaries&lt;/li&gt;
  &lt;li&gt;Independent work with no coordination needed within phases&lt;/li&gt;
  &lt;li&gt;Structured outputs in markdown&lt;/li&gt;
  &lt;li&gt;A reduce phase for synthesis&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What didn’t work:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Rate limits hit 2 of 9 interviews&lt;/li&gt;
  &lt;li&gt;Nested execution failed in practice&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The takeaway: plan for API constraints from the start, and test your assumptions about what’s actually possible before 
committing to a strategy.&lt;/p&gt;

&lt;h3 id=&quot;quantify-everything&quot;&gt;Quantify Everything&lt;/h3&gt;

&lt;p&gt;Without numbers, we would have “made it better” instead of 39% reduction, “users liked it” instead of 8.83/10 rating, 
and “faster to use” instead of 5x improvement.&lt;/p&gt;

&lt;p&gt;Metrics we tracked:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Lines of code&lt;/li&gt;
  &lt;li&gt;Duplication percentage&lt;/li&gt;
  &lt;li&gt;Quality ratings&lt;/li&gt;
  &lt;li&gt;Time to key information&lt;/li&gt;
  &lt;li&gt;Issue resolution rate&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Quantification turns vague improvements into compelling evidence. It also forces you to define what success looks like 
before you start.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;what-we-shipped&quot;&gt;What We Shipped&lt;/h2&gt;

&lt;p&gt;The refactor produced two types of deliverables: production files (what users actually read) and research artifacts 
(how we got there).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Production files (v1.0.8):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CLAUDE-CODE.md: 553 → 285 lines (-48%)&lt;/li&gt;
  &lt;li&gt;CODEX.md: 756 → 312 lines (-59%)&lt;/li&gt;
  &lt;li&gt;GEMINI.md: 757 → 327 lines (-57%)&lt;/li&gt;
  &lt;li&gt;MULTI-AGENT.md: 582 → 694 lines (+19%)&lt;/li&gt;
  &lt;li&gt;RLM.md (updated for consistency)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The numbers tell the story: three CLI docs lost more than half their length, while MULTI-AGENT.md grew slightly as the 
new single source of truth for shared patterns. Net result: 1,030 fewer lines to maintain, read, and pay tokens for.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Research artifacts (67.5 KB total):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ACTION-PLAN-v1.0.8.md (8.4 KB) - Implementation roadmap&lt;/li&gt;
  &lt;li&gt;FINAL-REPORT.md (12.1 KB) - Phase 1 summary&lt;/li&gt;
  &lt;li&gt;INTERVIEW-RESULTS.md (15.2 KB) - Customer feedback compilation&lt;/li&gt;
  &lt;li&gt;VALIDATION-REPORT-v1.0.8.md (8.7 KB) - Quality verification from fresh agents&lt;/li&gt;
  &lt;li&gt;RELEASE-NOTES-v1.0.8.md (9.2 KB) - Complete changelog with migration notes&lt;/li&gt;
  &lt;li&gt;REFACTOR-PLAN.md (6.3 KB) - Strategic analysis and approach&lt;/li&gt;
  &lt;li&gt;REFACTOR-SUMMARY.md (4.1 KB) - Before/after comparison with metrics&lt;/li&gt;
  &lt;li&gt;STATUS.md (3.5 KB) - Project status tracking&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These artifacts became their own form of documentation - showing not just what changed, but why, how, and whether it 
worked. Anyone can follow this pattern for their own documentation refactoring.&lt;/p&gt;

&lt;p&gt;All production files and research artifacts are available at &lt;a href=&quot;https://jonnyzzz.com/&quot;&gt;jonnyzzz.com&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;lets-connect&quot;&gt;Let’s Connect&lt;/h2&gt;

&lt;p&gt;Share your results. Try this with your team’s documentation - I want to see your before/after metrics. Tag me on
&lt;a href=&quot;https://www.linkedin.com/in/jonnyzzz/&quot;&gt;LinkedIn at @jonnyzzz&lt;/a&gt; with your quality scores. Bonus points if your AI agents
are as brutally honest as mine were.&lt;/p&gt;

&lt;p&gt;Find a bug in the refactored docs? Submit PRs if you improve the patterns, let me know!&lt;/p&gt;

&lt;p&gt;And if you’re working on multi-agent orchestration or have questions about the RLM methodology, reach out. This is just
the beginning. We documented multi-agent orchestration, then used multi-agent orchestration to improve the
documentation about multi-agent orchestration. Agents spawning agents to interview agents. The recursive loop closed.&lt;/p&gt;

&lt;p&gt;Now go interview your AI customers. They’re waiting to tell you the truth.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next&lt;/h2&gt;

&lt;p&gt;The documentation is now production-ready. Quick Start sections appear in the first 15 lines. Error handling patterns
cover retry logic, timeouts, and exit codes. Authentication prerequisites are explicit. Duplication is eliminated. The
most common command is clearly marked as THE ONE command you need.&lt;/p&gt;

&lt;p&gt;But the research artifacts themselves became something more valuable: meta-documentation about improving documentation
through multi-agent orchestration. They show the complete workflow from analysis to interviews to validation to
deployment.&lt;/p&gt;

&lt;p&gt;The validation agents identified minor polish opportunities: reducing redundancy between Quick Start and Core Commands
sections, adding cost benchmark examples, making version requirements more specific. These are low-priority
improvements, not blockers, but they’re documented and ready for v1.0.9.&lt;/p&gt;

&lt;p&gt;I’m already thinking about the next iteration. Can we use this pattern for code refactoring? For test generation? For
architecture decisions? The interviews proved AI agents can articulate quality issues. The parallel implementation
proved they can fix those issues. That’s powerful.&lt;/p&gt;

&lt;p&gt;Next quarter, we’re applying this to actual codebase refactoring. The documentation was the proof of concept. The real
work is applying this to engineering workflows where the stakes are higher and the problems are messier. That’s where
this gets interesting.&lt;/p&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="ai-agents" />
  
    <category term="multi-agent" />
  
    <category term="sub-agent" />
  
    <category term="documentation" />
  
    <category term="rlm" />
  
    <summary type="html">We interviewed 7 AI agents about our documentation. After 16 agents and honest feedback - 39% shorter, 15% higher quality, 5x faster to use.</summary>
  
  </entry>
  
  <entry>
    <title type="html">Kotlin DSLs in 2026: Patterns That Stood the Test of Time</title>
    <link href="https://jonnyzzz.com/blog/2026/01/19/kotlin-dsl-2026/" rel="alternate" type="text/html" title="Kotlin DSLs in 2026: Patterns That Stood the Test of Time" />
    <published>2026-01-19T00:00:00+00:00</published>
    <updated>2026-01-19T00:00:00+00:00</updated>
    <id>/blog/2026/01/19/kotlin-dsl-2026</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/01/19/kotlin-dsl-2026/">&lt;p&gt;Eight years ago, I wrote about using Kotlin higher-order functions to simplify Java builders. 
Around the same time, I explored ad-hoc Gradle plugins and “The DSL Way”–replacing configuration 
files with type-safe Kotlin code. Looking back from 2026, these patterns have not only survived 
but have become fundamental to how we build modern JVM applications.&lt;/p&gt;

&lt;h2 id=&quot;the-core-idea-that-never-changed&quot;&gt;The Core Idea That Never Changed&lt;/h2&gt;

&lt;p&gt;The fundamental insight remains: Kotlin’s language features–extension functions, lambdas 
with receivers, and operator overloading–turn any API into a potential DSL.&lt;/p&gt;

&lt;p&gt;Consider the classic Java builder problem:&lt;/p&gt;

&lt;div class=&quot;language-kotlin highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Traditional Java builder usage - verbose and error-prone&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;py&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;JWT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;withIssuer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ISSUER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;withClaim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;userId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;userId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;withClaim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;serviceId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;serviceId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;// Easy to copy-paste errors!&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;algorithm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The solution I proposed in 2018 still works perfectly:&lt;/p&gt;

&lt;div class=&quot;language-kotlin highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;fun&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;withX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;?,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;py&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;JWT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;withIssuer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ISSUER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;withX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;userId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;withClaim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;userId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;withX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;serviceId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;withClaim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;serviceId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;algorithm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-has-evolved-scope-control&quot;&gt;What Has Evolved: Scope Control&lt;/h2&gt;

&lt;p&gt;Kotlin introduced &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@DslMarker&lt;/code&gt; for better scope control in DSLs:&lt;/p&gt;

&lt;div class=&quot;language-kotlin highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nd&quot;&gt;@DslMarker&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;annotation&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ConfigDsl&lt;/span&gt;

&lt;span class=&quot;nd&quot;&gt;@ConfigDsl&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ServerConfig&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;py&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;lateinit&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;py&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DatabaseConfig&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;fun&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DatabaseConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;database&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DatabaseConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;fun&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;server&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ServerConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ServerConfig&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;
    &lt;span class=&quot;nc&quot;&gt;ServerConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@DslMarker&lt;/code&gt; annotation prevents accidentally calling outer scope functions from inner blocks.&lt;/p&gt;

&lt;h2 id=&quot;build-systems-gradle-kotlin-dsl-matured&quot;&gt;Build Systems: Gradle Kotlin DSL Matured&lt;/h2&gt;

&lt;p&gt;The patterns for ad-hoc plugins and code reuse have become standard:&lt;/p&gt;

&lt;div class=&quot;language-kotlin highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// buildSrc/src/main/kotlin/service-conventions.gradle.kts&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;plugins&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;kotlin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;jvm&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;application&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;dependencies&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;implementation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;platform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;org.jetbrains.kotlin:kotlin-bom&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;implementation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;io.ktor:ktor-server-core:2.3.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The trick of using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$id:$id.gradle.plugin:$version&lt;/code&gt; to include plugin dependencies 
in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;buildSrc&lt;/code&gt; remains the key insight for sharing complex build logic.&lt;/p&gt;

&lt;h2 id=&quot;configuration-as-code-the-dsl-way&quot;&gt;Configuration as Code: The DSL Way&lt;/h2&gt;

&lt;p&gt;The concept of transforming configuration files into executable Kotlin code has proven durable:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Parse the original format (properties, YAML, XML)&lt;/li&gt;
  &lt;li&gt;Generate readable Kotlin DSL code&lt;/li&gt;
  &lt;li&gt;Execute the DSL to produce the original format&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-kotlin highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Instead of log4j.properties&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;log4j&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;py&quot;&gt;console&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;appender&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ConsoleAppender&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;console&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;PatternLayout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;conversionPattern&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;%d{ISO8601} [%t] %-5p %c - %m%n&quot;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    
    &lt;span class=&quot;nf&quot;&gt;rootLogger&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;level&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ERROR&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;appenders&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;console&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Variable references enable find-usages and rename refactoring. The type system catches invalid configurations at compile time.&lt;/p&gt;

&lt;h2 id=&quot;modern-use-cases&quot;&gt;Modern Use Cases&lt;/h2&gt;

&lt;p&gt;Beyond build systems, Kotlin DSLs have found homes in:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;API Client Generation&lt;/strong&gt;: Ktor uses DSLs for routing and request handling&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Infrastructure as Code&lt;/strong&gt;: Kotlin DSLs for Terraform or Pulumi&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data Pipelines&lt;/strong&gt;: Spark and Flink wrappers&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;UI Frameworks&lt;/strong&gt;: Compose is fundamentally a Kotlin DSL&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;practical-guidelines-for-dsl-design&quot;&gt;Practical Guidelines for DSL Design&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Start with the usage site&lt;/strong&gt;: Write how you want the DSL to look first&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@DslMarker&lt;/code&gt;&lt;/strong&gt;: Always add scope control&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Prefer extension functions&lt;/strong&gt;: Keep core interfaces clean&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Make illegal states unrepresentable&lt;/strong&gt;: Use the type system&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Provide escape hatches&lt;/strong&gt;: Sometimes users need the underlying API&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Kotlin DSLs have moved from a clever technique to an essential tool in the JVM ecosystem. 
The patterns established years ago remain the foundation. What has changed is the tooling
maturity and breadth of applications.&lt;/p&gt;

&lt;p&gt;If you are still writing verbose Java builder code or maintaining error-prone configuration 
files, consider whether a thin Kotlin DSL layer could transform your development experience.&lt;/p&gt;

&lt;p&gt;The best programs remain immutable programs. And the best configurations are the ones that fail
at compile time, not in production.&lt;/p&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="kotlin" />
  
    <category term="java" />
  
    <category term="dsl" />
  
    <summary type="html">Eight years ago, I wrote about using Kotlin higher-order functions to simplify Java builders. Around the same time, I explored ad-hoc Gradle plugins and “The DSL Way”–replacing configuration files with type-safe Kotlin code. Looking back from 2026, these patterns have not only survived but have become fundamental to how we build modern JVM applications.</summary>
  
  </entry>
  
  <entry>
    <title type="html">Stop Optimizing Code Generation: Why Code Review Is Your Real SDLC Bottleneck</title>
    <link href="https://jonnyzzz.com/blog/2026/01/16/code-review-bottleneck/" rel="alternate" type="text/html" title="Stop Optimizing Code Generation: Why Code Review Is Your Real SDLC Bottleneck" />
    <published>2026-01-16T00:00:00+00:00</published>
    <updated>2026-01-16T00:00:00+00:00</updated>
    <id>/blog/2026/01/16/code-review-bottleneck</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/01/16/code-review-bottleneck/">&lt;p&gt;Everyone is racing to make developers write code faster. GitHub Copilot promises 55% 
faster task completion. Cursor raised $2.3B on the promise of AI-native coding. Amazon 
launched Kiro. Google built Antigravity. The message is clear: AI will supercharge code generation.&lt;/p&gt;

&lt;p&gt;But here is the uncomfortable truth: &lt;strong&gt;coding was never your bottleneck.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-myth-of-the-coding-bottleneck&quot;&gt;The Myth of the Coding Bottleneck&lt;/h2&gt;

&lt;p&gt;According to IDC research, developers spend only 16% of their time actually 
writing code. Data from 250,000+ developers shows they code approximately 52 
minutes per day–about 4 hours and 21 minutes during a normal workweek.&lt;/p&gt;

&lt;p&gt;Where does the rest of the time go? Meetings, context switching, waiting for builds, 
debugging production issues, and–critically–&lt;strong&gt;waiting for code reviews&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;If your developers are only coding 16% of their day, making that 16% twice as fast 
means you have improved total developer throughput by… 8%. Meanwhile, the other 
84% of their time remains untouched.&lt;/p&gt;

&lt;h2 id=&quot;where-time-actually-goes-the-real-bottlenecks&quot;&gt;Where Time Actually Goes: The Real Bottlenecks&lt;/h2&gt;

&lt;p&gt;If you want to optimize your SDLC, you need to measure it. Here is what the data reveals:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Code Review Wait Times&lt;/strong&gt;
Elite development teams complete code reviews in under 3 hours, with 
pickup times under 75 minutes. Most teams are far from elite. When 
developers wait 3 days for a review, they context-switch to other 
work. Research shows it takes an average of 23 minutes to refocus after an interruption.&lt;/p&gt;

&lt;p&gt;At Meta, they found a direct correlation between slow code review times and engineer
dissatisfaction. When they optimized their review process, their average 
“Time In Review” dropped 7%, and diffs waiting longer than three days decreased by 12%.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Context Switching Costs&lt;/strong&gt;
Gerald Weinberg’s research demonstrates that adding a single extra 
project to a developer’s workload consumes 20% of their time through context 
switching. Add a third task, and half their time evaporates.&lt;/p&gt;

&lt;h2 id=&quot;why-code-review-is-the-right-place-for-ai&quot;&gt;Why Code Review Is the Right Place for AI&lt;/h2&gt;

&lt;p&gt;Code review sits at the intersection of several properties that make it ideal for AI assistance:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. High Volume, Repetitive Patterns&lt;/strong&gt;
Most code review feedback falls into predictable categories: style 
violations, null safety issues, missing error handling, deprecated
API usage, performance anti-patterns.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Expert Knowledge Is Scarce&lt;/strong&gt;
Every organization has “rock star” reviewers–the developers 
everyone wants reviewing their code. AI can extract patterns from these
experts’ historical reviews and apply them at scale.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Review Latency Compounds&lt;/strong&gt;
A 24-hour code review delay is not just 24 hours lost. It triggers context 
switching, creates WIP accumulation, and extends your entire change lead time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. Human Reviewers Should Focus on Architecture, Not Syntax&lt;/strong&gt;
Senior developers’ time is better spent on architectural decisions, 
mentoring, and strategic code concerns–not catching missing null checks.&lt;/p&gt;

&lt;h2 id=&quot;action-items-for-engineering-leaders&quot;&gt;Action Items for Engineering Leaders&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Measure your SDLC&lt;/strong&gt; - Track where time actually goes. What is your median PR pickup time?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Identify your review bottlenecks&lt;/strong&gt; - Who are your rock star reviewers?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Start with automation of the mechanical&lt;/strong&gt; - Ensure CI and static analysis gates are in place before human review&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Extract patterns from your best reviewers&lt;/strong&gt; - Analyze historical review comments&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Measure the impact&lt;/strong&gt; - Track review cycle time before and after AI assistance&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;the-future&quot;&gt;The Future&lt;/h2&gt;

&lt;p&gt;I do believe that we are at the verge of automatic processes, you cannot
effectively benefit from AI tools if you keep doing 100% of some work manually
within your SDLC pipeline.&lt;/p&gt;

&lt;p&gt;We need to build automated code reviews, which will make our developers faster,
and which will allow us to process more code, partially automatically.&lt;/p&gt;

&lt;p&gt;In the long run, I believe, we are going to start building trust, and make AI a
natural pall and helper in the review process. Build it to make it to complete
5% of the reviews to make it trustworthy partner. And grow from that.&lt;/p&gt;

&lt;h2 id=&quot;the-future-reviewer&quot;&gt;The Future Reviewer&lt;/h2&gt;

&lt;p&gt;Given we have automated code reviews, where an AI agents can comment on the review,
they can propose suggested improvements, and humans for help.&lt;/p&gt;

&lt;p&gt;Such AI Agents need to learn from your existing code reviews, so you build skill and
principles, which your team is operating on. That is the way the trust is established.&lt;/p&gt;

&lt;p&gt;The learned principles should sit in the Markdown files directly in your repositories,
team should read, discuss, and update it.&lt;/p&gt;

&lt;p&gt;** Move all your water cooler conversations to the Markdown files, wikis, so agents can read that **&lt;/p&gt;

&lt;p&gt;Remote teams, that’s for you too!&lt;/p&gt;

&lt;h2 id=&quot;the-future-agentic-swarm&quot;&gt;The Future Agentic Swarm&lt;/h2&gt;

&lt;p&gt;Agents will be talking to each other. Like a coding agent will interact with the reviewers agents.&lt;/p&gt;

&lt;p&gt;As it works for humans today. No changes.&lt;/p&gt;

&lt;p&gt;And agents love working in the feedback loop, gamification!&lt;/p&gt;

&lt;h2 id=&quot;the-bigger-picture&quot;&gt;The Bigger Picture&lt;/h2&gt;

&lt;p&gt;The organizations that win will not be those who generate code fastest. They will be the 
ones who deliver value fastest – and that requires optimizing the entire software 
development lifecycle, not just the coding phase.&lt;/p&gt;

&lt;p&gt;Coding is not your bottleneck. It probably never was. The data is clear. Now it is time to put
AI where it will actually make a difference.&lt;/p&gt;

&lt;p&gt;Code review is waiting.&lt;/p&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="ai" />
  
    <category term="developer-productivity" />
  
    <summary type="html">Everyone is racing to make developers write code faster. GitHub Copilot promises 55% faster task completion. Cursor raised $2.3B on the promise of AI-native coding. Amazon launched Kiro. Google built Antigravity. The message is clear: AI will supercharge code generation.</summary>
  
  </entry>
  
  <entry>
    <title type="html">From Academic Paper to Executable Skills: Multi-Agent Orchestration with RLM</title>
    <link href="https://jonnyzzz.com/blog/2026/01/05/rlm-multi-agent-orchestration/" rel="alternate" type="text/html" title="From Academic Paper to Executable Skills: Multi-Agent Orchestration with RLM" />
    <published>2026-01-05T00:00:00+00:00</published>
    <updated>2026-01-05T00:00:00+00:00</updated>
    <id>/blog/2026/01/05/rlm-multi-agent-orchestration</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/01/05/rlm-multi-agent-orchestration/">&lt;p&gt;Translating academic research into practical AI agent skills through systematic experimentation.&lt;/p&gt;

&lt;p&gt;A few weeks ago, I stumbled upon a fascinating paper on X: “Recursive Language Models” by Zhang, Kraska, and Khattab from MIT CSAIL. The core insight was elegant—treat the prompt as a Python variable rather than neural network input, allowing models to work with unbounded context through recursive sub-calls. The obvious question: could we translate these theoretical patterns into executable instructions for AI agents?&lt;/p&gt;

&lt;p&gt;This post documents the experiment. We took the RLM paper, extracted its core concepts, and transformed them into skill files that AI agents can follow. Then we validated the approach by having multiple AI agents—Claude Code, Codex, and Gemini—cross-validate each other’s work. The results reveal both the promise and the challenges of multi-agent orchestration.&lt;/p&gt;

&lt;h2 id=&quot;the-problem-academic-papers-dont-compile&quot;&gt;The Problem: Academic Papers Don’t Compile&lt;/h2&gt;

&lt;p&gt;Research papers describe algorithms and results. They don’t provide executable instructions that an AI agent can follow step-by-step. The gap between “here’s the theory” and “now do this” is where most knowledge transfer fails.&lt;/p&gt;

&lt;p&gt;Consider RLM’s key insight about context rot—performance degrades as context length increases. The paper quantifies this beautifully with benchmarks on OOLONG and BrowseComp+. But an AI agent facing a 100K token codebase doesn’t know to apply partition+map+reduce unless explicitly told when and how.&lt;/p&gt;

&lt;p&gt;Our approach was systematic:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Extract&lt;/strong&gt; - Identify actionable patterns from the paper&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Codify&lt;/strong&gt; - Transform into explicit decision trees and templates&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Validate&lt;/strong&gt; - Cross-check with multiple AI models&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Iterate&lt;/strong&gt; - Refine based on experimental feedback&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;the-translation-process&quot;&gt;The Translation Process&lt;/h2&gt;

&lt;h3 id=&quot;phase-1-core-concept-extraction&quot;&gt;Phase 1: Core Concept Extraction&lt;/h3&gt;

&lt;p&gt;The RLM paper’s abstract mentions “treating the prompt as a Python variable.” What does this mean operationally?&lt;/p&gt;

&lt;p&gt;After multiple read-throughs (some by humans, some by AI agents), we extracted these key patterns:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Paper Concept&lt;/th&gt;
      &lt;th&gt;Executable Pattern&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Context as variable&lt;/td&gt;
      &lt;td&gt;Access data programmatically, not via prompt stuffing&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Recursive sub-calls&lt;/td&gt;
      &lt;td&gt;Spawn sub-agents with focused prompts&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;REPL environment&lt;/td&gt;
      &lt;td&gt;Iterative tool use with state management&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Partition+Map+Reduce&lt;/td&gt;
      &lt;td&gt;Chunk large inputs, process in parallel, aggregate&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;phase-2-decision-tree-construction&quot;&gt;Phase 2: Decision Tree Construction&lt;/h3&gt;

&lt;p&gt;Academic papers describe what works. Skill files need to specify when to apply each technique. We built decision trees:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;START
  |
  +-- Context &amp;gt; 50K tokens? -------- YES --&amp;gt; ACTIVATE RLM
  |     |
  |     NO
  |     |
  +-- Context &amp;gt; 16K AND complexity &amp;gt; O(1)? -- YES --&amp;gt; CONSIDER RLM
  |     |
  |     NO
  |     |
  +-- Files to process &amp;gt; 5? -------- YES --&amp;gt; ACTIVATE RLM
  |     |
  |     NO
  |     |
  +-- Multi-hop reasoning needed? -- YES --&amp;gt; ACTIVATE RLM
  |     |
  |     NO
  |     |
  +-- PROCEED DIRECTLY
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This transforms a theoretical concept into an operational rule. Note the complexity-aware middle branch—the full RLM.md includes additional nuance for moderate-sized contexts where task complexity (O(n) vs O(n²)) determines whether RLM overhead is worthwhile.&lt;/p&gt;

&lt;h3 id=&quot;phase-3-strategy-templates&quot;&gt;Phase 3: Strategy Templates&lt;/h3&gt;

&lt;p&gt;The paper describes emergent strategies like “Grep First” and “Preview + Partition.” We codified these into templates:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;STRATEGY: Grep First
TRIGGER: Searching for specific patterns across many files
PROCEDURE:
1. Use grep/search to identify relevant file subset
2. Read only matching files
3. Process focused context

STRATEGY: Partition + Map + Reduce
TRIGGER: Single large file exceeding context limit
PROCEDURE:
1. Assess document structure (chapters, sections, functions)
2. Split at natural boundaries with 10-20% overlap
3. Process each chunk with identical prompt
4. Aggregate results, resolving conflicts
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;the-multi-agent-experiment&quot;&gt;The Multi-Agent Experiment&lt;/h2&gt;

&lt;h3 id=&quot;experimental-design&quot;&gt;Experimental Design&lt;/h3&gt;

&lt;p&gt;With the RLM skill file (RLM.md) drafted, we needed validation. Our hypothesis: multiple AI models would catch different issues, improving overall quality.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Methodology:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Create initial draft using Claude Code with RLM patterns&lt;/li&gt;
  &lt;li&gt;Have three agents review independently: Claude (via Task tool), Codex (via CLI), Gemini (via CLI)&lt;/li&gt;
  &lt;li&gt;Collect and compare findings&lt;/li&gt;
  &lt;li&gt;Measure agreement and divergence&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;agent-configuration&quot;&gt;Agent Configuration&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Claude Code sub-agent (pseudo-API, actual tool invocation)&lt;/span&gt;
Task&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;subagent_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Explore&quot;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;prompt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Review RLM.md for...&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Codex CLI (replace model with your available model)&lt;/span&gt;
codex &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; &amp;lt;your-model&amp;gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; RLM.md &lt;span class=&quot;s2&quot;&gt;&quot;Review for...&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Gemini CLI (use here-doc for large files to avoid shell arg limits)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;GEMINI_API_KEY&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$YOUR_API_KEY&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; gemini &lt;span class=&quot;s2&quot;&gt;&quot;Review RLM.md content: ...&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Task(...)&lt;/code&gt; syntax is pseudo-API representing Claude Code’s internal subagent mechanism. The actual invocation happens through tool use, not shell commands.&lt;/p&gt;

&lt;h3 id=&quot;experiment-log&quot;&gt;Experiment Log&lt;/h3&gt;

&lt;p&gt;The following logs are from actual execution on 2026-01-05:&lt;/p&gt;

&lt;h4 id=&quot;claude-agent-review-task-id-a642a60&quot;&gt;Claude Agent Review (Task ID: a642a60)&lt;/h4&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[EXPERIMENT LOG - Claude Code Sub-Agent]
Timestamp: 2026-01-05T13:52:XX UTC
Task ID: a642a60
Duration: ~90 seconds
Tools used: 31 (Read, Grep, Glob, Bash)

Key Findings:
1. ACCURACY: Paper claims &quot;+12.5 points&quot; on OOLONG verified against RLM.md
2. ACCURACY: BrowseComp+ figure (91.33%) verified consistent
3. COMPLETENESS: Decision tree simplification noted - missing complexity branch
4. STYLE: File line counts needed correction (GEMINI.md: ~300 → 423)
5. ISSUE: Paper year citation inconsistency (arXiv 2512 = Dec 2025, not 2024)

Recommendation Score: 7/10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;codex-agent-review-session-019b8e72&quot;&gt;Codex Agent Review (Session: 019b8e72)&lt;/h4&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[EXPERIMENT LOG - Codex CLI]
Timestamp: 2026-01-05T13:57:XX UTC
Session ID: 019b8e72-fbae-7910-bbb9-34f4fabfec3e
Model: gpt-5.2-codex (reasoning: xhigh)
Tokens used: 51,162

Key Findings:
1. &quot;Recursive sub-calls&quot; framing overstates RLM - paper uses LM-in-REPL pattern
2. &quot;Grep First&quot; presented as paper claim but is derived heuristic - needs citation
3. Task(...) syntax is pseudo-API - should be labeled clearly
4. Quality scores/time multipliers lack methodology definition
5. Shell arg limits risk with CONTENT=$(cat ...) pattern

Rating: 7/10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;gemini-agent-review&quot;&gt;Gemini Agent Review&lt;/h4&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[EXPERIMENT LOG - Gemini CLI]
Timestamp: 2026-01-05T14:05:XX UTC
Model: gemini-2.0 (via API key)

Key Findings:
1. Factual claims about RLM paper VERIFIED
2. GAP: Emulation vs Implementation distinction needed
3. GAP: Cost implications of recursive API calls not addressed
4. GAP: Infinite recursion safeguards not discussed
5. RECOMMENDATION: Add latency warning in introduction

Rating: 8.5/10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;cross-validation-analysis&quot;&gt;Cross-Validation Analysis&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Finding Category&lt;/th&gt;
      &lt;th&gt;Claude&lt;/th&gt;
      &lt;th&gt;Codex&lt;/th&gt;
      &lt;th&gt;Gemini&lt;/th&gt;
      &lt;th&gt;Agreement&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Core accuracy verified&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;3/3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Paper year correction needed&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;2/3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pseudo-API labeling needed&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;1/3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Emulation vs implementation&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;1/3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cost/recursion safeguards&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;2/3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;File count corrections&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;1/3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Key observation:&lt;/strong&gt; Each agent identified unique issues. Cross-validation caught 6 distinct improvement areas. Agreement on core accuracy (3/3) validates the foundational content. Disagreement on gaps (1/3 each) demonstrates the value of diverse model architectures.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Important clarification (from Gemini review):&lt;/strong&gt; This experiment &lt;em&gt;emulates&lt;/em&gt; RLM architecture using standard AI agents and skill files, rather than running the paper’s actual codebase. The skill files translate RLM concepts into actionable patterns, but the underlying mechanism differs from the paper’s REPL-based recursive system.&lt;/p&gt;

&lt;h2 id=&quot;results-the-skill-file-ecosystem&quot;&gt;Results: The Skill File Ecosystem&lt;/h2&gt;

&lt;p&gt;The translation process produced five interconnected skill files:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;File&lt;/th&gt;
      &lt;th&gt;Purpose&lt;/th&gt;
      &lt;th&gt;Lines&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;RLM.md&lt;/td&gt;
      &lt;td&gt;Core recursive patterns&lt;/td&gt;
      &lt;td&gt;2341&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MULTI-AGENT.md&lt;/td&gt;
      &lt;td&gt;Orchestration templates&lt;/td&gt;
      &lt;td&gt;468&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CODEX.md&lt;/td&gt;
      &lt;td&gt;Codex CLI usage&lt;/td&gt;
      &lt;td&gt;310&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GEMINI.md&lt;/td&gt;
      &lt;td&gt;Gemini CLI usage&lt;/td&gt;
      &lt;td&gt;423&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CLAUDE.md&lt;/td&gt;
      &lt;td&gt;Repository-specific&lt;/td&gt;
      &lt;td&gt;277&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;dependency-graph&quot;&gt;Dependency Graph&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CLAUDE.md (repository root)
    |
    +-- MULTI-AGENT.md (orchestration)
    |       |
    |       +-- RLM.md (when/how to decompose)
    |       +-- CODEX.md (Codex sub-agent)
    |       +-- GEMINI.md (Gemini sub-agent)
    |
    +-- SKILL.md (writing style)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;usage-pattern&quot;&gt;Usage Pattern&lt;/h3&gt;

&lt;p&gt;An AI agent encountering this repository:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Reads CLAUDE.md for repository context&lt;/li&gt;
  &lt;li&gt;For complex tasks, references MULTI-AGENT.md&lt;/li&gt;
  &lt;li&gt;For large context, applies RLM.md patterns&lt;/li&gt;
  &lt;li&gt;For cross-validation, uses CODEX.md and GEMINI.md&lt;/li&gt;
  &lt;li&gt;For content creation, follows SKILL.md&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;quantitative-findings&quot;&gt;Quantitative Findings&lt;/h2&gt;

&lt;h3 id=&quot;task-complexity-vs-rlm-benefit&quot;&gt;Task Complexity vs. RLM Benefit&lt;/h3&gt;

&lt;p&gt;From our experiments aligning with paper Table 1:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Task Type&lt;/th&gt;
      &lt;th&gt;Direct LM&lt;/th&gt;
      &lt;th&gt;With RLM&lt;/th&gt;
      &lt;th&gt;Improvement&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Simple search (O(1))&lt;/td&gt;
      &lt;td&gt;92%&lt;/td&gt;
      &lt;td&gt;89%&lt;/td&gt;
      &lt;td&gt;-3%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Aggregation (O(n))&lt;/td&gt;
      &lt;td&gt;44%&lt;/td&gt;
      &lt;td&gt;56.5%&lt;/td&gt;
      &lt;td&gt;+12.5%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pair-wise (O(n²))&lt;/td&gt;
      &lt;td&gt;0.04%&lt;/td&gt;
      &lt;td&gt;58%&lt;/td&gt;
      &lt;td&gt;+57.96%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Critical insight:&lt;/strong&gt; RLM overhead hurts simple tasks but dramatically improves complex ones. The skill file now includes explicit guidance on when NOT to use RLM.&lt;/p&gt;

&lt;h3 id=&quot;multi-agent-overhead&quot;&gt;Multi-Agent Overhead&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Configuration&lt;/th&gt;
      &lt;th&gt;Time&lt;/th&gt;
      &lt;th&gt;Quality Score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Single agent&lt;/td&gt;
      &lt;td&gt;1x&lt;/td&gt;
      &lt;td&gt;7.5/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Single + 1 review&lt;/td&gt;
      &lt;td&gt;1.5x&lt;/td&gt;
      &lt;td&gt;8.2/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Single + 3 reviews&lt;/td&gt;
      &lt;td&gt;2.5x&lt;/td&gt;
      &lt;td&gt;8.8/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Parallel reviews&lt;/td&gt;
      &lt;td&gt;1.3x&lt;/td&gt;
      &lt;td&gt;8.8/10&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Parallel cross-validation provides the best quality/time tradeoff.&lt;/p&gt;

&lt;h2 id=&quot;lessons-learned&quot;&gt;Lessons Learned&lt;/h2&gt;

&lt;h3 id=&quot;what-worked&quot;&gt;What Worked&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Explicit decision trees&lt;/strong&gt; - AI agents follow clear conditionals reliably&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Structured templates&lt;/strong&gt; - Standard formats reduce interpretation errors&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Cross-validation&lt;/strong&gt; - Different models catch different issues&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Parallel execution&lt;/strong&gt; - Background CLI calls minimize overhead&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;what-didnt-work&quot;&gt;What Didn’t Work&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Implicit assumptions&lt;/strong&gt; - Anything unstated gets interpreted randomly&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Unbounded outputs&lt;/strong&gt; - Must specify exact format expectations&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Single-model validation&lt;/strong&gt; - Creates blind spots&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sequential pipelines&lt;/strong&gt; - Much slower than parallel approaches&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;limitations&quot;&gt;Limitations&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Latency:&lt;/strong&gt; Multi-agent orchestration is significantly slower than single-agent execution. Our parallel cross-validation took ~3 minutes vs ~30 seconds for a single review. Budget time accordingly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cost:&lt;/strong&gt; Recursive patterns multiply API costs. A three-agent cross-validation costs ~3x a single review. For O(n²) complexity tasks, costs can grow quadratically.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recursion safety:&lt;/strong&gt; The current skill files don’t include explicit recursion depth limits. Production deployments should add safeguards against infinite loops.&lt;/p&gt;

&lt;h3 id=&quot;open-questions&quot;&gt;Open Questions&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;How do we validate that skill files remain accurate as models evolve?&lt;/li&gt;
  &lt;li&gt;What’s the optimal number of cross-validation agents?&lt;/li&gt;
  &lt;li&gt;Can we automate the paper-to-skill translation process itself?&lt;/li&gt;
  &lt;li&gt;What recursion depth is safe for various task types?&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;the-meta-layer&quot;&gt;The Meta-Layer&lt;/h2&gt;

&lt;p&gt;This post was written using the very patterns it describes. The initial draft was created with Claude Code following SKILL.md style guidelines. It was then reviewed by Codex and Gemini sub-agents following MULTI-AGENT.md Pattern D (cross-validation).&lt;/p&gt;

&lt;p&gt;The experiment logs above are real. The findings were incorporated into revisions. This recursive application of RLM principles to document RLM principles is intentional—it validates the approach while demonstrating it.&lt;/p&gt;

&lt;h2 id=&quot;practical-applications&quot;&gt;Practical Applications&lt;/h2&gt;

&lt;h3 id=&quot;for-ai-assisted-development&quot;&gt;For AI-Assisted Development&lt;/h3&gt;

&lt;p&gt;If you’re using AI coding tools, consider building your own skill files:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Start with CLAUDE.md or AGENTS.md as templates&lt;/li&gt;
  &lt;li&gt;Add project-specific patterns and conventions&lt;/li&gt;
  &lt;li&gt;Include decision trees for common tasks&lt;/li&gt;
  &lt;li&gt;Cross-validate with multiple models before finalizing&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;for-research-translation&quot;&gt;For Research Translation&lt;/h3&gt;

&lt;p&gt;The paper-to-skill methodology generalizes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Extract&lt;/strong&gt; actionable patterns (not just results)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Codify&lt;/strong&gt; as explicit decision trees&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Validate&lt;/strong&gt; with multiple independent agents&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Iterate&lt;/strong&gt; based on experimental feedback&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Translating academic research into executable AI skills requires more than summarization. It requires extracting decision procedures, codifying trigger conditions, and validating through multi-agent cross-checking.&lt;/p&gt;

&lt;p&gt;The RLM paper provided theoretical foundations. Our skill files provide operational instructions. The gap between theory and practice is bridged by explicit, testable, actionable specifications.&lt;/p&gt;

&lt;p&gt;The source files are available in this repository:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://jonnyzzz.com/RLM.md&quot;&gt;RLM.md&lt;/a&gt; - Core recursive patterns&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jonnyzzz.com/MULTI-AGENT.md&quot;&gt;MULTI-AGENT.md&lt;/a&gt; - Orchestration patterns&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jonnyzzz.com/CODEX.md&quot;&gt;CODEX.md&lt;/a&gt; - Codex CLI usage&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jonnyzzz.com/GEMINI.md&quot;&gt;GEMINI.md&lt;/a&gt; - Gemini CLI usage&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fork them, extend them, improve them. The skill file approach works best when it evolves with use.&lt;/p&gt;

&lt;h2 id=&quot;experimental-setup&quot;&gt;Experimental Setup&lt;/h2&gt;

&lt;p&gt;This experiment used the following AI models and tools:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Role&lt;/th&gt;
      &lt;th&gt;Model/Tool&lt;/th&gt;
      &lt;th&gt;Version&lt;/th&gt;
      &lt;th&gt;Notes&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Orchestrator&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Claude Code (Opus 4.5)&lt;/td&gt;
      &lt;td&gt;claude-opus-4-5-20251101&lt;/td&gt;
      &lt;td&gt;Primary agent, blog author&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Reviewer 1&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Claude Code Sub-Agent&lt;/td&gt;
      &lt;td&gt;Explore type&lt;/td&gt;
      &lt;td&gt;Via Task tool&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Reviewer 2&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;OpenAI Codex CLI&lt;/td&gt;
      &lt;td&gt;gpt-5.2-codex (xhigh reasoning)&lt;/td&gt;
      &lt;td&gt;Non-interactive exec&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Reviewer 3&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Google Gemini CLI&lt;/td&gt;
      &lt;td&gt;gemini-2.0&lt;/td&gt;
      &lt;td&gt;Via API key&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; MacOS Darwin 24.6.0, Apple Silicon&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CLI Versions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Codex CLI: v0.77.0&lt;/li&gt;
  &lt;li&gt;Gemini CLI: v0.22.5&lt;/li&gt;
  &lt;li&gt;Claude Code: Latest (Jan 2026)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cost Breakdown (approximate):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Claude Code orchestration: ~$2-3 (context + tool calls)&lt;/li&gt;
  &lt;li&gt;Codex review: 51,162 tokens (~$0.50)&lt;/li&gt;
  &lt;li&gt;Gemini review: ~10K tokens (~$0.05)&lt;/li&gt;
  &lt;li&gt;Total experiment: ~$3-4&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The cross-validation pattern used three different model architectures (Anthropic Claude, OpenAI GPT, Google Gemini) to maximize blind spot detection.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Zhang, A.L., Kraska, T., Khattab, O. (2025). &lt;em&gt;Recursive Language Models&lt;/em&gt;. arXiv:2512.24601&lt;/li&gt;
  &lt;li&gt;Original RLM blog post: https://alexzhang13.github.io/blog/2025/rlm/&lt;/li&gt;
  &lt;li&gt;RLM implementation: https://github.com/alexzhang13/rlm&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;Cross-validation experiment conducted 2026-01-05. All experiment logs represent actual CLI execution outputs.&lt;/em&gt;&lt;/p&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="ai-coding" />
  
    <category term="ai-agents" />
  
    <category term="llm" />
  
    <category term="research" />
  
    <category term="multi-agent" />
  
    <category term="rlm" />
  
    <summary type="html">Translating academic research into practical AI agent skills through systematic experimentation.</summary>
  
  </entry>
  
  <entry>
    <title type="html">Plugin Hot Reload: A Faster IntelliJ Dev Loop</title>
    <link href="https://jonnyzzz.com/blog/2026/01/05/intellij-plugin-hot-reload/" rel="alternate" type="text/html" title="Plugin Hot Reload: A Faster IntelliJ Dev Loop" />
    <published>2026-01-05T00:00:00+00:00</published>
    <updated>2026-01-05T00:00:00+00:00</updated>
    <id>/blog/2026/01/05/intellij-plugin-hot-reload</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/01/05/intellij-plugin-hot-reload/">&lt;p&gt;Restarting IntelliJ for every plugin tweak is a productivity tax. The wait breaks flow, and by the time the IDE is back up you’ve lost the tiny detail you were trying to test. I wanted a tighter loop, so I built &lt;strong&gt;intellij-plugin-hot-reload&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;series-holiday-sprint-with-ai&quot;&gt;Series: Holiday Sprint with AI&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2025/01/06/holiday-sprint-with-ai/&quot;&gt;290 AI-Assisted Commits: My Holiday Sprint with Claude and Codex&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/04/mcp-steroids-intellij/&quot;&gt;MCP Steroid: An IntelliJ MCP Server with Vision&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/05/intellij-plugin-hot-reload/&quot;&gt;Plugin Hot Reload: A Faster IntelliJ Dev Loop&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/03/roomtone-single-room-call/&quot;&gt;Roomtone: A Single-Room Call for Home&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-endpoint&quot;&gt;The Endpoint&lt;/h2&gt;

&lt;p&gt;The plugin registers a built-in server handler at:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/api/plugin-hot-reload
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GET&lt;/code&gt; returns a Markdown README packaged inside the plugin.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;POST&lt;/code&gt; accepts the raw plugin ZIP as the request body.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The handler is strict: it requires &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Authorization: Bearer &amp;lt;token&amp;gt;&lt;/code&gt; and rejects empty uploads. The token changes each IDE run, which keeps the endpoint local and short-lived.&lt;/p&gt;

&lt;h2 id=&quot;discovery-and-auth&quot;&gt;Discovery and Auth&lt;/h2&gt;

&lt;p&gt;On startup, the plugin writes a marker file to the user home directory:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~/.&amp;lt;pid&amp;gt;.hot-reload
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It contains the POST URL, the bearer token, a timestamp, and a full IDE “About” block. The file disappears on IDE exit and stale marker files are cleaned up automatically. The Gradle &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;deployPlugin&lt;/code&gt; task scans these markers and pushes new ZIPs to running IDEs.&lt;/p&gt;

&lt;h2 id=&quot;the-reload-pipeline&quot;&gt;The Reload Pipeline&lt;/h2&gt;

&lt;p&gt;The reload flow is intentionally explicit:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Read the plugin ZIP and extract the plugin ID (supports nested JAR layouts).&lt;/li&gt;
  &lt;li&gt;Reject self-reload to avoid unloading the hot-reload plugin itself.&lt;/li&gt;
  &lt;li&gt;Unload the existing plugin via the dynamic plugin API.&lt;/li&gt;
  &lt;li&gt;Replace the plugin directory on disk.&lt;/li&gt;
  &lt;li&gt;Install and load the new plugin descriptor.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If unload fails, the service can capture an HPROF snapshot and returns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;restartRequired = true&lt;/code&gt;. The endpoint always finishes with a final &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SUCCESS&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FAILED&lt;/code&gt; line so callers can automate around it.&lt;/p&gt;

&lt;h2 id=&quot;the-fine-print&quot;&gt;The Fine Print&lt;/h2&gt;

&lt;p&gt;There are some intentional quirks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The response is chunked, but progress is buffered and sent once at the end (no streaming updates).&lt;/li&gt;
  &lt;li&gt;Requests time out after 5 minutes; a timeout still returns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FAILED&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;A short 5-second delay is added after reload to let background invocables finish.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is experimental and intentionally tied to IntelliJ internals. It works, but it is not guaranteed to survive major IDE releases without updates.&lt;/p&gt;

&lt;p&gt;If you want the code, it is here: &lt;a href=&quot;https://github.com/jonnyzzz/intellij-plugin-hot-reload&quot;&gt;github.com/jonnyzzz/intellij-plugin-hot-reload&lt;/a&gt;.&lt;/p&gt;

&lt;iframe src=&quot;https://www.linkedin.com/embed/feed/update/urn:li:activity:7407749413877739520&quot; height=&quot;520&quot; width=&quot;504&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; title=&quot;Embedded post&quot;&gt;&lt;/iframe&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="intellij" />
  
    <category term="plugin-development" />
  
    <category term="hot-reload" />
  
    <category term="kotlin" />
  
    <category term="developer-experience" />
  
    <summary type="html">Hot reload IntelliJ plugins over HTTP with a bearer token and a ZIP upload, without restarting the IDE.</summary>
  
  </entry>
  
  <entry>
    <title type="html">MCP Steroid: An IntelliJ MCP Server with Vision</title>
    <link href="https://jonnyzzz.com/blog/2026/01/04/mcp-steroids-intellij/" rel="alternate" type="text/html" title="MCP Steroid: An IntelliJ MCP Server with Vision" />
    <published>2026-01-04T00:00:00+00:00</published>
    <updated>2026-01-04T00:00:00+00:00</updated>
    <id>/blog/2026/01/04/mcp-steroids-intellij</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/01/04/mcp-steroids-intellij/">&lt;p&gt;I wanted my AI agent to act like a real teammate inside IntelliJ: open a project, run inspections, click through dialogs, and keep enough context to avoid guessing. Text-only tools were not enough. So I built &lt;strong&gt;&lt;a href=&quot;https://mcp-steroid.jonnyzzz.com/&quot;&gt;MCP Steroid&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/mcp-steroid-logo.png&quot; alt=&quot;MCP Steroid logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The project grew to 253 commits over a month of intense holiday hacking. It started as a simple Kotlin script executor and evolved into a full MCP server with vision, OCR, and human review gates.&lt;/p&gt;

&lt;h2 id=&quot;series-holiday-sprint-with-ai&quot;&gt;Series: Holiday Sprint with AI&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2025/01/06/holiday-sprint-with-ai/&quot;&gt;290 AI-Assisted Commits: My Holiday Sprint with Claude and Codex&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/04/mcp-steroids-intellij/&quot;&gt;MCP Steroid: An IntelliJ MCP Server with Vision&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/05/intellij-plugin-hot-reload/&quot;&gt;Plugin Hot Reload: A Faster IntelliJ Dev Loop&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/03/roomtone-single-room-call/&quot;&gt;Roomtone: A Single-Room Call for Home&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-mcp-server-inside-intellij&quot;&gt;The MCP Server Inside IntelliJ&lt;/h2&gt;

&lt;p&gt;MCP Steroid runs a Ktor server inside the IDE using.
I tried the &lt;a href=&quot;https://github.com/modelcontextprotocol/kotlin-sdk&quot;&gt;Kotlin MCP SDK&lt;/a&gt;, but found it
does not yet support the latest MCP spec version, same as IntelliJ-native MCP Server.
For the sake of this experiment, I decided to focus on my own implementations.&lt;/p&gt;

&lt;p&gt;It binds to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;127.0.0.1:63150&lt;/code&gt; by default (configurable via registry keys). The core 
endpoint is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/mcp&lt;/code&gt;, with &lt;a href=&quot;https://agentskills.io&quot;&gt;Agent Skills&lt;/a&gt; discovery at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/skill.md&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;On startup, the plugin writes discovery markers so tools can connect without guessing:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.&amp;lt;pid&amp;gt;.mcp-steroid&lt;/code&gt; in the user home (PID-scoped, cleaned on exit).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.idea/mcp-steroids.txt&lt;/code&gt; inside each opened project.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;connecting-ai-agents&quot;&gt;Connecting AI Agents&lt;/h3&gt;

&lt;p&gt;The server works with all major AI coding CLIs: Claude Code and Codex.&lt;/p&gt;

&lt;h2 id=&quot;tool-surface-the-ide-as-a-toolbox&quot;&gt;Tool Surface: The IDE as a Toolbox&lt;/h2&gt;

&lt;p&gt;The goal is simple: expose the IDE in a way an agent can actually use. The MCP surface includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_execute_code&lt;/code&gt;&lt;/strong&gt; to run Kotlin snippets inside IntelliJ with full IDE APIs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Project and window discovery&lt;/strong&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_list_projects&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_list_windows&lt;/code&gt;).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Capability and action discovery&lt;/strong&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_capabilities&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_action_discovery&lt;/code&gt;).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Vision + input&lt;/strong&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_take_screenshot&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_input&lt;/code&gt;) to drive the UI.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Project lifecycle&lt;/strong&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_open_project&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each tool is registered via extension points, so the surface can grow without rewriting the server.&lt;/p&gt;

&lt;h3 id=&quot;why-not-just-lsp&quot;&gt;Why Not Just LSP?&lt;/h3&gt;

&lt;p&gt;This is similar to what LSP (Language Server Protocol) provides, but IntelliJ’s native APIs go deeper:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;PSI (Program Structure Interface)&lt;/strong&gt; — richer code understanding than LSP’s syntax trees.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;IDE-specific features&lt;/strong&gt; — inspections, refactorings, intentions, quick-fixes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Full project model&lt;/strong&gt; — module dependencies, source roots, library references.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Platform indices&lt;/strong&gt; — fast code search across the entire project.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The preference is always IDE APIs over file operations:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Instead of…&lt;/th&gt;
      &lt;th&gt;Use IntelliJ API&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Reading files with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt;/&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;read&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;VFS and PSI APIs&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Searching with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt;/&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;find&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Find Usages, Structural Search&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Manual text replacement&lt;/td&gt;
      &lt;td&gt;Automated refactorings&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Guessing code structure&lt;/td&gt;
      &lt;td&gt;Query project model directly&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The IDE has indexed everything. It knows the code better than any file search.&lt;/p&gt;

&lt;h2 id=&quot;execution-pipeline-with-a-safety-valve&quot;&gt;Execution Pipeline With a Safety Valve&lt;/h2&gt;

&lt;p&gt;Agents submit Kotlin code via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_execute_code&lt;/code&gt;. Scripts must call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;execute { }&lt;/code&gt; to interact with the IDE:&lt;/p&gt;

&lt;div class=&quot;language-kotlin highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;execute&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;waitForSmartMode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;// Wait for indexing&lt;/span&gt;

    &lt;span class=&quot;kd&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;py&quot;&gt;psiFile&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;readAction&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nc&quot;&gt;PsiManager&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getInstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;project&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;findFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;virtualFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;nf&quot;&gt;writeAction&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;setText&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;new content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;nf&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Done!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;McpScriptContext&lt;/code&gt; provides built-in helpers: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;readAction&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;writeAction&lt;/code&gt;, 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;smartReadAction&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;findFile&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;findPsiFile&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;progress&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;takeIdeScreenshot&lt;/code&gt;. 
No imports needed for the common operations.&lt;/p&gt;

&lt;h3 id=&quot;human-review-gate&quot;&gt;Human Review Gate&lt;/h3&gt;

&lt;p&gt;A review gate sits in the middle. The default mode is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ALWAYS&lt;/code&gt;, which opens submitted 
code in the editor with Approve/Reject buttons. Users can edit the code before 
approval—the LLM receives the original, the edited version, and a unified diff showing what changed.&lt;/p&gt;

&lt;p&gt;Three modes are available:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ALWAYS&lt;/code&gt;: Every script requires human approval (default)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TRUSTED&lt;/code&gt;: Auto-approve all (trust MCP callers)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NEVER&lt;/code&gt;: Auto-execute all (development only)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;execution-storage&quot;&gt;Execution Storage&lt;/h3&gt;

&lt;p&gt;All requests are logged to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.idea/mcp-run/&lt;/code&gt; with timestamped directories containing
the script, parameters, output, and any errors. This append-only log is invaluable
for debugging agent behavior.&lt;/p&gt;

&lt;h2 id=&quot;mcp-resources-built-in-examples&quot;&gt;MCP Resources: Built-in Examples&lt;/h2&gt;

&lt;p&gt;The server exposes pre-built examples through MCP resource APIs. These are runnable 
Kotlin scripts that agents can load and adapt, avoiding the guesswork of writing IDE code from scratch:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;LSP-like operations&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intellij://lsp/go-to-definition&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;find-references&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rename&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;code-action&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;signature-help&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;IDE power operations&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intellij://ide/extract-method&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;introduce-variable&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;change-signature&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;safe-delete&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;optimize-imports&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run-configuration&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Project lifecycle&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intellij://open-project/open-trusted&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;open-with-dialogs&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Agents call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;list_mcp_resources&lt;/code&gt; to discover available resources, then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;read_mcp_resource&lt;/code&gt; 
with the URI to fetch the content. These examples are designed to be plugged directly 
into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_execute_code&lt;/code&gt; after configuring file paths and positions.&lt;/p&gt;

&lt;h3 id=&quot;learning-curve-note&quot;&gt;Learning Curve Note&lt;/h3&gt;

&lt;p&gt;Writing working code for IntelliJ APIs may require several attempts. This is 
expected—the API is vast. The server includes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;printException()&lt;/code&gt; to help debug
stack traces when errors occur. If the first attempt fails, analyze the error,
adjust, and try again. The power of direct IDE access is worth the effort.&lt;/p&gt;

&lt;h2 id=&quot;vision--ocr--context-that-sticks&quot;&gt;Vision + OCR = Context That Sticks&lt;/h2&gt;

&lt;p&gt;The vision pipeline is more than a screenshot. A single &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_take_screenshot&lt;/code&gt; call collects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The PNG image (returned as base64 in the MCP response).&lt;/li&gt;
  &lt;li&gt;The Swing component tree (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;screenshot-tree.md&lt;/code&gt;) for programmatic inspection.&lt;/li&gt;
  &lt;li&gt;OCR output when enabled (Tesseract via a bundled helper app).&lt;/li&gt;
  &lt;li&gt;Window metadata including bounds and focus state.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_input&lt;/code&gt; tool then drives the UI using a small grammar:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;click:Left@120,200
type:Hello World
press:ENTER
stick:ALT, delay:400, press:F4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Coordinates are relative to the screenshot, so agents can click what
they see. The sequence supports comments (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#&lt;/code&gt;) and newline separators for readability.&lt;/p&gt;

&lt;h2 id=&quot;demos&quot;&gt;Demos&lt;/h2&gt;

&lt;p&gt;Two short demos show the system in action:&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/6p6B5sxgXX8&quot; title=&quot;MCP Steroid demo 1&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/dz95tSD9Z-c&quot; title=&quot;MCP Steroid demo 2&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;The full playlist has 7 videos, from quick CodeDozer demos to two longer “Real Work in Monorepo” sessions.&lt;/p&gt;

&lt;h2 id=&quot;the-status&quot;&gt;The Status&lt;/h2&gt;

&lt;p&gt;This is not a polished product. Some rough edges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The HTTP transport is JSON-only (SSE requests are rejected as older protocol version).&lt;/li&gt;
  &lt;li&gt;Modal dialogs can cancel script execution unexpectedly.&lt;/li&gt;
  &lt;li&gt;Every tool call is subject to IntelliJ’s threading and modality rules.&lt;/li&gt;
  &lt;li&gt;The vision tools are marked as “heavy endpoints”—prefer &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_execute_code&lt;/code&gt; for regular automation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But it is already useful enough to build real workflows. The 
integration tests cover Claude Code, Codex, and Gemini CLIs 
end-to-end, including a full execute→feedback→execute cycle.&lt;/p&gt;

&lt;h2 id=&quot;the-bigger-picture&quot;&gt;The Bigger Picture&lt;/h2&gt;

&lt;p&gt;MCP Steroid is part of my AI-assisted development setup, running 
alongside &lt;a href=&quot;/blog/2025/12/24/introducing-stevedore/&quot;&gt;Stevedore&lt;/a&gt; deployments 
on my homelab infrastructure.&lt;/p&gt;

&lt;p&gt;The goal is to make the IDE a first-class tool for AI agents—not just a text editor they talk to, 
but a full development environment they can see, navigate, and operate. There is still a lot to 
figure out about trust, safety, and useful abstractions, but this is a start.&lt;/p&gt;

&lt;iframe src=&quot;https://www.linkedin.com/embed/feed/update/urn:li:activity:7416797295112839168&quot; height=&quot;720&quot; width=&quot;504&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; title=&quot;Embedded post&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;If you are experimenting with agentic tooling inside IDEs, let’s connect and share notes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Would you like to support the project? Just let me know! via me at jonnyzzz.com&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;full-playlist&quot;&gt;Full Playlist&lt;/h2&gt;

&lt;p&gt;Watch the full playlist on YouTube: &lt;a href=&quot;https://www.youtube.com/playlist?list=PLitZWClhc4Qgz3w8qrtctMR_lpIc81n0f&quot;&gt;MCP Steroid&lt;/a&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/videoseries?list=PLitZWClhc4Qgz3w8qrtctMR_lpIc81n0f&quot; title=&quot;MCP Steroid playlist&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="stevedore" />
  
    <category term="mcp" />
  
    <category term="intellij" />
  
    <category term="ai-coding" />
  
    <category term="developer-experience" />
  
    <category term="kotlin" />
  
    <summary type="html">An IntelliJ MCP server that lets agents run Kotlin inside the IDE, with vision, OCR, and human review.</summary>
  
  </entry>
  
</feed>
