<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xml" href="https://jonnyzzz.com/feed.xslt.xml"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="https://jonnyzzz.com/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://jonnyzzz.com/" rel="alternate" type="text/html" />
  <updated>2026-01-26T08:51:20+00:00</updated>
  <id>/</id>

  
  <title type="html">Eugene Petrenko</title>
  

  
  <subtitle>Founding Engineering Leader | Agentic AI DevTools &amp; Experience</subtitle>
  

  

  
  
  <entry>
    <title type="html">How 16 AI Agents Fixed Our Documentation Problem</title>
    <link href="https://jonnyzzz.com/blog/2026/01/24/16-ai-agents-documentation-refactor/" rel="alternate" type="text/html" title="How 16 AI Agents Fixed Our Documentation Problem" />
    <published>2026-01-24T00:00:00+00:00</published>
    <updated>2026-01-24T00:00:00+00:00</updated>
    <id>/blog/2026/01/24/16-ai-agents-documentation-refactor</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/01/24/16-ai-agents-documentation-refactor/">&lt;p&gt;I’ve been documenting multi-agent orchestration patterns for months, and I had no idea how bad our docs had become
until I asked our customers — AI agents, and they were brutally honest.&lt;/p&gt;

&lt;p&gt;Our documentation had grown to 2,648 lines across four files. The most important commands were buried at line 91 of
CLAUDE-CODE.md, after 90 lines of introductions, explanations, and edge cases. We’d duplicated entire sections across
three CLI docs. And when AI agents tried to use these docs to spawn other AI agents, they struggled.&lt;/p&gt;

&lt;p&gt;Here’s the recursive twist: I sent agents to interview agents about documentation for spawning agents. We used
multi-agent orchestration to fix documentation about multi-agent orchestration. The tools improved themselves.&lt;/p&gt;

&lt;p&gt;This is the story of how 16 AI agents helped us refactor documentation that was both about them and for them. This was
attempt #3, and we finally got it right by doing something simple: we stopped guessing and started asking.&lt;/p&gt;

&lt;h2 id=&quot;the-bigger-picture&quot;&gt;The Bigger Picture&lt;/h2&gt;

&lt;p&gt;This wasn’t just about fixing documentation. It was about proving something I’d suspected but couldn’t demonstrate:
multi-agent orchestration works for real engineering problems, not just toy examples.&lt;/p&gt;

&lt;p&gt;I’ve been experimenting with multi-agent patterns for months, documenting the RLM methodology, building orchestration
guides. But until this project, I hadn’t stress-tested it on something complex and messy. Documentation refactoring is
exactly that - subjective, iterative, full of judgment calls.&lt;/p&gt;

&lt;p&gt;Here’s where it gets meta-recursive: we used agents to spawn agents to interview agents about documentation that
teaches agents how to spawn agents. The documentation problem was its own solution. We needed better docs for
multi-agent orchestration, so we used multi-agent orchestration to improve those docs. The methodology ate its own
dogfood and came out stronger.&lt;/p&gt;

&lt;p&gt;The fact that 16 AI agents could analyze, interview, implement, and validate better than I could alone? That changes
how I think about engineering workflows. But more importantly, the agents-spawning-agents-to-fix-agent-docs approach
proved the recursive viability of the entire pattern.&lt;/p&gt;

&lt;p&gt;We spawned 16 agents across four distinct phases. We ran them in parallel when possible, sequentially when necessary.
We treated AI agents as actual customers and conducted structured interviews to understand their needs. We validated
the results with fresh agents who had no prior context. Some of those interview agents spawned their own sub-agents to
dig deeper into specific sections.&lt;/p&gt;

&lt;p&gt;The outcome? Measurable improvements: 39% shorter documentation, 15% higher quality ratings, 5x faster navigation to
key commands. Production-ready output that both humans and AI agents can use effectively.&lt;/p&gt;

&lt;p&gt;But the bigger lesson is about treating AI agents as customers. They can’t be polite out of social obligation. They
won’t pretend to understand confusing documentation. They’ll tell you exactly where you buried the lead, what’s
missing, and what’s duplicated. They’re brutally honest feedback machines. And when they give feedback about their own
operational instructions? That’s when the recursion gets really interesting.&lt;/p&gt;

&lt;h2 id=&quot;why-this-mattered&quot;&gt;Why This Mattered&lt;/h2&gt;

&lt;h3 id=&quot;the-dual-role-problem&quot;&gt;The Dual Role Problem&lt;/h3&gt;

&lt;p&gt;We’d written 2,648 lines of documentation across four files: CLAUDE-CODE.md, CODEX.md, GEMINI.md, and MULTI-AGENT.md. 
Standard developer docs, right? Not quite.&lt;/p&gt;

&lt;p&gt;The twist: this documentation was both &lt;strong&gt;about&lt;/strong&gt; AI agents and &lt;strong&gt;for&lt;/strong&gt; AI agents. Human developers would read it to 
learn how to spawn sub-agents. But more importantly, the AI agents themselves would read these docs when they needed to 
spawn other AI agents. An agent working on your behalf would need to parse these instructions, understand the patterns, 
and correctly invoke another agent with the right commands and flags.&lt;/p&gt;

&lt;p&gt;If the documentation was confusing, ambiguous, or poorly structured, the AI wouldn’t just get frustrated and ask for 
clarification. It would hallucinate. It would guess. It would burn the context and tokens to figure out the right
parameters. It would use the wrong flags or miss critical setup steps. The 
documentation quality directly impacted whether agent-to-agent orchestration actually worked.&lt;/p&gt;

&lt;h3 id=&quot;why-ai-agents-care-about-documentation-quality&quot;&gt;Why AI Agents Care About Documentation Quality&lt;/h3&gt;

&lt;p&gt;AI agents aren’t just slower readers than humans - they have fundamentally different constraints:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Token costs are real.&lt;/strong&gt; Reading 2,648 lines of documentation translates to roughly 40,000 tokens. Every time an agent 
needed to spawn a sub-agent, it would load these docs into context. That’s not just slow, it’s expensive. Token costs 
compound across every operation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Context limits matter.&lt;/strong&gt; Even with 200K token context windows, shorter is better. The more documentation an agent has 
to process, the slower it responds and the more likely it is to miss crucial details buried in the middle.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Precision matters more than style.&lt;/strong&gt; Ambiguity that humans can resolve through intuition becomes a hallucination risk 
for AI. When documentation says three commands are “RECOMMENDED” with no clear priority, humans might make an educated 
guess. AI agents will pick the first one they see, or worse, combine flags in creative ways that don’t work.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Navigation is harder.&lt;/strong&gt; Humans can Ctrl+F for keywords or skim section headers. AI agents must parse the document 
sequentially, building a mental model as they go. If the most important information is at line 91, they’ve already 
spent 90 lines of processing budget on preliminaries.&lt;/p&gt;

&lt;h3 id=&quot;the-meta-challenge&quot;&gt;The Meta-Challenge&lt;/h3&gt;

&lt;p&gt;Here’s what made this interesting: we needed documentation good enough that an AI reading it could correctly spawn 
another AI, which might then need to spawn yet another AI. The clarity requirements weren’t just high - they were 
recursive. Every ambiguity would compound across agent hierarchies.&lt;/p&gt;

&lt;p&gt;One validator put it perfectly:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Strong operational doc. With recommended command moved to top and error handling added, would be 9/10.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That quote came from an AI agent evaluating the documentation. And it nailed exactly what was missing: quick access to 
the essential commands and proper error handling patterns.&lt;/p&gt;

&lt;p&gt;This was our third attempt. The first two? We tested on agents, and improved with agents with various prompts. 
Each next iteration creates a more improved outcome. But I would not run it too many times to avoid a drift
from the main focus. I’d optimized for completeness over usability, documenting every option and edge case
while forgetting to answer the only question that mattered: “What command do I run right now?”&lt;/p&gt;

&lt;p&gt;What changed? We stopped assuming we knew what good documentation looked like and started treating AI agents as actual 
customers who could tell us. They knew what they needed better than I did.&lt;/p&gt;

&lt;h2 id=&quot;what-the-interviews-revealed&quot;&gt;What the Interviews Revealed&lt;/h2&gt;

&lt;p&gt;The interview data surprised the root agent — seven agents, completely independent, and they all complained 
about the same five things. That kind of consistency doesn’t lie.&lt;/p&gt;

&lt;h3 id=&quot;pain-point-1-buried-the-lead&quot;&gt;Pain Point #1: Buried the Lead&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Severity: HIGH (reported by 5/7 interviews)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This was the killer. I’d buried our most important command at line 91 in CLAUDE-CODE.md, after 90 lines of 
introductions, prerequisites, and edge cases.&lt;/p&gt;

&lt;p&gt;From Interview #1:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Most important command (line 91) hidden after 90 lines of setup. Users want copy-paste immediately.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Looking back, I see exactly what went wrong. I wrote the docs the way I’d learned to write academic papers: 
introduction, background, methodology, results. But documentation isn’t a paper. Users want the answer immediately.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Before:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Introduction]
[Background]
[Architecture overview]
[Installation]
[Configuration]
[Prerequisites]
...
Line 91: # RECOMMENDED: Use this command...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;After:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## Purpose
Command-line interface for spawning Claude Code sub-agents.

### Quick Start
# THIS IS THE ONE COMMAND YOU NEED
claude -p --tools default --permission-mode dontAsk &quot;prompt&quot; 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Impact:&lt;/strong&gt; Time-to-command dropped from 60-90 seconds to 10-15 seconds. For an AI agent reading sequentially, that’s a 
5x improvement. For a human developer scanning the page, it’s the difference between finding what you need and giving 
up.&lt;/p&gt;

&lt;h3 id=&quot;pain-point-2-error-handling-completely-absent&quot;&gt;Pain Point #2: Error Handling Completely Absent&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Severity: HIGH (4/7 interviews)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Not a single retry example. No timeout recommendations. Zero guidance on detecting token limits or API errors. No exit 
code documentation. Our docs assumed success on first try, which is approximately never how real agent orchestration 
works.&lt;/p&gt;

&lt;p&gt;Agent feedback from Interview #4:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“No guidance on detecting token limits/API errors. No concrete retry example with exit code checking.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This wasn’t an edge case concern. Four separate agents independently identified it as a blocker. When your sub-agent 
hits a rate limit at 3 AM, you need to know: retry or fail? How long to wait? What exit code means “try again” vs “give 
up”?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The fix:&lt;/strong&gt; Added Part 9 to MULTI-AGENT.md with production-ready error handling patterns:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;MAX_RETRIES&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3
&lt;span class=&quot;nv&quot;&gt;RETRY_COUNT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0
&lt;span class=&quot;nv&quot;&gt;TIMEOUT_SEC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;900

&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$RETRY_COUNT&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-lt&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$MAX_RETRIES&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do
  if &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;timeout&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$TIMEOUT_SEC&lt;/span&gt; claude &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tools&lt;/span&gt; default &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
     &lt;span class=&quot;nt&quot;&gt;--permission-mode&lt;/span&gt; dontAsk &lt;span class=&quot;s2&quot;&gt;&quot;prompt&quot;&lt;/span&gt; 2&amp;gt;&amp;amp;1&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;then
    &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;SUCCESS&quot;&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;break
  &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;fi
  &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;EXIT_CODE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$?&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Attempt &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$((&lt;/span&gt;RETRY_COUNT+1&lt;span class=&quot;k&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$MAX_RETRIES&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; failed (exit &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$EXIT_CODE&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;)&quot;&lt;/span&gt;
  &lt;span class=&quot;nv&quot;&gt;RETRY_COUNT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$((&lt;/span&gt;RETRY_COUNT &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;))&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;sleep&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;$((&lt;/span&gt;RETRY_COUNT &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;))&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# Exponential backoff: 2s, 4s, 6s&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The new section includes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Retry logic with exponential backoff (2s, 4s, 6s delays)&lt;/li&gt;
  &lt;li&gt;Timeout recommendations by task type (30s for simple queries, 600s for complex analysis)&lt;/li&gt;
  &lt;li&gt;Exit code meanings (0 = success, 1 = general error, 41 = rate limit, 124 = timeout)&lt;/li&gt;
  &lt;li&gt;Token limit detection patterns (grep for specific error messages)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pain-point-3-authentication-steps-missing&quot;&gt;Pain Point #3: Authentication Steps Missing&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Severity: HIGH (3/7 interviews)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;CODEX.md referenced &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.codex/auth.json&lt;/code&gt; in multiple places but never explained how to create it. I’d assumed you’d 
already authenticated before reading the docs.&lt;/p&gt;

&lt;p&gt;Interview #6 agent feedback:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Document never explains how auth.json is created. Assumes user already authenticated.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is the classic curse of knowledge. As the person who wrote the docs, I already had &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auth.json&lt;/code&gt; on my machine. I’d 
logged in months ago. The auth step was invisible to me, but catastrophic for new users.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The fix:&lt;/strong&gt; Added Prerequisites sections to all three CLI docs with step-by-step auth flow:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Verify installation: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;codex --version&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Authenticate: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;codex login&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Confirm: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ls ~/.codex/auth.json&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Test: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;codex exec &quot;hello&quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Plus a troubleshooting table for common issues:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Symptom&lt;/th&gt;
      &lt;th&gt;Diagnosis&lt;/th&gt;
      &lt;th&gt;Fix&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auth.json not found&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Not logged in yet&lt;/td&gt;
      &lt;td&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;codex login&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;invalid credentials&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Token expired&lt;/td&gt;
      &lt;td&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;codex login&lt;/code&gt; again&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;permission denied&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Wrong file permissions&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;chmod 600 ~/.codex/auth.json&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;pain-point-4-20-token-overhead-from-duplication&quot;&gt;Pain Point #4: 20% Token Overhead from Duplication&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Severity: HIGH (reported by 2/7 interviews, measured by our team)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We’d copy-pasted entire sections across CLAUDE-CODE.md, CODEX.md, and GEMINI.md:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;MCP visibility tables (nearly identical, 80+ lines each)&lt;/li&gt;
  &lt;li&gt;Parallel execution patterns (minimal CLI differences, 100+ lines)&lt;/li&gt;
  &lt;li&gt;DO/DON’T best practices lists (word-for-word duplicates, 50+ lines)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This created 795 lines of duplicated content - content that every agent reading any CLI doc had to parse.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The fix:&lt;/strong&gt; Extract to MULTI-AGENT.md as single source of truth. CLI docs now reference rather than duplicate.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Total lines: 2,648 → 1,618 (-39%)&lt;/li&gt;
  &lt;li&gt;Estimated tokens: ~40,000 → ~32,000 (-20%)&lt;/li&gt;
  &lt;li&gt;Maintenance burden: Update 1 file instead of 3&lt;/li&gt;
  &lt;li&gt;Consistency: No more drift between docs&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pain-point-5-mcp-troubleshooting-confusion&quot;&gt;Pain Point #5: MCP Troubleshooting Confusion&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Severity: MEDIUM (2/7 interviews)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Users didn’t understand when Model Context Protocol servers were available to sub-agents, or how to diagnose visibility 
issues. The docs mentioned MCP but never explained the inheritance rules or debugging steps.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The fix:&lt;/strong&gt; Added MCP Troubleshooting section with diagnostic table:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Symptom&lt;/th&gt;
      &lt;th&gt;Diagnosis&lt;/th&gt;
      &lt;th&gt;Fix&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Sub-agent doesn’t see MCP&lt;/td&gt;
      &lt;td&gt;Not registered globally&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;claude mcp add &amp;lt;name&amp;gt; &amp;lt;cmd&amp;gt;&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MCP listed but unavailable&lt;/td&gt;
      &lt;td&gt;Restrictive tool flags&lt;/td&gt;
      &lt;td&gt;Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--tools default&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--tools none&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Connection errors&lt;/td&gt;
      &lt;td&gt;Server not running&lt;/td&gt;
      &lt;td&gt;Check IntelliJ IDE is open&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Works in parent, not in child&lt;/td&gt;
      &lt;td&gt;Command-line args block inheritance&lt;/td&gt;
      &lt;td&gt;Remove explicit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--mcp&lt;/code&gt; flags&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Validation score:&lt;/strong&gt; One agent rated this 9/10, commenting: “diagnostic table format is perfect - covers the exact 
confusion I had.”&lt;/p&gt;

&lt;h3 id=&quot;the-one-command-problem&quot;&gt;The “ONE Command” Problem&lt;/h3&gt;

&lt;p&gt;This wasn’t in our original analysis, but three interviews independently mentioned it: CODEX.md had THREE commands 
marked “RECOMMENDED” with no clear priority.&lt;/p&gt;

&lt;p&gt;Agent feedback:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Ambiguity about which command to use. Mark ONE as primary for 95% of cases.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I thought I was being helpful by showing options. I was actually creating decision paralysis.&lt;/p&gt;

&lt;p&gt;The problem was real. We had:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# RECOMMENDED: Minimal non-interactive&lt;/span&gt;
codex &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;prompt&quot;&lt;/span&gt; 2&amp;gt;&amp;amp;1

&lt;span class=&quot;c&quot;&gt;# RECOMMENDED: Low-friction auto mode&lt;/span&gt;
codex &lt;span class=&quot;nt&quot;&gt;--full-auto&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;prompt&quot;&lt;/span&gt; 2&amp;gt;&amp;amp;1

&lt;span class=&quot;c&quot;&gt;# RECOMMENDED: Maximum tool access&lt;/span&gt;
codex &lt;span class=&quot;nt&quot;&gt;--tools&lt;/span&gt; default &lt;span class=&quot;nt&quot;&gt;--full-auto&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;prompt&quot;&lt;/span&gt; 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Three “recommended” commands means zero recommended commands. An AI agent parsing this has to guess which one we 
actually want them to use.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The fix in CODEX.md:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;### Quick Start&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# ✅ THIS IS THE ONE COMMAND YOU NEED&lt;/span&gt;
codex &lt;span class=&quot;nt&quot;&gt;--full-auto&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;prompt&quot;&lt;/span&gt; 2&amp;gt;&amp;amp;1

Don&lt;span class=&quot;s1&quot;&gt;&apos;t use alternative flag combinations unless you have
a specific reason (debugging, restricting tool access, etc.)
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Validation result:&lt;/strong&gt; 10/10 rating from independent reviewer: “Clear and effective. Zero ambiguity.”&lt;/p&gt;

&lt;p&gt;This became our guiding principle for all three CLI docs: users need ONE command to copy-paste. Not three options. Not 
a menu of flags. One command that works 95% of the time, with a separate “Advanced Usage” section for the 5% edge cases.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;the-implementation-rlm-in-action&quot;&gt;The Implementation: RLM in Action&lt;/h2&gt;

&lt;h3 id=&quot;what-is-rlm&quot;&gt;What Is RLM?&lt;/h3&gt;

&lt;p&gt;Before I explain how we fixed everything, let me introduce the pattern that made this possible.&lt;/p&gt;

&lt;p&gt;RLM, &lt;a href=&quot;https://arxiv.org/abs/2512.24601&quot;&gt;Recursive Language Model&lt;/a&gt;, is a workflow for breaking large 
tasks into parallel agent work. If you’ve used 
MapReduce for data processing, RLM is the same concept applied to AI agents:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PARTITION:&lt;/strong&gt; Divide work into independent tasks with clear boundaries. The rule: no inter-task dependencies. Each 
agent must be able to complete its work without waiting for another agent’s output. For this project, we partitioned by 
file: 5 agents, 5 files to refactor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MAP:&lt;/strong&gt; Execute all tasks simultaneously. Use shell background jobs (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;amp;&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt;) to run agents in parallel. Watch 
for rate limits - most APIs allow 3-5 concurrent requests. Each agent writes to its own output file to avoid conflicts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;REDUCE:&lt;/strong&gt; Manual synthesis of all outputs. This is where the human comes in. Check that cross-references align (no 
broken links between files). Verify terminology is consistent (don’t mix “sub-agent” and “child agent”). Ensure version 
numbers match. Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;diff&lt;/code&gt; to spot conflicts, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; to validate references, and manual review for quality.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;When RLM works:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Tasks are independent (refactoring separate files)&lt;/li&gt;
  &lt;li&gt;Output format is structured (markdown, JSON)&lt;/li&gt;
  &lt;li&gt;Human can validate and merge results&lt;/li&gt;
  &lt;li&gt;Time savings justify coordination cost&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;When RLM fails:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Tasks require sequential coordination (Agent 2 needs Agent 1’s output first)&lt;/li&gt;
  &lt;li&gt;Outputs conflict (both agents edit the same section differently)&lt;/li&gt;
  &lt;li&gt;API rate limits block parallelism entirely&lt;/li&gt;
  &lt;li&gt;Coordination overhead exceeds time savings&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For this project: perfect RLM fit. Five files, five agents, independent work. Parallelism cut execution time from 2+ 
hours to 30 minutes for the implementation phase.&lt;/p&gt;

&lt;p&gt;Full methodology (chopped for agents by agents) with error handling patterns:
&lt;a href=&quot;https://jonnyzzz.com/RLM.md&quot;&gt;jonnyzzz.com/RLM.md&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;working-in-parallel&quot;&gt;Working in Parallel&lt;/h3&gt;

&lt;p&gt;Once we had customer feedback, I had a decision: fix these issues sequentially (safe, slow) or spawn parallel agents 
(risky, fast). I chose fast.&lt;/p&gt;

&lt;p&gt;We spawned 5 agents in parallel:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Agent 1: Refactor CLAUDE-CODE.md (Quick Start, error handling, auth setup)&lt;/li&gt;
  &lt;li&gt;Agent 2: Refactor CODEX.md (single “THE ONE command”, prerequisites, MCP troubleshooting)&lt;/li&gt;
  &lt;li&gt;Agent 3: Refactor GEMINI.md (consistency with other CLIs)&lt;/li&gt;
  &lt;li&gt;Agent 4: Add Part 9 to MULTI-AGENT.md (error handling patterns with retry logic)&lt;/li&gt;
  &lt;li&gt;Agent 5: Add Part 11 to MULTI-AGENT.md (cost management and token optimization)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This was the most nerve-wracking phase. Five agents working simultaneously meant five potential points of failure. If 
two agents edited the same section differently, we’d have merge conflicts. If one agent misunderstood the requirements, 
we’d ship broken docs. But the interviews were so specific - line numbers, severity ratings, concrete examples - that 
the risk felt manageable.&lt;/p&gt;

&lt;p&gt;Each agent received:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Specific pain points from interviews&lt;/li&gt;
  &lt;li&gt;Target file to modify&lt;/li&gt;
  &lt;li&gt;Success criteria (what “done” looks like)&lt;/li&gt;
  &lt;li&gt;Examples of desired patterns&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;timeline-and-reality-check&quot;&gt;Timeline and Reality Check&lt;/h3&gt;

&lt;p&gt;Here’s how the complete project actually played out:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase 1 - Discovery (30 minutes):&lt;/strong&gt;
4 analysis agents examined the existing documentation structure, identified duplication, and mapped pain points. 
Parallelism cut what would have been 2 hours of sequential reading down to 30 minutes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase 2 - Customer Interviews (45 minutes):&lt;/strong&gt;
9 interview agents spawned, though only 7 completed successfully (2 hit API rate limits). Most ran in parallel, with 
some sequential execution when we hit connection limits. This was the most valuable phase - getting honest feedback 
from agents who would actually use these docs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase 3 - Implementation (30 minutes execution, 2 hours human coordination):&lt;/strong&gt;
5 implementation agents working simultaneously on the actual file changes. Agent execution: 30 minutes. Human 
coordination in the reduce phase (checking cross-references, resolving version number mismatches, validating changes): 
2 hours. This is where the RLM pattern requires human oversight.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phase 4 - Validation (30 minutes):&lt;/strong&gt;
3 fresh validator agents reviewed the updated documentation. No prior context, just “read this and tell us if it’s 
better.” I was genuinely nervous. What if the refactor made things worse? What if we’d optimized for the wrong thing?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Total: ~4 hours including human coordination, 2 hours of agent execution time&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Without parallelism? I’d still be refactoring. Twelve to fifteen hours of sequential work, minimum. Parallelism cut 
that to 4 hours total. That’s why this matters.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What went wrong:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;2 of 9 interview agents hit API rate limits and failed silently&lt;/li&gt;
  &lt;li&gt;Agent 4’s first attempt at error handling patterns was too generic - had to re-run with specific examples&lt;/li&gt;
  &lt;li&gt;Manual coordination in the Reduce phase took longer than expected because version numbers didn’t match across files&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;command-examples-how-to-interview-ai-agents&quot;&gt;Command Examples: How to Interview AI Agents&lt;/h3&gt;

&lt;p&gt;Here’s the actual command we used to spawn customer interview agents. These templates are copy-paste ready:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Template 1: Accessibility Audit&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;You are auditing CLAUDE-CODE.md for accessibility.

Answer these questions:
1. Where does the Quick Start section appear? (line number)
2. Can you find the most important command in 15 seconds? (yes/no + why)
3. What information is repeated unnecessarily? (quote sections)
4. Rate accessibility: 1-10

Max 300 words, be specific with line numbers.&quot;&lt;/span&gt; | &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  claude &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tools&lt;/span&gt; default &lt;span class=&quot;nt&quot;&gt;--permission-mode&lt;/span&gt; dontAsk 2&amp;gt;&amp;amp;1 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; interview-accessibility.md
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Template 2: Completeness Check&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;You are checking CODEX.md for completeness.

Identify what&apos;s missing:
1. Prerequisites (installation, auth, dependencies)
2. Error handling (retry logic, timeouts, exit codes)
3. Examples (are they copy-paste ready?)
4. Rate completeness: 1-10

Max 300 words, cite line numbers.&quot;&lt;/span&gt; | &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  claude &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tools&lt;/span&gt; default &lt;span class=&quot;nt&quot;&gt;--permission-mode&lt;/span&gt; dontAsk 2&amp;gt;&amp;amp;1 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; interview-completeness.md
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Template 3: Ambiguity Detection&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;You are identifying ambiguous documentation in GEMINI.md.

Find these issues:
1. Sections where the recommended action is unclear
2. Conflicting instructions (if any)
3. Jargon or undefined terms
4. Rate clarity: 1-10

Max 300 words, quote ambiguous phrases.&quot;&lt;/span&gt; | &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  claude &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tools&lt;/span&gt; default &lt;span class=&quot;nt&quot;&gt;--permission-mode&lt;/span&gt; dontAsk 2&amp;gt;&amp;amp;1 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; interview-clarity.md
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The key flags:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-p&lt;/code&gt; (proactive mode): Let the agent use tools without asking&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--tools default&lt;/code&gt;: Give access to standard toolset (Read, Grep, etc.)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--permission-mode dontAsk&lt;/code&gt;: No interactive prompts, fully automated&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2&amp;gt;&amp;amp;1&lt;/code&gt;: Capture both stdout and stderr&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For parallel execution, we used background jobs:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Run 3 interviews in parallel&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Interview 1 prompt&quot;&lt;/span&gt; | claude &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tools&lt;/span&gt; default &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--permission-mode&lt;/span&gt; dontAsk 2&amp;gt;&amp;amp;1 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; interview-1.md&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &amp;amp;

&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Interview 2 prompt&quot;&lt;/span&gt; | claude &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tools&lt;/span&gt; default &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--permission-mode&lt;/span&gt; dontAsk 2&amp;gt;&amp;amp;1 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; interview-2.md&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &amp;amp;

&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Interview 3 prompt&quot;&lt;/span&gt; | claude &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tools&lt;/span&gt; default &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--permission-mode&lt;/span&gt; dontAsk 2&amp;gt;&amp;amp;1 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; interview-3.md&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &amp;amp;

&lt;span class=&quot;nb&quot;&gt;wait
echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;All interviews complete&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt; command is critical - it ensures all background processes finish before proceeding to the next phase.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;validation-did-it-actually-work&quot;&gt;Validation: Did It Actually Work?&lt;/h2&gt;

&lt;h3 id=&quot;the-proof-version-108-testing&quot;&gt;The Proof: Version 1.0.8 Testing&lt;/h3&gt;

&lt;p&gt;After implementation, we had a problem: how do we know the refactored documentation is actually better? We couldn’t 
grade our own work. So we brought in external reviewers.&lt;/p&gt;

&lt;p&gt;Except our external reviewers were AI agents.&lt;/p&gt;

&lt;p&gt;We spawned 3 brand new agents with zero context. No memory of the interviews. No knowledge of what changed. Just: 
“Here’s the documentation. Evaluate it.”&lt;/p&gt;

&lt;p&gt;The first validator came back with an 8.5/10. I exhaled. The second: 9.0/10. I started to relax. The third validator’s 
report opened with: “This is exactly what I needed. Why didn’t it look like this before?”&lt;/p&gt;

&lt;p&gt;That question hit hard. Because I’d written the original docs. I’d thought they were good. I’d been wrong.&lt;/p&gt;

&lt;h3 id=&quot;beforeafter-scores&quot;&gt;Before/After Scores&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Document&lt;/th&gt;
      &lt;th&gt;v1.0.7 Score&lt;/th&gt;
      &lt;th&gt;v1.0.8 Score&lt;/th&gt;
      &lt;th&gt;Improvement&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;CLAUDE-CODE.md&lt;/td&gt;
      &lt;td&gt;8.0/10&lt;/td&gt;
      &lt;td&gt;8.5/10&lt;/td&gt;
      &lt;td&gt;+6%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CODEX.md&lt;/td&gt;
      &lt;td&gt;7.0/10&lt;/td&gt;
      &lt;td&gt;9.0/10&lt;/td&gt;
      &lt;td&gt;+29%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MULTI-AGENT.md&lt;/td&gt;
      &lt;td&gt;8.0/10&lt;/td&gt;
      &lt;td&gt;9.0/10&lt;/td&gt;
      &lt;td&gt;+13%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Average&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;7.67/10&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;8.83/10&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;+15%&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The biggest improvement was CODEX.md - from 7.0 to 9.0. That makes sense: it had the most severe issues (3 
“recommended” commands, buried essentials, missing auth setup). The fixes were dramatic.&lt;/p&gt;

&lt;p&gt;CLAUDE-CODE.md saw a smaller gain because it was already the cleanest of the three. But even there, moving the Quick 
Start section to the top and adding error handling examples pushed it from “good” to “very good.”&lt;/p&gt;

&lt;h3 id=&quot;validator-quotes&quot;&gt;Validator Quotes&lt;/h3&gt;

&lt;p&gt;From VALIDATION-REPORT-v1.0.8.md:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Quick Start sections score 9-10/10 for accessibility”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The validators loved the new structure. Quick Start at the top meant they could find and execute the most important 
command in 15 seconds instead of scrolling through 90 lines of setup.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“‘THE ONE command’ messaging is clear and effective” (10/10)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CODEX.md went from 3 ambiguous “recommended” options to one crystal-clear primary command with explicit guidance on 
when to deviate. No more decision paralysis.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Diagnostic table format is perfect” (9/10)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The MCP troubleshooting table gave validators a quick reference for debugging. Symptom → Diagnosis → Fix. Exactly
what they needed.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Production-ready guidance with measurable metrics” (9/10)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Adding error handling patterns with concrete examples (retry logic, exit codes, timeouts) transformed the docs from 
“here’s what you can do” to “here’s how to do it reliably.”&lt;/p&gt;

&lt;h3 id=&quot;the-numbers&quot;&gt;The Numbers&lt;/h3&gt;

&lt;p&gt;Let’s look at the concrete efficiency metrics:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Documentation efficiency:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Total lines: 2,648 → 1,618 (-39%)&lt;/li&gt;
  &lt;li&gt;Duplication: 795 lines repeated → 0 lines repeated&lt;/li&gt;
  &lt;li&gt;Distance to Quick Start: Line 90+ → Line 10-15 (80% closer)&lt;/li&gt;
  &lt;li&gt;Error handling patterns: 0 → 15+ examples added&lt;/li&gt;
  &lt;li&gt;Cost guidance: None → Comprehensive (Part 11 added)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Customer satisfaction:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Critical issues resolved: 5 out of 5 top pain points (100%)&lt;/li&gt;
  &lt;li&gt;Quality improvement: 7.67/10 → 8.83/10 (+15%)&lt;/li&gt;
  &lt;li&gt;Production ready: ✅ All 3 validators approved for production use&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Time savings:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Time to find recommended command: 60-90 seconds → 10-15 seconds (5x faster)&lt;/li&gt;
  &lt;li&gt;Time to understand error handling: ∞ (didn’t exist) → 2-3 minutes&lt;/li&gt;
  &lt;li&gt;Time to set up authentication: Failed often → Documented with Prerequisites section&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-surprise&quot;&gt;The Surprise&lt;/h3&gt;

&lt;p&gt;Here’s what I didn’t expect: the validators gave us higher scores (8.83/10) than the interviews did (7.67/10). Why?&lt;/p&gt;

&lt;p&gt;The interviewers were looking at broken documentation with a critical eye. The validators were looking at fixed 
documentation with no prior context. They didn’t know it had been worse. They just saw working, clear documentation and 
rated it highly.&lt;/p&gt;

&lt;p&gt;This taught us something about user research: users who experience the pain give you better feedback than users who 
only see the solution. The interview phase was more valuable than the validation phase, even though validation gave us 
better scores.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;what-i-learned&quot;&gt;What I Learned&lt;/h2&gt;

&lt;h3 id=&quot;interview-your-users-even-if-theyre-ais&quot;&gt;Interview Your Users (Even If They’re AIs)&lt;/h3&gt;

&lt;p&gt;I spent two full versions guessing what was wrong. Version 1.0.6? Not idea. Version 1.0.7? Still not good enough. 
One round of structured interviews revealed what users actually needed better than months of my assumptions.&lt;/p&gt;

&lt;p&gt;AI agents are surprisingly good at articulating what makes documentation effective. They can’t be polite out of social 
obligation — if it’s confusing, they’ll tell you.&lt;/p&gt;

&lt;p&gt;The practical takeaway: don’t assume, ask. Spawn 5-10 agents with structured questions, collect ratings, prioritize by 
frequency. That’s it.&lt;/p&gt;

&lt;h3 id=&quot;recommended-is-not-enough&quot;&gt;“Recommended” Is Not Enough&lt;/h3&gt;

&lt;p&gt;CODEX.md had three commands marked “RECOMMENDED” with no clear priority. As one validator put it: “Ambiguity about 
which command to use.” I thought I was being helpful by showing options. I was actually creating decision paralysis.&lt;/p&gt;

&lt;p&gt;Having three recommended commands equals having none. The fix: mark ONE primary command for 95% of use cases, then 
explain when to deviate. After this change, validators gave it a 10/10.&lt;/p&gt;

&lt;p&gt;Your users shouldn’t have to think about which tool to reach for.&lt;/p&gt;

&lt;h3 id=&quot;duplication-has-hidden-costs&quot;&gt;Duplication Has Hidden Costs&lt;/h3&gt;

&lt;p&gt;Twenty percent token overhead seems small until you multiply by every agent reading the docs, every spawned sub-agent, 
every validation test, and every future update requiring 3x maintenance work.&lt;/p&gt;

&lt;p&gt;We removed 795 lines of duplication, saving 8,000 tokens per agent spawn. The ROI calculation: 4 hours to remove 
duplication, 2x time saved per future update, break-even after 2-3 updates.&lt;/p&gt;

&lt;p&gt;Small percentages compound fast.&lt;/p&gt;

&lt;h3 id=&quot;error-handling-is-not-optional&quot;&gt;Error Handling Is Not Optional&lt;/h3&gt;

&lt;p&gt;Zero interviews complained about lack of advanced features. Four out of seven complained about missing error handling.&lt;/p&gt;

&lt;p&gt;Users need to know:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;What to do when it fails (retry logic)&lt;/li&gt;
  &lt;li&gt;How long to wait (timeouts)&lt;/li&gt;
  &lt;li&gt;How to interpret errors (exit codes)&lt;/li&gt;
  &lt;li&gt;When to give up (max retries)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We added 15+ error handling patterns based on this feedback. The lesson: error states are not edge cases, they’re the 
primary use case for production systems.&lt;/p&gt;

&lt;h3 id=&quot;multi-agent-orchestration-works&quot;&gt;Multi-Agent Orchestration Works&lt;/h3&gt;

&lt;p&gt;Sixteen agents spawned across four phases - analysis, interviews, implementation, validation. What made it work:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Clear task boundaries&lt;/li&gt;
  &lt;li&gt;Independent work with no coordination needed within phases&lt;/li&gt;
  &lt;li&gt;Structured outputs in markdown&lt;/li&gt;
  &lt;li&gt;A reduce phase for synthesis&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What didn’t work:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Rate limits hit 2 of 9 interviews&lt;/li&gt;
  &lt;li&gt;Nested execution failed in practice&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The takeaway: plan for API constraints from the start, and test your assumptions about what’s actually possible before 
committing to a strategy.&lt;/p&gt;

&lt;h3 id=&quot;quantify-everything&quot;&gt;Quantify Everything&lt;/h3&gt;

&lt;p&gt;Without numbers, we would have “made it better” instead of 39% reduction, “users liked it” instead of 8.83/10 rating, 
and “faster to use” instead of 5x improvement.&lt;/p&gt;

&lt;p&gt;Metrics we tracked:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Lines of code&lt;/li&gt;
  &lt;li&gt;Duplication percentage&lt;/li&gt;
  &lt;li&gt;Quality ratings&lt;/li&gt;
  &lt;li&gt;Time to key information&lt;/li&gt;
  &lt;li&gt;Issue resolution rate&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Quantification turns vague improvements into compelling evidence. It also forces you to define what success looks like 
before you start.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;what-we-shipped&quot;&gt;What We Shipped&lt;/h2&gt;

&lt;p&gt;The refactor produced two types of deliverables: production files (what users actually read) and research artifacts 
(how we got there).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Production files (v1.0.8):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CLAUDE-CODE.md: 553 → 285 lines (-48%)&lt;/li&gt;
  &lt;li&gt;CODEX.md: 756 → 312 lines (-59%)&lt;/li&gt;
  &lt;li&gt;GEMINI.md: 757 → 327 lines (-57%)&lt;/li&gt;
  &lt;li&gt;MULTI-AGENT.md: 582 → 694 lines (+19%)&lt;/li&gt;
  &lt;li&gt;RLM.md (updated for consistency)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The numbers tell the story: three CLI docs lost more than half their length, while MULTI-AGENT.md grew slightly as the 
new single source of truth for shared patterns. Net result: 1,030 fewer lines to maintain, read, and pay tokens for.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Research artifacts (67.5 KB total):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ACTION-PLAN-v1.0.8.md (8.4 KB) - Implementation roadmap&lt;/li&gt;
  &lt;li&gt;FINAL-REPORT.md (12.1 KB) - Phase 1 summary&lt;/li&gt;
  &lt;li&gt;INTERVIEW-RESULTS.md (15.2 KB) - Customer feedback compilation&lt;/li&gt;
  &lt;li&gt;VALIDATION-REPORT-v1.0.8.md (8.7 KB) - Quality verification from fresh agents&lt;/li&gt;
  &lt;li&gt;RELEASE-NOTES-v1.0.8.md (9.2 KB) - Complete changelog with migration notes&lt;/li&gt;
  &lt;li&gt;REFACTOR-PLAN.md (6.3 KB) - Strategic analysis and approach&lt;/li&gt;
  &lt;li&gt;REFACTOR-SUMMARY.md (4.1 KB) - Before/after comparison with metrics&lt;/li&gt;
  &lt;li&gt;STATUS.md (3.5 KB) - Project status tracking&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These artifacts became their own form of documentation - showing not just what changed, but why, how, and whether it 
worked. Anyone can follow this pattern for their own documentation refactoring.&lt;/p&gt;

&lt;p&gt;All production files and research artifacts are available at &lt;a href=&quot;https://jonnyzzz.com/&quot;&gt;jonnyzzz.com&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;lets-connect&quot;&gt;Let’s Connect&lt;/h2&gt;

&lt;p&gt;Share your results. Try this with your team’s documentation - I want to see your before/after metrics. Tag me on
&lt;a href=&quot;https://www.linkedin.com/in/jonnyzzz/&quot;&gt;LinkedIn at @jonnyzzz&lt;/a&gt; with your quality scores. Bonus points if your AI agents
are as brutally honest as mine were.&lt;/p&gt;

&lt;p&gt;Find a bug in the refactored docs? Submit PRs if you improve the patterns, let me know!&lt;/p&gt;

&lt;p&gt;And if you’re working on multi-agent orchestration or have questions about the RLM methodology, reach out. This is just
the beginning. We documented multi-agent orchestration, then used multi-agent orchestration to improve the
documentation about multi-agent orchestration. Agents spawning agents to interview agents. The recursive loop closed.&lt;/p&gt;

&lt;p&gt;Now go interview your AI customers. They’re waiting to tell you the truth.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next&lt;/h2&gt;

&lt;p&gt;The documentation is now production-ready. Quick Start sections appear in the first 15 lines. Error handling patterns
cover retry logic, timeouts, and exit codes. Authentication prerequisites are explicit. Duplication is eliminated. The
most common command is clearly marked as THE ONE command you need.&lt;/p&gt;

&lt;p&gt;But the research artifacts themselves became something more valuable: meta-documentation about improving documentation
through multi-agent orchestration. They show the complete workflow from analysis to interviews to validation to
deployment.&lt;/p&gt;

&lt;p&gt;The validation agents identified minor polish opportunities: reducing redundancy between Quick Start and Core Commands
sections, adding cost benchmark examples, making version requirements more specific. These are low-priority
improvements, not blockers, but they’re documented and ready for v1.0.9.&lt;/p&gt;

&lt;p&gt;I’m already thinking about the next iteration. Can we use this pattern for code refactoring? For test generation? For
architecture decisions? The interviews proved AI agents can articulate quality issues. The parallel implementation
proved they can fix those issues. That’s powerful.&lt;/p&gt;

&lt;p&gt;Next quarter, we’re applying this to actual codebase refactoring. The documentation was the proof of concept. The real
work is applying this to engineering workflows where the stakes are higher and the problems are messier. That’s where
this gets interesting.&lt;/p&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="ai-agents" />
  
    <category term="multi-agent" />
  
    <category term="documentation" />
  
    <category term="rlm" />
  
    <summary type="html">We interviewed 7 AI agents about our documentation. After 16 agents and honest feedback - 39% shorter, 15% higher quality, 5x faster to use.</summary>
  
  </entry>
  
  <entry>
    <title type="html">Kotlin DSLs in 2026: Patterns That Stood the Test of Time</title>
    <link href="https://jonnyzzz.com/blog/2026/01/19/kotlin-dsl-2026/" rel="alternate" type="text/html" title="Kotlin DSLs in 2026: Patterns That Stood the Test of Time" />
    <published>2026-01-19T00:00:00+00:00</published>
    <updated>2026-01-19T00:00:00+00:00</updated>
    <id>/blog/2026/01/19/kotlin-dsl-2026</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/01/19/kotlin-dsl-2026/">&lt;p&gt;Eight years ago, I wrote about using Kotlin higher-order functions to simplify Java builders. 
Around the same time, I explored ad-hoc Gradle plugins and “The DSL Way”–replacing configuration 
files with type-safe Kotlin code. Looking back from 2026, these patterns have not only survived 
but have become fundamental to how we build modern JVM applications.&lt;/p&gt;

&lt;h2 id=&quot;the-core-idea-that-never-changed&quot;&gt;The Core Idea That Never Changed&lt;/h2&gt;

&lt;p&gt;The fundamental insight remains: Kotlin’s language features–extension functions, lambdas 
with receivers, and operator overloading–turn any API into a potential DSL.&lt;/p&gt;

&lt;p&gt;Consider the classic Java builder problem:&lt;/p&gt;

&lt;div class=&quot;language-kotlin highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Traditional Java builder usage - verbose and error-prone&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;py&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;JWT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;withIssuer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ISSUER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;withClaim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;userId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;userId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;withClaim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;serviceId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;serviceId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;// Easy to copy-paste errors!&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;algorithm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The solution I proposed in 2018 still works perfectly:&lt;/p&gt;

&lt;div class=&quot;language-kotlin highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;fun&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;withX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;?,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;py&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;JWT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;withIssuer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ISSUER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;withX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;userId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;withClaim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;userId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;withX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;serviceId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;withClaim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;serviceId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;algorithm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-has-evolved-scope-control&quot;&gt;What Has Evolved: Scope Control&lt;/h2&gt;

&lt;p&gt;Kotlin introduced &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@DslMarker&lt;/code&gt; for better scope control in DSLs:&lt;/p&gt;

&lt;div class=&quot;language-kotlin highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nd&quot;&gt;@DslMarker&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;annotation&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ConfigDsl&lt;/span&gt;

&lt;span class=&quot;nd&quot;&gt;@ConfigDsl&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ServerConfig&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;py&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;lateinit&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;py&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DatabaseConfig&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;fun&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DatabaseConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;database&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DatabaseConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;fun&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;server&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ServerConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ServerConfig&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;
    &lt;span class=&quot;nc&quot;&gt;ServerConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@DslMarker&lt;/code&gt; annotation prevents accidentally calling outer scope functions from inner blocks.&lt;/p&gt;

&lt;h2 id=&quot;build-systems-gradle-kotlin-dsl-matured&quot;&gt;Build Systems: Gradle Kotlin DSL Matured&lt;/h2&gt;

&lt;p&gt;The patterns for ad-hoc plugins and code reuse have become standard:&lt;/p&gt;

&lt;div class=&quot;language-kotlin highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// buildSrc/src/main/kotlin/service-conventions.gradle.kts&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;plugins&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;kotlin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;jvm&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;application&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;dependencies&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;implementation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;platform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;org.jetbrains.kotlin:kotlin-bom&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;implementation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;io.ktor:ktor-server-core:2.3.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The trick of using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$id:$id.gradle.plugin:$version&lt;/code&gt; to include plugin dependencies 
in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;buildSrc&lt;/code&gt; remains the key insight for sharing complex build logic.&lt;/p&gt;

&lt;h2 id=&quot;configuration-as-code-the-dsl-way&quot;&gt;Configuration as Code: The DSL Way&lt;/h2&gt;

&lt;p&gt;The concept of transforming configuration files into executable Kotlin code has proven durable:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Parse the original format (properties, YAML, XML)&lt;/li&gt;
  &lt;li&gt;Generate readable Kotlin DSL code&lt;/li&gt;
  &lt;li&gt;Execute the DSL to produce the original format&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-kotlin highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Instead of log4j.properties&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;log4j&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;py&quot;&gt;console&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;appender&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ConsoleAppender&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;console&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;PatternLayout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;conversionPattern&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;%d{ISO8601} [%t] %-5p %c - %m%n&quot;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    
    &lt;span class=&quot;nf&quot;&gt;rootLogger&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;level&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ERROR&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;appenders&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;console&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Variable references enable find-usages and rename refactoring. The type system catches invalid configurations at compile time.&lt;/p&gt;

&lt;h2 id=&quot;modern-use-cases&quot;&gt;Modern Use Cases&lt;/h2&gt;

&lt;p&gt;Beyond build systems, Kotlin DSLs have found homes in:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;API Client Generation&lt;/strong&gt;: Ktor uses DSLs for routing and request handling&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Infrastructure as Code&lt;/strong&gt;: Kotlin DSLs for Terraform or Pulumi&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data Pipelines&lt;/strong&gt;: Spark and Flink wrappers&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;UI Frameworks&lt;/strong&gt;: Compose is fundamentally a Kotlin DSL&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;practical-guidelines-for-dsl-design&quot;&gt;Practical Guidelines for DSL Design&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Start with the usage site&lt;/strong&gt;: Write how you want the DSL to look first&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@DslMarker&lt;/code&gt;&lt;/strong&gt;: Always add scope control&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Prefer extension functions&lt;/strong&gt;: Keep core interfaces clean&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Make illegal states unrepresentable&lt;/strong&gt;: Use the type system&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Provide escape hatches&lt;/strong&gt;: Sometimes users need the underlying API&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Kotlin DSLs have moved from a clever technique to an essential tool in the JVM ecosystem. 
The patterns established years ago remain the foundation. What has changed is the tooling
maturity and breadth of applications.&lt;/p&gt;

&lt;p&gt;If you are still writing verbose Java builder code or maintaining error-prone configuration 
files, consider whether a thin Kotlin DSL layer could transform your development experience.&lt;/p&gt;

&lt;p&gt;The best programs remain immutable programs. And the best configurations are the ones that fail
at compile time, not in production.&lt;/p&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="kotlin" />
  
    <category term="java" />
  
    <category term="dsl" />
  
    <summary type="html">Eight years ago, I wrote about using Kotlin higher-order functions to simplify Java builders. Around the same time, I explored ad-hoc Gradle plugins and “The DSL Way”–replacing configuration files with type-safe Kotlin code. Looking back from 2026, these patterns have not only survived but have become fundamental to how we build modern JVM applications.</summary>
  
  </entry>
  
  <entry>
    <title type="html">Stop Optimizing Code Generation: Why Code Review Is Your Real SDLC Bottleneck</title>
    <link href="https://jonnyzzz.com/blog/2026/01/16/code-review-bottleneck/" rel="alternate" type="text/html" title="Stop Optimizing Code Generation: Why Code Review Is Your Real SDLC Bottleneck" />
    <published>2026-01-16T00:00:00+00:00</published>
    <updated>2026-01-16T00:00:00+00:00</updated>
    <id>/blog/2026/01/16/code-review-bottleneck</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/01/16/code-review-bottleneck/">&lt;p&gt;Everyone is racing to make developers write code faster. GitHub Copilot promises 55% 
faster task completion. Cursor raised $2.3B on the promise of AI-native coding. Amazon 
launched Kiro. Google built Antigravity. The message is clear: AI will supercharge code generation.&lt;/p&gt;

&lt;p&gt;But here is the uncomfortable truth: &lt;strong&gt;coding was never your bottleneck.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-myth-of-the-coding-bottleneck&quot;&gt;The Myth of the Coding Bottleneck&lt;/h2&gt;

&lt;p&gt;According to IDC research, developers spend only 16% of their time actually 
writing code. Data from 250,000+ developers shows they code approximately 52 
minutes per day–about 4 hours and 21 minutes during a normal workweek.&lt;/p&gt;

&lt;p&gt;Where does the rest of the time go? Meetings, context switching, waiting for builds, 
debugging production issues, and–critically–&lt;strong&gt;waiting for code reviews&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;If your developers are only coding 16% of their day, making that 16% twice as fast 
means you have improved total developer throughput by… 8%. Meanwhile, the other 
84% of their time remains untouched.&lt;/p&gt;

&lt;h2 id=&quot;where-time-actually-goes-the-real-bottlenecks&quot;&gt;Where Time Actually Goes: The Real Bottlenecks&lt;/h2&gt;

&lt;p&gt;If you want to optimize your SDLC, you need to measure it. Here is what the data reveals:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Code Review Wait Times&lt;/strong&gt;
Elite development teams complete code reviews in under 3 hours, with 
pickup times under 75 minutes. Most teams are far from elite. When 
developers wait 3 days for a review, they context-switch to other 
work. Research shows it takes an average of 23 minutes to refocus after an interruption.&lt;/p&gt;

&lt;p&gt;At Meta, they found a direct correlation between slow code review times and engineer
dissatisfaction. When they optimized their review process, their average 
“Time In Review” dropped 7%, and diffs waiting longer than three days decreased by 12%.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Context Switching Costs&lt;/strong&gt;
Gerald Weinberg’s research demonstrates that adding a single extra 
project to a developer’s workload consumes 20% of their time through context 
switching. Add a third task, and half their time evaporates.&lt;/p&gt;

&lt;h2 id=&quot;why-code-review-is-the-right-place-for-ai&quot;&gt;Why Code Review Is the Right Place for AI&lt;/h2&gt;

&lt;p&gt;Code review sits at the intersection of several properties that make it ideal for AI assistance:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. High Volume, Repetitive Patterns&lt;/strong&gt;
Most code review feedback falls into predictable categories: style 
violations, null safety issues, missing error handling, deprecated
API usage, performance anti-patterns.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Expert Knowledge Is Scarce&lt;/strong&gt;
Every organization has “rock star” reviewers–the developers 
everyone wants reviewing their code. AI can extract patterns from these
experts’ historical reviews and apply them at scale.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Review Latency Compounds&lt;/strong&gt;
A 24-hour code review delay is not just 24 hours lost. It triggers context 
switching, creates WIP accumulation, and extends your entire change lead time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. Human Reviewers Should Focus on Architecture, Not Syntax&lt;/strong&gt;
Senior developers’ time is better spent on architectural decisions, 
mentoring, and strategic code concerns–not catching missing null checks.&lt;/p&gt;

&lt;h2 id=&quot;action-items-for-engineering-leaders&quot;&gt;Action Items for Engineering Leaders&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Measure your SDLC&lt;/strong&gt; - Track where time actually goes. What is your median PR pickup time?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Identify your review bottlenecks&lt;/strong&gt; - Who are your rock star reviewers?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Start with automation of the mechanical&lt;/strong&gt; - Ensure CI and static analysis gates are in place before human review&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Extract patterns from your best reviewers&lt;/strong&gt; - Analyze historical review comments&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Measure the impact&lt;/strong&gt; - Track review cycle time before and after AI assistance&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;the-future&quot;&gt;The Future&lt;/h2&gt;

&lt;p&gt;I do believe that we are at the verge of automatic processes, you cannot
effectively benefit from AI tools if you keep doing 100% of some work manually
within your SDLC pipeline.&lt;/p&gt;

&lt;p&gt;We need to build automated code reviews, which will make our developers faster,
and which will allow us to process more code, partially automatically.&lt;/p&gt;

&lt;p&gt;In the long run, I believe, we are going to start building trust, and make AI a
natural pall and helper in the review process. Build it to make it to complete
5% of the reviews to make it trustworthy partner. And grow from that.&lt;/p&gt;

&lt;h2 id=&quot;the-future-reviewer&quot;&gt;The Future Reviewer&lt;/h2&gt;

&lt;p&gt;Given we have automated code reviews, where an AI agents can comment on the review,
they can propose suggested improvements, and humans for help.&lt;/p&gt;

&lt;p&gt;Such AI Agents need to learn from your existing code reviews, so you build skill and
principles, which your team is operating on. That is the way the trust is established.&lt;/p&gt;

&lt;p&gt;The learned principles should sit in the Markdown files directly in your repositories,
team should read, discuss, and update it.&lt;/p&gt;

&lt;p&gt;** Move all your water cooler conversations to the Markdown files, wikis, so agents can read that **&lt;/p&gt;

&lt;p&gt;Remote teams, that’s for you too!&lt;/p&gt;

&lt;h2 id=&quot;the-future-agentic-swarm&quot;&gt;The Future Agentic Swarm&lt;/h2&gt;

&lt;p&gt;Agents will be talking to each other. Like a coding agent will interact with the reviewers agents.&lt;/p&gt;

&lt;p&gt;As it works for humans today. No changes.&lt;/p&gt;

&lt;p&gt;And agents love working in the feedback loop, gamification!&lt;/p&gt;

&lt;h2 id=&quot;the-bigger-picture&quot;&gt;The Bigger Picture&lt;/h2&gt;

&lt;p&gt;The organizations that win will not be those who generate code fastest. They will be the 
ones who deliver value fastest – and that requires optimizing the entire software 
development lifecycle, not just the coding phase.&lt;/p&gt;

&lt;p&gt;Coding is not your bottleneck. It probably never was. The data is clear. Now it is time to put
AI where it will actually make a difference.&lt;/p&gt;

&lt;p&gt;Code review is waiting.&lt;/p&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="ai" />
  
    <category term="developer-productivity" />
  
    <summary type="html">Everyone is racing to make developers write code faster. GitHub Copilot promises 55% faster task completion. Cursor raised $2.3B on the promise of AI-native coding. Amazon launched Kiro. Google built Antigravity. The message is clear: AI will supercharge code generation.</summary>
  
  </entry>
  
  <entry>
    <title type="html">From Academic Paper to Executable Skills: Multi-Agent Orchestration with RLM</title>
    <link href="https://jonnyzzz.com/blog/2026/01/05/rlm-multi-agent-orchestration/" rel="alternate" type="text/html" title="From Academic Paper to Executable Skills: Multi-Agent Orchestration with RLM" />
    <published>2026-01-05T00:00:00+00:00</published>
    <updated>2026-01-05T00:00:00+00:00</updated>
    <id>/blog/2026/01/05/rlm-multi-agent-orchestration</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/01/05/rlm-multi-agent-orchestration/">&lt;p&gt;Translating academic research into practical AI agent skills through systematic experimentation.&lt;/p&gt;

&lt;p&gt;A few weeks ago, I stumbled upon a fascinating paper on X: “Recursive Language Models” by Zhang, Kraska, and Khattab from MIT CSAIL. The core insight was elegant—treat the prompt as a Python variable rather than neural network input, allowing models to work with unbounded context through recursive sub-calls. The obvious question: could we translate these theoretical patterns into executable instructions for AI agents?&lt;/p&gt;

&lt;p&gt;This post documents the experiment. We took the RLM paper, extracted its core concepts, and transformed them into skill files that AI agents can follow. Then we validated the approach by having multiple AI agents—Claude Code, Codex, and Gemini—cross-validate each other’s work. The results reveal both the promise and the challenges of multi-agent orchestration.&lt;/p&gt;

&lt;h2 id=&quot;the-problem-academic-papers-dont-compile&quot;&gt;The Problem: Academic Papers Don’t Compile&lt;/h2&gt;

&lt;p&gt;Research papers describe algorithms and results. They don’t provide executable instructions that an AI agent can follow step-by-step. The gap between “here’s the theory” and “now do this” is where most knowledge transfer fails.&lt;/p&gt;

&lt;p&gt;Consider RLM’s key insight about context rot—performance degrades as context length increases. The paper quantifies this beautifully with benchmarks on OOLONG and BrowseComp+. But an AI agent facing a 100K token codebase doesn’t know to apply partition+map+reduce unless explicitly told when and how.&lt;/p&gt;

&lt;p&gt;Our approach was systematic:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Extract&lt;/strong&gt; - Identify actionable patterns from the paper&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Codify&lt;/strong&gt; - Transform into explicit decision trees and templates&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Validate&lt;/strong&gt; - Cross-check with multiple AI models&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Iterate&lt;/strong&gt; - Refine based on experimental feedback&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;the-translation-process&quot;&gt;The Translation Process&lt;/h2&gt;

&lt;h3 id=&quot;phase-1-core-concept-extraction&quot;&gt;Phase 1: Core Concept Extraction&lt;/h3&gt;

&lt;p&gt;The RLM paper’s abstract mentions “treating the prompt as a Python variable.” What does this mean operationally?&lt;/p&gt;

&lt;p&gt;After multiple read-throughs (some by humans, some by AI agents), we extracted these key patterns:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Paper Concept&lt;/th&gt;
      &lt;th&gt;Executable Pattern&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Context as variable&lt;/td&gt;
      &lt;td&gt;Access data programmatically, not via prompt stuffing&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Recursive sub-calls&lt;/td&gt;
      &lt;td&gt;Spawn sub-agents with focused prompts&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;REPL environment&lt;/td&gt;
      &lt;td&gt;Iterative tool use with state management&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Partition+Map+Reduce&lt;/td&gt;
      &lt;td&gt;Chunk large inputs, process in parallel, aggregate&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;phase-2-decision-tree-construction&quot;&gt;Phase 2: Decision Tree Construction&lt;/h3&gt;

&lt;p&gt;Academic papers describe what works. Skill files need to specify when to apply each technique. We built decision trees:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;START
  |
  +-- Context &amp;gt; 50K tokens? -------- YES --&amp;gt; ACTIVATE RLM
  |     |
  |     NO
  |     |
  +-- Context &amp;gt; 16K AND complexity &amp;gt; O(1)? -- YES --&amp;gt; CONSIDER RLM
  |     |
  |     NO
  |     |
  +-- Files to process &amp;gt; 5? -------- YES --&amp;gt; ACTIVATE RLM
  |     |
  |     NO
  |     |
  +-- Multi-hop reasoning needed? -- YES --&amp;gt; ACTIVATE RLM
  |     |
  |     NO
  |     |
  +-- PROCEED DIRECTLY
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This transforms a theoretical concept into an operational rule. Note the complexity-aware middle branch—the full RLM.md includes additional nuance for moderate-sized contexts where task complexity (O(n) vs O(n²)) determines whether RLM overhead is worthwhile.&lt;/p&gt;

&lt;h3 id=&quot;phase-3-strategy-templates&quot;&gt;Phase 3: Strategy Templates&lt;/h3&gt;

&lt;p&gt;The paper describes emergent strategies like “Grep First” and “Preview + Partition.” We codified these into templates:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;STRATEGY: Grep First
TRIGGER: Searching for specific patterns across many files
PROCEDURE:
1. Use grep/search to identify relevant file subset
2. Read only matching files
3. Process focused context

STRATEGY: Partition + Map + Reduce
TRIGGER: Single large file exceeding context limit
PROCEDURE:
1. Assess document structure (chapters, sections, functions)
2. Split at natural boundaries with 10-20% overlap
3. Process each chunk with identical prompt
4. Aggregate results, resolving conflicts
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;the-multi-agent-experiment&quot;&gt;The Multi-Agent Experiment&lt;/h2&gt;

&lt;h3 id=&quot;experimental-design&quot;&gt;Experimental Design&lt;/h3&gt;

&lt;p&gt;With the RLM skill file (RLM.md) drafted, we needed validation. Our hypothesis: multiple AI models would catch different issues, improving overall quality.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Methodology:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Create initial draft using Claude Code with RLM patterns&lt;/li&gt;
  &lt;li&gt;Have three agents review independently: Claude (via Task tool), Codex (via CLI), Gemini (via CLI)&lt;/li&gt;
  &lt;li&gt;Collect and compare findings&lt;/li&gt;
  &lt;li&gt;Measure agreement and divergence&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;agent-configuration&quot;&gt;Agent Configuration&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Claude Code sub-agent (pseudo-API, actual tool invocation)&lt;/span&gt;
Task&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;subagent_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Explore&quot;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;prompt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Review RLM.md for...&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Codex CLI (replace model with your available model)&lt;/span&gt;
codex &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; &amp;lt;your-model&amp;gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; RLM.md &lt;span class=&quot;s2&quot;&gt;&quot;Review for...&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Gemini CLI (use here-doc for large files to avoid shell arg limits)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;GEMINI_API_KEY&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$YOUR_API_KEY&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; gemini &lt;span class=&quot;s2&quot;&gt;&quot;Review RLM.md content: ...&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Task(...)&lt;/code&gt; syntax is pseudo-API representing Claude Code’s internal subagent mechanism. The actual invocation happens through tool use, not shell commands.&lt;/p&gt;

&lt;h3 id=&quot;experiment-log&quot;&gt;Experiment Log&lt;/h3&gt;

&lt;p&gt;The following logs are from actual execution on 2026-01-05:&lt;/p&gt;

&lt;h4 id=&quot;claude-agent-review-task-id-a642a60&quot;&gt;Claude Agent Review (Task ID: a642a60)&lt;/h4&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[EXPERIMENT LOG - Claude Code Sub-Agent]
Timestamp: 2026-01-05T13:52:XX UTC
Task ID: a642a60
Duration: ~90 seconds
Tools used: 31 (Read, Grep, Glob, Bash)

Key Findings:
1. ACCURACY: Paper claims &quot;+12.5 points&quot; on OOLONG verified against RLM.md
2. ACCURACY: BrowseComp+ figure (91.33%) verified consistent
3. COMPLETENESS: Decision tree simplification noted - missing complexity branch
4. STYLE: File line counts needed correction (GEMINI.md: ~300 → 423)
5. ISSUE: Paper year citation inconsistency (arXiv 2512 = Dec 2025, not 2024)

Recommendation Score: 7/10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;codex-agent-review-session-019b8e72&quot;&gt;Codex Agent Review (Session: 019b8e72)&lt;/h4&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[EXPERIMENT LOG - Codex CLI]
Timestamp: 2026-01-05T13:57:XX UTC
Session ID: 019b8e72-fbae-7910-bbb9-34f4fabfec3e
Model: gpt-5.2-codex (reasoning: xhigh)
Tokens used: 51,162

Key Findings:
1. &quot;Recursive sub-calls&quot; framing overstates RLM - paper uses LM-in-REPL pattern
2. &quot;Grep First&quot; presented as paper claim but is derived heuristic - needs citation
3. Task(...) syntax is pseudo-API - should be labeled clearly
4. Quality scores/time multipliers lack methodology definition
5. Shell arg limits risk with CONTENT=$(cat ...) pattern

Rating: 7/10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;gemini-agent-review&quot;&gt;Gemini Agent Review&lt;/h4&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[EXPERIMENT LOG - Gemini CLI]
Timestamp: 2026-01-05T14:05:XX UTC
Model: gemini-2.0 (via API key)

Key Findings:
1. Factual claims about RLM paper VERIFIED
2. GAP: Emulation vs Implementation distinction needed
3. GAP: Cost implications of recursive API calls not addressed
4. GAP: Infinite recursion safeguards not discussed
5. RECOMMENDATION: Add latency warning in introduction

Rating: 8.5/10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;cross-validation-analysis&quot;&gt;Cross-Validation Analysis&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Finding Category&lt;/th&gt;
      &lt;th&gt;Claude&lt;/th&gt;
      &lt;th&gt;Codex&lt;/th&gt;
      &lt;th&gt;Gemini&lt;/th&gt;
      &lt;th&gt;Agreement&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Core accuracy verified&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;3/3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Paper year correction needed&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;2/3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pseudo-API labeling needed&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;1/3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Emulation vs implementation&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;1/3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cost/recursion safeguards&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;2/3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;File count corrections&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;1/3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Key observation:&lt;/strong&gt; Each agent identified unique issues. Cross-validation caught 6 distinct improvement areas. Agreement on core accuracy (3/3) validates the foundational content. Disagreement on gaps (1/3 each) demonstrates the value of diverse model architectures.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Important clarification (from Gemini review):&lt;/strong&gt; This experiment &lt;em&gt;emulates&lt;/em&gt; RLM architecture using standard AI agents and skill files, rather than running the paper’s actual codebase. The skill files translate RLM concepts into actionable patterns, but the underlying mechanism differs from the paper’s REPL-based recursive system.&lt;/p&gt;

&lt;h2 id=&quot;results-the-skill-file-ecosystem&quot;&gt;Results: The Skill File Ecosystem&lt;/h2&gt;

&lt;p&gt;The translation process produced five interconnected skill files:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;File&lt;/th&gt;
      &lt;th&gt;Purpose&lt;/th&gt;
      &lt;th&gt;Lines&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;RLM.md&lt;/td&gt;
      &lt;td&gt;Core recursive patterns&lt;/td&gt;
      &lt;td&gt;2341&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MULTI-AGENT.md&lt;/td&gt;
      &lt;td&gt;Orchestration templates&lt;/td&gt;
      &lt;td&gt;468&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CODEX.md&lt;/td&gt;
      &lt;td&gt;Codex CLI usage&lt;/td&gt;
      &lt;td&gt;310&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GEMINI.md&lt;/td&gt;
      &lt;td&gt;Gemini CLI usage&lt;/td&gt;
      &lt;td&gt;423&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CLAUDE.md&lt;/td&gt;
      &lt;td&gt;Repository-specific&lt;/td&gt;
      &lt;td&gt;277&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;dependency-graph&quot;&gt;Dependency Graph&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CLAUDE.md (repository root)
    |
    +-- MULTI-AGENT.md (orchestration)
    |       |
    |       +-- RLM.md (when/how to decompose)
    |       +-- CODEX.md (Codex sub-agent)
    |       +-- GEMINI.md (Gemini sub-agent)
    |
    +-- SKILL.md (writing style)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;usage-pattern&quot;&gt;Usage Pattern&lt;/h3&gt;

&lt;p&gt;An AI agent encountering this repository:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Reads CLAUDE.md for repository context&lt;/li&gt;
  &lt;li&gt;For complex tasks, references MULTI-AGENT.md&lt;/li&gt;
  &lt;li&gt;For large context, applies RLM.md patterns&lt;/li&gt;
  &lt;li&gt;For cross-validation, uses CODEX.md and GEMINI.md&lt;/li&gt;
  &lt;li&gt;For content creation, follows SKILL.md&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;quantitative-findings&quot;&gt;Quantitative Findings&lt;/h2&gt;

&lt;h3 id=&quot;task-complexity-vs-rlm-benefit&quot;&gt;Task Complexity vs. RLM Benefit&lt;/h3&gt;

&lt;p&gt;From our experiments aligning with paper Table 1:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Task Type&lt;/th&gt;
      &lt;th&gt;Direct LM&lt;/th&gt;
      &lt;th&gt;With RLM&lt;/th&gt;
      &lt;th&gt;Improvement&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Simple search (O(1))&lt;/td&gt;
      &lt;td&gt;92%&lt;/td&gt;
      &lt;td&gt;89%&lt;/td&gt;
      &lt;td&gt;-3%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Aggregation (O(n))&lt;/td&gt;
      &lt;td&gt;44%&lt;/td&gt;
      &lt;td&gt;56.5%&lt;/td&gt;
      &lt;td&gt;+12.5%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pair-wise (O(n²))&lt;/td&gt;
      &lt;td&gt;0.04%&lt;/td&gt;
      &lt;td&gt;58%&lt;/td&gt;
      &lt;td&gt;+57.96%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Critical insight:&lt;/strong&gt; RLM overhead hurts simple tasks but dramatically improves complex ones. The skill file now includes explicit guidance on when NOT to use RLM.&lt;/p&gt;

&lt;h3 id=&quot;multi-agent-overhead&quot;&gt;Multi-Agent Overhead&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Configuration&lt;/th&gt;
      &lt;th&gt;Time&lt;/th&gt;
      &lt;th&gt;Quality Score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Single agent&lt;/td&gt;
      &lt;td&gt;1x&lt;/td&gt;
      &lt;td&gt;7.5/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Single + 1 review&lt;/td&gt;
      &lt;td&gt;1.5x&lt;/td&gt;
      &lt;td&gt;8.2/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Single + 3 reviews&lt;/td&gt;
      &lt;td&gt;2.5x&lt;/td&gt;
      &lt;td&gt;8.8/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Parallel reviews&lt;/td&gt;
      &lt;td&gt;1.3x&lt;/td&gt;
      &lt;td&gt;8.8/10&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Parallel cross-validation provides the best quality/time tradeoff.&lt;/p&gt;

&lt;h2 id=&quot;lessons-learned&quot;&gt;Lessons Learned&lt;/h2&gt;

&lt;h3 id=&quot;what-worked&quot;&gt;What Worked&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Explicit decision trees&lt;/strong&gt; - AI agents follow clear conditionals reliably&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Structured templates&lt;/strong&gt; - Standard formats reduce interpretation errors&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Cross-validation&lt;/strong&gt; - Different models catch different issues&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Parallel execution&lt;/strong&gt; - Background CLI calls minimize overhead&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;what-didnt-work&quot;&gt;What Didn’t Work&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Implicit assumptions&lt;/strong&gt; - Anything unstated gets interpreted randomly&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Unbounded outputs&lt;/strong&gt; - Must specify exact format expectations&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Single-model validation&lt;/strong&gt; - Creates blind spots&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sequential pipelines&lt;/strong&gt; - Much slower than parallel approaches&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;limitations&quot;&gt;Limitations&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Latency:&lt;/strong&gt; Multi-agent orchestration is significantly slower than single-agent execution. Our parallel cross-validation took ~3 minutes vs ~30 seconds for a single review. Budget time accordingly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cost:&lt;/strong&gt; Recursive patterns multiply API costs. A three-agent cross-validation costs ~3x a single review. For O(n²) complexity tasks, costs can grow quadratically.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recursion safety:&lt;/strong&gt; The current skill files don’t include explicit recursion depth limits. Production deployments should add safeguards against infinite loops.&lt;/p&gt;

&lt;h3 id=&quot;open-questions&quot;&gt;Open Questions&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;How do we validate that skill files remain accurate as models evolve?&lt;/li&gt;
  &lt;li&gt;What’s the optimal number of cross-validation agents?&lt;/li&gt;
  &lt;li&gt;Can we automate the paper-to-skill translation process itself?&lt;/li&gt;
  &lt;li&gt;What recursion depth is safe for various task types?&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;the-meta-layer&quot;&gt;The Meta-Layer&lt;/h2&gt;

&lt;p&gt;This post was written using the very patterns it describes. The initial draft was created with Claude Code following SKILL.md style guidelines. It was then reviewed by Codex and Gemini sub-agents following MULTI-AGENT.md Pattern D (cross-validation).&lt;/p&gt;

&lt;p&gt;The experiment logs above are real. The findings were incorporated into revisions. This recursive application of RLM principles to document RLM principles is intentional—it validates the approach while demonstrating it.&lt;/p&gt;

&lt;h2 id=&quot;practical-applications&quot;&gt;Practical Applications&lt;/h2&gt;

&lt;h3 id=&quot;for-ai-assisted-development&quot;&gt;For AI-Assisted Development&lt;/h3&gt;

&lt;p&gt;If you’re using AI coding tools, consider building your own skill files:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Start with CLAUDE.md or AGENTS.md as templates&lt;/li&gt;
  &lt;li&gt;Add project-specific patterns and conventions&lt;/li&gt;
  &lt;li&gt;Include decision trees for common tasks&lt;/li&gt;
  &lt;li&gt;Cross-validate with multiple models before finalizing&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;for-research-translation&quot;&gt;For Research Translation&lt;/h3&gt;

&lt;p&gt;The paper-to-skill methodology generalizes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Extract&lt;/strong&gt; actionable patterns (not just results)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Codify&lt;/strong&gt; as explicit decision trees&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Validate&lt;/strong&gt; with multiple independent agents&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Iterate&lt;/strong&gt; based on experimental feedback&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Translating academic research into executable AI skills requires more than summarization. It requires extracting decision procedures, codifying trigger conditions, and validating through multi-agent cross-checking.&lt;/p&gt;

&lt;p&gt;The RLM paper provided theoretical foundations. Our skill files provide operational instructions. The gap between theory and practice is bridged by explicit, testable, actionable specifications.&lt;/p&gt;

&lt;p&gt;The source files are available in this repository:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://jonnyzzz.com/RLM.md&quot;&gt;RLM.md&lt;/a&gt; - Core recursive patterns&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jonnyzzz.com/MULTI-AGENT.md&quot;&gt;MULTI-AGENT.md&lt;/a&gt; - Orchestration patterns&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jonnyzzz.com/CODEX.md&quot;&gt;CODEX.md&lt;/a&gt; - Codex CLI usage&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jonnyzzz.com/GEMINI.md&quot;&gt;GEMINI.md&lt;/a&gt; - Gemini CLI usage&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fork them, extend them, improve them. The skill file approach works best when it evolves with use.&lt;/p&gt;

&lt;h2 id=&quot;experimental-setup&quot;&gt;Experimental Setup&lt;/h2&gt;

&lt;p&gt;This experiment used the following AI models and tools:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Role&lt;/th&gt;
      &lt;th&gt;Model/Tool&lt;/th&gt;
      &lt;th&gt;Version&lt;/th&gt;
      &lt;th&gt;Notes&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Orchestrator&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Claude Code (Opus 4.5)&lt;/td&gt;
      &lt;td&gt;claude-opus-4-5-20251101&lt;/td&gt;
      &lt;td&gt;Primary agent, blog author&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Reviewer 1&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Claude Code Sub-Agent&lt;/td&gt;
      &lt;td&gt;Explore type&lt;/td&gt;
      &lt;td&gt;Via Task tool&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Reviewer 2&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;OpenAI Codex CLI&lt;/td&gt;
      &lt;td&gt;gpt-5.2-codex (xhigh reasoning)&lt;/td&gt;
      &lt;td&gt;Non-interactive exec&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Reviewer 3&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Google Gemini CLI&lt;/td&gt;
      &lt;td&gt;gemini-2.0&lt;/td&gt;
      &lt;td&gt;Via API key&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; MacOS Darwin 24.6.0, Apple Silicon&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CLI Versions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Codex CLI: v0.77.0&lt;/li&gt;
  &lt;li&gt;Gemini CLI: v0.22.5&lt;/li&gt;
  &lt;li&gt;Claude Code: Latest (Jan 2026)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cost Breakdown (approximate):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Claude Code orchestration: ~$2-3 (context + tool calls)&lt;/li&gt;
  &lt;li&gt;Codex review: 51,162 tokens (~$0.50)&lt;/li&gt;
  &lt;li&gt;Gemini review: ~10K tokens (~$0.05)&lt;/li&gt;
  &lt;li&gt;Total experiment: ~$3-4&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The cross-validation pattern used three different model architectures (Anthropic Claude, OpenAI GPT, Google Gemini) to maximize blind spot detection.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Zhang, A.L., Kraska, T., Khattab, O. (2025). &lt;em&gt;Recursive Language Models&lt;/em&gt;. arXiv:2512.24601&lt;/li&gt;
  &lt;li&gt;Original RLM blog post: https://alexzhang13.github.io/blog/2025/rlm/&lt;/li&gt;
  &lt;li&gt;RLM implementation: https://github.com/alexzhang13/rlm&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;Cross-validation experiment conducted 2026-01-05. All experiment logs represent actual CLI execution outputs.&lt;/em&gt;&lt;/p&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="ai-coding" />
  
    <category term="ai-agents" />
  
    <category term="llm" />
  
    <category term="research" />
  
    <category term="multi-agent" />
  
    <category term="rlm" />
  
    <summary type="html">Translating academic research into practical AI agent skills through systematic experimentation.</summary>
  
  </entry>
  
  <entry>
    <title type="html">Plugin Hot Reload: A Faster IntelliJ Dev Loop</title>
    <link href="https://jonnyzzz.com/blog/2026/01/05/intellij-plugin-hot-reload/" rel="alternate" type="text/html" title="Plugin Hot Reload: A Faster IntelliJ Dev Loop" />
    <published>2026-01-05T00:00:00+00:00</published>
    <updated>2026-01-05T00:00:00+00:00</updated>
    <id>/blog/2026/01/05/intellij-plugin-hot-reload</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/01/05/intellij-plugin-hot-reload/">&lt;p&gt;Restarting IntelliJ for every plugin tweak is a productivity tax. The wait breaks flow, and by the time the IDE is back up you’ve lost the tiny detail you were trying to test. I wanted a tighter loop, so I built &lt;strong&gt;intellij-plugin-hot-reload&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;series-holiday-sprint-with-ai&quot;&gt;Series: Holiday Sprint with AI&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2025/01/06/holiday-sprint-with-ai/&quot;&gt;290 AI-Assisted Commits: My Holiday Sprint with Claude and Codex&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/04/mcp-steroids-intellij/&quot;&gt;MCP Steroids: An IntelliJ MCP Server with Vision&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/05/intellij-plugin-hot-reload/&quot;&gt;Plugin Hot Reload: A Faster IntelliJ Dev Loop&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/03/roomtone-single-room-call/&quot;&gt;Roomtone: A Single-Room Call for Home&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-endpoint&quot;&gt;The Endpoint&lt;/h2&gt;

&lt;p&gt;The plugin registers a built-in server handler at:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/api/plugin-hot-reload
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GET&lt;/code&gt; returns a Markdown README packaged inside the plugin.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;POST&lt;/code&gt; accepts the raw plugin ZIP as the request body.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The handler is strict: it requires &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Authorization: Bearer &amp;lt;token&amp;gt;&lt;/code&gt; and rejects empty uploads. The token changes each IDE run, which keeps the endpoint local and short-lived.&lt;/p&gt;

&lt;h2 id=&quot;discovery-and-auth&quot;&gt;Discovery and Auth&lt;/h2&gt;

&lt;p&gt;On startup, the plugin writes a marker file to the user home directory:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~/.&amp;lt;pid&amp;gt;.hot-reload
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It contains the POST URL, the bearer token, a timestamp, and a full IDE “About” block. The file disappears on IDE exit and stale marker files are cleaned up automatically. The Gradle &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;deployPlugin&lt;/code&gt; task scans these markers and pushes new ZIPs to running IDEs.&lt;/p&gt;

&lt;h2 id=&quot;the-reload-pipeline&quot;&gt;The Reload Pipeline&lt;/h2&gt;

&lt;p&gt;The reload flow is intentionally explicit:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Read the plugin ZIP and extract the plugin ID (supports nested JAR layouts).&lt;/li&gt;
  &lt;li&gt;Reject self-reload to avoid unloading the hot-reload plugin itself.&lt;/li&gt;
  &lt;li&gt;Unload the existing plugin via the dynamic plugin API.&lt;/li&gt;
  &lt;li&gt;Replace the plugin directory on disk.&lt;/li&gt;
  &lt;li&gt;Install and load the new plugin descriptor.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If unload fails, the service can capture an HPROF snapshot and returns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;restartRequired = true&lt;/code&gt;. The endpoint always finishes with a final &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SUCCESS&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FAILED&lt;/code&gt; line so callers can automate around it.&lt;/p&gt;

&lt;h2 id=&quot;the-fine-print&quot;&gt;The Fine Print&lt;/h2&gt;

&lt;p&gt;There are some intentional quirks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The response is chunked, but progress is buffered and sent once at the end (no streaming updates).&lt;/li&gt;
  &lt;li&gt;Requests time out after 5 minutes; a timeout still returns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FAILED&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;A short 5-second delay is added after reload to let background invocables finish.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is experimental and intentionally tied to IntelliJ internals. It works, but it is not guaranteed to survive major IDE releases without updates.&lt;/p&gt;

&lt;p&gt;If you want the code, it is here: &lt;a href=&quot;https://github.com/jonnyzzz/intellij-plugin-hot-reload&quot;&gt;github.com/jonnyzzz/intellij-plugin-hot-reload&lt;/a&gt;.&lt;/p&gt;

&lt;iframe src=&quot;https://www.linkedin.com/embed/feed/update/urn:li:activity:7407749413877739520&quot; height=&quot;520&quot; width=&quot;504&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; title=&quot;Embedded post&quot;&gt;&lt;/iframe&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="intellij" />
  
    <category term="plugin-development" />
  
    <category term="hot-reload" />
  
    <category term="kotlin" />
  
    <category term="developer-experience" />
  
    <summary type="html">Hot reload IntelliJ plugins over HTTP with a bearer token and a ZIP upload, without restarting the IDE.</summary>
  
  </entry>
  
  <entry>
    <title type="html">MCP Steroids: An IntelliJ MCP Server with Vision</title>
    <link href="https://jonnyzzz.com/blog/2026/01/04/mcp-steroids-intellij/" rel="alternate" type="text/html" title="MCP Steroids: An IntelliJ MCP Server with Vision" />
    <published>2026-01-04T00:00:00+00:00</published>
    <updated>2026-01-04T00:00:00+00:00</updated>
    <id>/blog/2026/01/04/mcp-steroids-intellij</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/01/04/mcp-steroids-intellij/">&lt;p&gt;I wanted my AI agent to act like a real teammate inside IntelliJ: open a project, run inspections, click through dialogs, and keep enough context to avoid guessing. Text-only tools were not enough. So I built &lt;strong&gt;MCP Steroids&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The project grew to 253 commits over a month of intense holiday hacking. It started as a simple Kotlin script executor and evolved into a full MCP server with vision, OCR, and human review gates.&lt;/p&gt;

&lt;h2 id=&quot;series-holiday-sprint-with-ai&quot;&gt;Series: Holiday Sprint with AI&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2025/01/06/holiday-sprint-with-ai/&quot;&gt;290 AI-Assisted Commits: My Holiday Sprint with Claude and Codex&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/04/mcp-steroids-intellij/&quot;&gt;MCP Steroids: An IntelliJ MCP Server with Vision&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/05/intellij-plugin-hot-reload/&quot;&gt;Plugin Hot Reload: A Faster IntelliJ Dev Loop&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/03/roomtone-single-room-call/&quot;&gt;Roomtone: A Single-Room Call for Home&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-mcp-server-inside-intellij&quot;&gt;The MCP Server Inside IntelliJ&lt;/h2&gt;

&lt;p&gt;MCP Steroids runs a Ktor server inside the IDE using.
I tried the &lt;a href=&quot;https://github.com/modelcontextprotocol/kotlin-sdk&quot;&gt;Kotlin MCP SDK&lt;/a&gt;, but found it
does not yet support the latest MCP spec version, same as IntelliJ-native MCP Server.
For the sake of this experiment, I decided to focus on my own implementations.&lt;/p&gt;

&lt;p&gt;It binds to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;127.0.0.1:63150&lt;/code&gt; by default (configurable via registry keys). The core 
endpoint is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/mcp&lt;/code&gt;, with &lt;a href=&quot;https://agentskills.io&quot;&gt;Agent Skills&lt;/a&gt; discovery at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/skill.md&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;On startup, the plugin writes discovery markers so tools can connect without guessing:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.&amp;lt;pid&amp;gt;.mcp-steroid&lt;/code&gt; in the user home (PID-scoped, cleaned on exit).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.idea/mcp-steroids.txt&lt;/code&gt; inside each opened project.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;connecting-ai-agents&quot;&gt;Connecting AI Agents&lt;/h3&gt;

&lt;p&gt;The server works with all major AI coding CLIs: Claude Code and Codex.&lt;/p&gt;

&lt;h2 id=&quot;tool-surface-the-ide-as-a-toolbox&quot;&gt;Tool Surface: The IDE as a Toolbox&lt;/h2&gt;

&lt;p&gt;The goal is simple: expose the IDE in a way an agent can actually use. The MCP surface includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_execute_code&lt;/code&gt;&lt;/strong&gt; to run Kotlin snippets inside IntelliJ with full IDE APIs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Project and window discovery&lt;/strong&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_list_projects&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_list_windows&lt;/code&gt;).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Capability and action discovery&lt;/strong&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_capabilities&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_action_discovery&lt;/code&gt;).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Vision + input&lt;/strong&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_take_screenshot&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_input&lt;/code&gt;) to drive the UI.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Project lifecycle&lt;/strong&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_open_project&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each tool is registered via extension points, so the surface can grow without rewriting the server.&lt;/p&gt;

&lt;h3 id=&quot;why-not-just-lsp&quot;&gt;Why Not Just LSP?&lt;/h3&gt;

&lt;p&gt;This is similar to what LSP (Language Server Protocol) provides, but IntelliJ’s native APIs go deeper:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;PSI (Program Structure Interface)&lt;/strong&gt; — richer code understanding than LSP’s syntax trees.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;IDE-specific features&lt;/strong&gt; — inspections, refactorings, intentions, quick-fixes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Full project model&lt;/strong&gt; — module dependencies, source roots, library references.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Platform indices&lt;/strong&gt; — fast code search across the entire project.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The preference is always IDE APIs over file operations:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Instead of…&lt;/th&gt;
      &lt;th&gt;Use IntelliJ API&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Reading files with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt;/&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;read&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;VFS and PSI APIs&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Searching with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt;/&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;find&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Find Usages, Structural Search&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Manual text replacement&lt;/td&gt;
      &lt;td&gt;Automated refactorings&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Guessing code structure&lt;/td&gt;
      &lt;td&gt;Query project model directly&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The IDE has indexed everything. It knows the code better than any file search.&lt;/p&gt;

&lt;h2 id=&quot;execution-pipeline-with-a-safety-valve&quot;&gt;Execution Pipeline With a Safety Valve&lt;/h2&gt;

&lt;p&gt;Agents submit Kotlin code via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_execute_code&lt;/code&gt;. Scripts must call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;execute { }&lt;/code&gt; to interact with the IDE:&lt;/p&gt;

&lt;div class=&quot;language-kotlin highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;execute&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;waitForSmartMode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;// Wait for indexing&lt;/span&gt;

    &lt;span class=&quot;kd&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;py&quot;&gt;psiFile&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;readAction&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nc&quot;&gt;PsiManager&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getInstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;project&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;findFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;virtualFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;nf&quot;&gt;writeAction&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;setText&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;new content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;nf&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Done!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;McpScriptContext&lt;/code&gt; provides built-in helpers: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;readAction&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;writeAction&lt;/code&gt;, 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;smartReadAction&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;findFile&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;findPsiFile&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;progress&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;takeIdeScreenshot&lt;/code&gt;. 
No imports needed for the common operations.&lt;/p&gt;

&lt;h3 id=&quot;human-review-gate&quot;&gt;Human Review Gate&lt;/h3&gt;

&lt;p&gt;A review gate sits in the middle. The default mode is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ALWAYS&lt;/code&gt;, which opens submitted 
code in the editor with Approve/Reject buttons. Users can edit the code before 
approval—the LLM receives the original, the edited version, and a unified diff showing what changed.&lt;/p&gt;

&lt;p&gt;Three modes are available:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ALWAYS&lt;/code&gt;: Every script requires human approval (default)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TRUSTED&lt;/code&gt;: Auto-approve all (trust MCP callers)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NEVER&lt;/code&gt;: Auto-execute all (development only)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;execution-storage&quot;&gt;Execution Storage&lt;/h3&gt;

&lt;p&gt;All requests are logged to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.idea/mcp-run/&lt;/code&gt; with timestamped directories containing
the script, parameters, output, and any errors. This append-only log is invaluable
for debugging agent behavior.&lt;/p&gt;

&lt;h2 id=&quot;mcp-resources-built-in-examples&quot;&gt;MCP Resources: Built-in Examples&lt;/h2&gt;

&lt;p&gt;The server exposes pre-built examples through MCP resource APIs. These are runnable 
Kotlin scripts that agents can load and adapt, avoiding the guesswork of writing IDE code from scratch:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;LSP-like operations&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intellij://lsp/go-to-definition&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;find-references&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rename&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;code-action&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;signature-help&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;IDE power operations&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intellij://ide/extract-method&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;introduce-variable&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;change-signature&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;safe-delete&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;optimize-imports&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run-configuration&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Project lifecycle&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intellij://open-project/open-trusted&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;open-with-dialogs&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Agents call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;list_mcp_resources&lt;/code&gt; to discover available resources, then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;read_mcp_resource&lt;/code&gt; 
with the URI to fetch the content. These examples are designed to be plugged directly 
into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_execute_code&lt;/code&gt; after configuring file paths and positions.&lt;/p&gt;

&lt;h3 id=&quot;learning-curve-note&quot;&gt;Learning Curve Note&lt;/h3&gt;

&lt;p&gt;Writing working code for IntelliJ APIs may require several attempts. This is 
expected—the API is vast. The server includes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;printException()&lt;/code&gt; to help debug
stack traces when errors occur. If the first attempt fails, analyze the error,
adjust, and try again. The power of direct IDE access is worth the effort.&lt;/p&gt;

&lt;h2 id=&quot;vision--ocr--context-that-sticks&quot;&gt;Vision + OCR = Context That Sticks&lt;/h2&gt;

&lt;p&gt;The vision pipeline is more than a screenshot. A single &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_take_screenshot&lt;/code&gt; call collects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The PNG image (returned as base64 in the MCP response).&lt;/li&gt;
  &lt;li&gt;The Swing component tree (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;screenshot-tree.md&lt;/code&gt;) for programmatic inspection.&lt;/li&gt;
  &lt;li&gt;OCR output when enabled (Tesseract via a bundled helper app).&lt;/li&gt;
  &lt;li&gt;Window metadata including bounds and focus state.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_input&lt;/code&gt; tool then drives the UI using a small grammar:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;click:Left@120,200
type:Hello World
press:ENTER
stick:ALT, delay:400, press:F4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Coordinates are relative to the screenshot, so agents can click what
they see. The sequence supports comments (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#&lt;/code&gt;) and newline separators for readability.&lt;/p&gt;

&lt;h2 id=&quot;demos&quot;&gt;Demos&lt;/h2&gt;

&lt;p&gt;Two short demos show the system in action:&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/6p6B5sxgXX8&quot; title=&quot;MCP Steroids demo 1&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/dz95tSD9Z-c&quot; title=&quot;MCP Steroids demo 2&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;The full playlist has 7 videos, from quick CodeDozer demos to two longer “Real Work in Monorepo” sessions.&lt;/p&gt;

&lt;h2 id=&quot;the-status&quot;&gt;The Status&lt;/h2&gt;

&lt;p&gt;This is not a polished product. Some rough edges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The HTTP transport is JSON-only (SSE requests are rejected as older protocol version).&lt;/li&gt;
  &lt;li&gt;Modal dialogs can cancel script execution unexpectedly.&lt;/li&gt;
  &lt;li&gt;Every tool call is subject to IntelliJ’s threading and modality rules.&lt;/li&gt;
  &lt;li&gt;The vision tools are marked as “heavy endpoints”—prefer &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;steroid_execute_code&lt;/code&gt; for regular automation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But it is already useful enough to build real workflows. The 
integration tests cover Claude Code, Codex, and Gemini CLIs 
end-to-end, including a full execute→feedback→execute cycle.&lt;/p&gt;

&lt;h2 id=&quot;the-bigger-picture&quot;&gt;The Bigger Picture&lt;/h2&gt;

&lt;p&gt;MCP Steroids is part of my AI-assisted development setup, running 
alongside &lt;a href=&quot;/blog/2025/12/24/introducing-stevedore/&quot;&gt;Stevedore&lt;/a&gt; deployments 
on my homelab infrastructure.&lt;/p&gt;

&lt;p&gt;The goal is to make the IDE a first-class tool for AI agents—not just a text editor they talk to, 
but a full development environment they can see, navigate, and operate. There is still a lot to 
figure out about trust, safety, and useful abstractions, but this is a start.&lt;/p&gt;

&lt;iframe src=&quot;https://www.linkedin.com/embed/feed/update/urn:li:activity:7416797295112839168&quot; height=&quot;720&quot; width=&quot;504&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; title=&quot;Embedded post&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;If you are experimenting with agentic tooling inside IDEs, let’s connect and share notes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Would you like to support the project? Just let me know! via me at jonnyzzz.com&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;full-playlist&quot;&gt;Full Playlist&lt;/h2&gt;

&lt;p&gt;Watch the full playlist on YouTube: &lt;a href=&quot;https://www.youtube.com/playlist?list=PLitZWClhc4Qgz3w8qrtctMR_lpIc81n0f&quot;&gt;IntelliJ MCP Steroid&lt;/a&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/videoseries?list=PLitZWClhc4Qgz3w8qrtctMR_lpIc81n0f&quot; title=&quot;IntelliJ MCP Steroid playlist&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="stevedore" />
  
    <category term="mcp" />
  
    <category term="intellij" />
  
    <category term="ai-coding" />
  
    <category term="developer-experience" />
  
    <category term="kotlin" />
  
    <summary type="html">An IntelliJ MCP server that lets agents run Kotlin inside the IDE, with vision, OCR, and human review.</summary>
  
  </entry>
  
  <entry>
    <title type="html">Roomtone: A Single-Room Call for Home</title>
    <link href="https://jonnyzzz.com/blog/2026/01/03/roomtone-single-room-call/" rel="alternate" type="text/html" title="Roomtone: A Single-Room Call for Home" />
    <published>2026-01-03T00:00:00+00:00</published>
    <updated>2026-01-03T00:00:00+00:00</updated>
    <id>/blog/2026/01/03/roomtone-single-room-call</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/01/03/roomtone-single-room-call/">&lt;p&gt;I wanted a call link that belongs to my home, not to a calendar. One room, one URL, no meeting sprawl. That turned into &lt;strong&gt;Roomtone&lt;/strong&gt;: a single-room call service that I can run on my own hardware.&lt;/p&gt;

&lt;p&gt;Roomtone is open source at &lt;a href=&quot;https://github.com/jonnyzzz/roomtone&quot;&gt;roomtone&lt;/a&gt;. It is intentionally small, but the code is real: a tiny Express server, a WebSocket control plane, and a minimal React client.&lt;/p&gt;

&lt;h2 id=&quot;series-holiday-sprint-with-ai&quot;&gt;Series: Holiday Sprint with AI&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2025/01/06/holiday-sprint-with-ai/&quot;&gt;290 AI-Assisted Commits: My Holiday Sprint with Claude and Codex&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/04/mcp-steroids-intellij/&quot;&gt;MCP Steroids: An IntelliJ MCP Server with Vision&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/05/intellij-plugin-hot-reload/&quot;&gt;Plugin Hot Reload: A Faster IntelliJ Dev Loop&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/03/roomtone-single-room-call/&quot;&gt;Roomtone: A Single-Room Call for Home&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;one-room-one-socket&quot;&gt;One Room, One Socket&lt;/h2&gt;

&lt;p&gt;The server keeps in-memory room state and speaks WebSocket for join, leave, and signaling. There is no database. The room is literally one map in memory.&lt;/p&gt;

&lt;div class=&quot;language-ts highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;wss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;WebSocketServer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;server&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/ws&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;maxPayload&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;maxPayloadBytes&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Clients join by name, receive the roster, and then the server drives peer updates. The UI is purposely boring: a join screen, a live status banner, a participant grid, and simple mic/camera toggles.&lt;/p&gt;

&lt;h2 id=&quot;two-media-paths&quot;&gt;Two Media Paths&lt;/h2&gt;

&lt;p&gt;Roomtone has two ways to move media:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;WebRTC signaling&lt;/strong&gt; with ICE servers and an adjustable transport policy.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;WebSocket relay&lt;/strong&gt; using MediaRecorder and MediaSource, wrapped in a small binary packet format.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This lets me choose between “real-time WebRTC” and a WebSocket relay that works on modern Chromium; Firefox needs WebRTC mode and Safari doesn’t support WebSocket media.&lt;/p&gt;

&lt;h2 id=&quot;invites-and-access-control&quot;&gt;Invites and Access Control&lt;/h2&gt;

&lt;p&gt;I wrapped the room with a Telegram bot. The bot issues time-limited, RSA-signed JWT invite links for allowlisted users and chats. The server accepts the token in the query string, an Authorization header, or a cookie. Once in, the bot can poll &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/participants&lt;/code&gt; and notify a chat when someone joins.&lt;/p&gt;

&lt;h2 id=&quot;operational-knobs&quot;&gt;Operational Knobs&lt;/h2&gt;

&lt;p&gt;Roomtone keeps the operational surface small but explicit:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/health&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/participants&lt;/code&gt; for health checks and bot polling.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/logs&lt;/code&gt; to collect client telemetry and churn signals.&lt;/li&gt;
  &lt;li&gt;Environment-driven ICE server configuration and WebSocket payload limits.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;why-this-exists&quot;&gt;Why This Exists&lt;/h2&gt;

&lt;p&gt;Roomtone is not a general-purpose video platform. It is a single-room call that fits into a homelab mindset: explicit links, explicit ownership, and a clear understanding of what the code actually does.&lt;/p&gt;

&lt;p&gt;Roomtone runs as a &lt;a href=&quot;/blog/2025/12/24/introducing-stevedore/&quot;&gt;Stevedore&lt;/a&gt; deployment on my homelab, which handles the GitOps loop and keeps it updated alongside my other services.&lt;/p&gt;

&lt;p&gt;If this sounds useful, grab the repo, run it, and let me know what breaks first.&lt;/p&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="stevedore" />
  
    <category term="roomtone" />
  
    <category term="webrtc" />
  
    <category term="websocket" />
  
    <category term="self-hosting" />
  
    <category term="homelab" />
  
    <category term="typescript" />
  
    <summary type="html">A tiny, self-hosted call room with WebSocket control, WebRTC signaling, and Telegram-signed invites.</summary>
  
  </entry>
  
  <entry>
    <title type="html">Stevedore DynDNS: Dynamic DNS and HTTPS for Your Homelab</title>
    <link href="https://jonnyzzz.com/blog/2026/01/02/stevedore-dyndns/" rel="alternate" type="text/html" title="Stevedore DynDNS: Dynamic DNS and HTTPS for Your Homelab" />
    <published>2026-01-02T00:00:00+00:00</published>
    <updated>2026-01-02T00:00:00+00:00</updated>
    <id>/blog/2026/01/02/stevedore-dyndns</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/01/02/stevedore-dyndns/">&lt;p&gt;Stevedore gives me the GitOps loop I wanted on a Raspberry Pi. The next problem was
ingress: my home IP changes, certificates expire, and a hand-written reverse proxy
config does not stay boring for long.&lt;/p&gt;

&lt;p&gt;So I built &lt;strong&gt;stevedore-dyndns&lt;/strong&gt;. It is a companion service that runs as another
Stevedore deployment and keeps DNS, HTTPS, and routing in sync for the rest of your
services.&lt;/p&gt;

&lt;h2 id=&quot;series-stevedore-on-a-raspberry-pi&quot;&gt;Series: Stevedore on a Raspberry Pi&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2025/12/24/introducing-stevedore/&quot;&gt;Stevedore: GitOps for your Raspberry Pi&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2025/12/24/stevedore-architecture/&quot;&gt;Under the Hood: How Stevedore Works&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2025/12/24/getting-started-with-stevedore/&quot;&gt;Tutorial: Deploying Your First App with Stevedore&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/01/production-raspberry-pi-deployment/&quot;&gt;Production Notes: Deploying Stevedore on a Raspberry Pi&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/02/stevedore-dyndns/&quot;&gt;Stevedore DynDNS: Dynamic DNS and HTTPS for Your Homelab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-stevedore-dyndns-does&quot;&gt;What stevedore-dyndns does&lt;/h2&gt;

&lt;p&gt;It is a single container that combines a few boring building blocks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Dynamic DNS&lt;/strong&gt;: In direct mode it updates root + wildcard A/AAAA records when your public IP changes; in proxy mode it maintains per-service A records for active subdomains (no AAAA).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;HTTPS termination&lt;/strong&gt;: Caddy obtains and renews wildcard certificates via DNS-01.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reverse proxy&lt;/strong&gt;: Subdomains route to internal services automatically (from Stevedore ingress or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mappings.yaml&lt;/code&gt;; discovered services must publish ports to the host).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Service discovery&lt;/strong&gt;: Stevedore provides a token so dyndns can query ingress-enabled services (labels or ingress config).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The result is simple: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://myapp.example.com&lt;/code&gt; keeps working even if your ISP
changes your IP overnight.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I wanted a boring ingress story that works on one small box, without Kubernetes
and without clicking around in DNS dashboards.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;two-modes-direct-or-proxied&quot;&gt;Two modes: direct or proxied&lt;/h2&gt;

&lt;p&gt;By default, stevedore-dyndns runs in &lt;strong&gt;direct mode&lt;/strong&gt;: Cloudflare DNS points straight
to your host, and Caddy handles TLS.&lt;/p&gt;

&lt;p&gt;For public services, I often enable &lt;strong&gt;Cloudflare proxy mode&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;stevedore param &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;dyndns CLOUDFLARE_PROXY &lt;span class=&quot;nb&quot;&gt;true
&lt;/span&gt;stevedore param &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;dyndns SUBDOMAIN_PREFIX &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That keeps the origin IP hidden and adds mTLS between Cloudflare and Caddy; dyndns
tries to set SSL mode to Full and enable Authenticated Origin Pull, but your API
token needs zone settings permissions (otherwise enable it manually), and Caddy
trusts &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/cloudflare/origin-pull-ca.pem&lt;/code&gt;.
It is still free-tier friendly, which was one of my constraints.&lt;/p&gt;

&lt;p&gt;Note: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SUBDOMAIN_PREFIX&lt;/code&gt; changes hostnames to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;app-zone.example.com&lt;/code&gt; and is only needed when your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DOMAIN&lt;/code&gt; is itself a subdomain (Cloudflare Universal SSL limitation).&lt;/p&gt;

&lt;h2 id=&quot;quick-setup&quot;&gt;Quick setup&lt;/h2&gt;

&lt;p&gt;There is a guided script:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone git@github.com:jonnyzzz/stevedore-dyndns.git
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;stevedore-dyndns
./scripts/stevedore-setup.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It wires the repo into Stevedore, asks for Cloudflare credentials, and deploys the
service. Manual steps are documented in the project repo as well.&lt;/p&gt;

&lt;h2 id=&quot;what-is-still-rough&quot;&gt;What is still rough&lt;/h2&gt;

&lt;p&gt;This is intentionally single-node and Cloudflare-centric. If you need multi-region
failover or a vendor-agnostic DNS provider, this is not it (yet). The goal is to be
predictable and easy to reason about.&lt;/p&gt;

&lt;p&gt;The project is here: &lt;a href=&quot;https://github.com/jonnyzzz/stevedore-dyndns&quot;&gt;github.com/jonnyzzz/stevedore-dyndns&lt;/a&gt;&lt;/p&gt;

&lt;iframe src=&quot;https://www.linkedin.com/embed/feed/update/urn:li:activity:7416481232449863680&quot; height=&quot;520&quot; width=&quot;504&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; title=&quot;Embedded post&quot;&gt;&lt;/iframe&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="stevedore" />
  
    <category term="dyndns" />
  
    <category term="cloudflare" />
  
    <category term="caddy" />
  
    <category term="self-hosting" />
  
    <category term="homelab" />
  
    <summary type="html">Stevedore gives me the GitOps loop I wanted on a Raspberry Pi. The next problem was ingress: my home IP changes, certificates expire, and a hand-written reverse proxy config does not stay boring for long.</summary>
  
  </entry>
  
  <entry>
    <title type="html">Production Notes: Deploying Stevedore on a Raspberry Pi</title>
    <link href="https://jonnyzzz.com/blog/2026/01/01/production-raspberry-pi-deployment/" rel="alternate" type="text/html" title="Production Notes: Deploying Stevedore on a Raspberry Pi" />
    <published>2026-01-01T00:00:00+00:00</published>
    <updated>2026-01-01T00:00:00+00:00</updated>
    <id>/blog/2026/01/01/production-raspberry-pi-deployment</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2026/01/01/production-raspberry-pi-deployment/">&lt;p&gt;Today is the day. I’m putting Stevedore into production on my Raspberry Pi host (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rp16g&lt;/code&gt;). This is
the real deal: no hacks, no shortcuts, just the supported install path and careful verification.&lt;/p&gt;

&lt;p&gt;Stevedore is designed for this exact setup: a single Docker-first control plane, a systemd service
when available, and a clear separation between the host and the container runtime. The goal is to
leave the machine in a stable, boring state and make future updates predictable.&lt;/p&gt;

&lt;h2 id=&quot;series-stevedore-on-a-raspberry-pi&quot;&gt;Series: Stevedore on a Raspberry Pi&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2025/12/24/introducing-stevedore/&quot;&gt;Stevedore: GitOps for your Raspberry Pi&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2025/12/24/stevedore-architecture/&quot;&gt;Under the Hood: How Stevedore Works&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2025/12/24/getting-started-with-stevedore/&quot;&gt;Tutorial: Deploying Your First App with Stevedore&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/01/production-raspberry-pi-deployment/&quot;&gt;Production Notes: Deploying Stevedore on a Raspberry Pi&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/02/stevedore-dyndns/&quot;&gt;Stevedore DynDNS: Dynamic DNS and HTTPS for Your Homelab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;guardrails&quot;&gt;Guardrails&lt;/h2&gt;

&lt;p&gt;Before touching production, I’m setting a few guardrails:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use the official installer (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;./stevedore-install.sh&lt;/code&gt;), not ad-hoc docker run commands.&lt;/li&gt;
  &lt;li&gt;Keep state under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/opt/stevedore&lt;/code&gt; (default layout).&lt;/li&gt;
  &lt;li&gt;Prefer systemd (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stevedore.service&lt;/code&gt;) to ensure the container survives reboots.&lt;/li&gt;
  &lt;li&gt;Never paste secrets into this post. Keys stay on the host, paths are fine to mention.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-success-story-in-short&quot;&gt;The Success Story (In Short)&lt;/h2&gt;

&lt;p&gt;This deployment did exactly what Stevedore promises. I installed it using the official script,
bootstrapped the self-managed deployment, and proved the entire lifecycle: keys, sync, self-update,
and stability over time. The host is now boring in the best way: Stevedore is running under systemd,
the daemon health checks match, and updates are a controlled, repeatable process.&lt;/p&gt;

&lt;h2 id=&quot;highlights&quot;&gt;Highlights&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The systemd unit keeps the control plane alive across reboots with zero manual babysitting.&lt;/li&gt;
  &lt;li&gt;The self-deployment syncs via a read-only GitHub deploy key when the repo uses SSH (public HTTPS clones still work without it), and I can rotate it manually.&lt;/li&gt;
  &lt;li&gt;Self-update works end-to-end when I trigger &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stevedore self-update&lt;/code&gt;: it pulled new commits, built a new image, and swapped cleanly.&lt;/li&gt;
  &lt;li&gt;The five-hour stability check shows the daemon healthy and the control plane steady.&lt;/li&gt;
  &lt;li&gt;Docs and onboarding guidance were improved based on real production friction.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-i-learned-and-turned-into-improvements&quot;&gt;What I Learned (And Turned Into Improvements)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;GitHub deploy keys are picky: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;read_only=true&lt;/code&gt; needs &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-F&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gh api&lt;/code&gt;, so I documented the exact
command and wired it into the CLI guidance.&lt;/li&gt;
  &lt;li&gt;Self-update builds run in the foreground; once the update worker is spawned, the container swap
finishes server-side, and the backup image gives a rollback path.&lt;/li&gt;
  &lt;li&gt;The systemd-managed control plane and a self-deployment compose project can collide on container
naming if the compose file sets &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;container_name: stevedore&lt;/code&gt;. That’s a good reminder that “boring”
operations need crisp boundaries.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;outcome&quot;&gt;Outcome&lt;/h2&gt;

&lt;p&gt;Stevedore is now running in production on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rp16g&lt;/code&gt;, updating itself and staying healthy without
manual intervention. The deployment is stable, the operational checks are clean, and the process is
documented end-to-end. This is exactly the boring, reliable, Git-driven workflow I wanted.&lt;/p&gt;

&lt;p&gt;Next time, I want to tighten the self-deployment status story (so it reports cleanly even with the
systemd-managed control plane) and keep iterating on the onboarding docs as more users go through
this path.&lt;/p&gt;

&lt;h2 id=&quot;agentic-finale&quot;&gt;Agentic Finale&lt;/h2&gt;

&lt;p&gt;The funny fact – I created stevedore to simplify my human actions to deploy services. 
It appeared that Claude Code and Codex are much better in dealing with deployments that I,
basically, I unstructured them to use deliver the changes over SSH. Both Agents were great
to figure out how to bind the deploy keys, use stevedore to manage repositories, run updates.&lt;/p&gt;

&lt;iframe src=&quot;https://www.linkedin.com/embed/feed/update/urn:li:share:7416481230835245057&quot; height=&quot;705&quot; width=&quot;504&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; title=&quot;Embedded post&quot;&gt;&lt;/iframe&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="stevedore" />
  
    <category term="raspberry-pi" />
  
    <category term="production" />
  
    <category term="systemd" />
  
    <category term="gitops" />
  
    <category term="self-hosting" />
  
    <summary type="html">Today is the day. I’m putting Stevedore into production on my Raspberry Pi host (rp16g). This is the real deal: no hacks, no shortcuts, just the supported install path and careful verification.</summary>
  
  </entry>
  
  <entry>
    <title type="html">Under the Hood: How Stevedore Works</title>
    <link href="https://jonnyzzz.com/blog/2025/12/24/stevedore-architecture/" rel="alternate" type="text/html" title="Under the Hood: How Stevedore Works" />
    <published>2025-12-24T00:00:00+00:00</published>
    <updated>2025-12-24T00:00:00+00:00</updated>
    <id>/blog/2025/12/24/stevedore-architecture</id>
    <content type="html" xml:base="https://jonnyzzz.com/blog/2025/12/24/stevedore-architecture/">&lt;p&gt;In my &lt;a href=&quot;/blog/2025/12/24/introducing-stevedore/&quot;&gt;previous post&lt;/a&gt;, I introduced Stevedore, my lightweight alternative to Kubernetes for small servers. Today, I want to dive into the architecture. How do we build a robust deployment system in Go without reinventing the wheel?&lt;/p&gt;

&lt;h2 id=&quot;series-stevedore-on-a-raspberry-pi&quot;&gt;Series: Stevedore on a Raspberry Pi&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2025/12/24/introducing-stevedore/&quot;&gt;Stevedore: GitOps for your Raspberry Pi&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2025/12/24/stevedore-architecture/&quot;&gt;Under the Hood: How Stevedore Works&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2025/12/24/getting-started-with-stevedore/&quot;&gt;Tutorial: Deploying Your First App with Stevedore&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/01/production-raspberry-pi-deployment/&quot;&gt;Production Notes: Deploying Stevedore on a Raspberry Pi&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2026/01/02/stevedore-dyndns/&quot;&gt;Stevedore DynDNS: Dynamic DNS and HTTPS for Your Homelab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-loop&quot;&gt;The Loop&lt;/h2&gt;

&lt;p&gt;At its heart, Stevedore is a polling loop. It checks your Git repositories on a configurable schedule (per deployment). But we don’t just run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git pull&lt;/code&gt; on the host. That’s messy.&lt;/p&gt;

&lt;h3 id=&quot;worker-containers&quot;&gt;Worker Containers&lt;/h3&gt;

&lt;p&gt;We follow the “Docker-first” philosophy. The Stevedore daemon itself runs as a single container. When it needs to perform complex state changes or isolated tasks, it spawns a &lt;strong&gt;Worker Container&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;For example, when Stevedore updates itself, it spawns an “Update Worker” that stops the old daemon and starts the new one. It’s like a brain transplant, performed by a robot arm.&lt;/p&gt;

&lt;p&gt;There is a git-worker implementation that runs operations in ephemeral &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alpine/git&lt;/code&gt; containers, but the daemon’s polling loop uses local &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GitCheckRemote&lt;/code&gt;/&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GitSyncClean&lt;/code&gt;) rather than routing syncs through the worker container.&lt;/p&gt;

&lt;h2 id=&quot;state-on-disk&quot;&gt;State on Disk&lt;/h2&gt;

&lt;p&gt;I am tired of distributed key-value stores. Stevedore runs on &lt;em&gt;one&lt;/em&gt; node. We don’t need etcd.&lt;/p&gt;

&lt;p&gt;By default, persistent state lives in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/opt/stevedore&lt;/code&gt; (shared data is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/opt/stevedore/shared&lt;/code&gt;; the query socket defaults to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/var/run/stevedore/query.sock&lt;/code&gt;).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/deployments&lt;/code&gt;: per-deployment repo checkout + SSH keys, parameters/runtime state, plus &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data/&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;logs/&lt;/code&gt; directories.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/system&lt;/code&gt;: The internal database.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We use &lt;strong&gt;SQLite&lt;/strong&gt;. It’s rock solid. But storing secrets (like API keys) in plain text is a bad idea, even on a private server. So we use &lt;strong&gt;SQLCipher&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;When you install Stevedore, it generates a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;db.key&lt;/code&gt; file. This key encrypts the SQLite database. Your secrets are safe at rest. It’s a simple, pragmatic trade-off. We lose high availability (if the disk dies, we die), but we gain immense simplicity.&lt;/p&gt;

&lt;h2 id=&quot;docker-compose-as-the-spec&quot;&gt;Docker Compose as the Spec&lt;/h2&gt;

&lt;p&gt;I didn’t want to create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stevedore.yaml&lt;/code&gt; format. Docker Compose is already the industry standard for defining multi-container applications.&lt;/p&gt;

&lt;p&gt;Stevedore looks for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose.yaml&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compose.yaml&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compose.yml&lt;/code&gt;, or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stevedore.yaml&lt;/code&gt; at the repo root. It respects standard features like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;healthcheck&lt;/code&gt; for status reporting. Automatic rollback on unhealthy containers is planned but not implemented yet.&lt;/p&gt;

&lt;p&gt;We inject &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;STEVEDORE_DEPLOYMENT&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;STEVEDORE_DATA&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;STEVEDORE_LOGS&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;STEVEDORE_SHARED&lt;/code&gt; into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker compose&lt;/code&gt; environment (along with any DB parameters). These are host paths you can mount in your compose file to make data/logs/shared storage persistent across redeploys.&lt;/p&gt;

&lt;h2 id=&quot;go--docker-cli-for-now&quot;&gt;Go + Docker CLI (for now)&lt;/h2&gt;

&lt;p&gt;The code is written in Go. It’s robust, statically typed, and has excellent libraries. For now Stevedore shells out to the Docker CLI (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker compose&lt;/code&gt;) instead of using the Docker SDK directly. It’s simple and predictable across hosts. If we need tighter control later, we can move to the SDK.&lt;/p&gt;

&lt;p&gt;Check out &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;internal/stevedore/git_worker.go&lt;/code&gt; to see how we manage the ephemeral containers. It’s a fun read if you’re into system programming!&lt;/p&gt;</content>

  
  
  
  
  

    <author>
      <name>Eugene Petrenko</name>
    </author>

  
    <category term="stevedore" />
  
    <category term="architecture" />
  
    <category term="gitops" />
  
    <category term="docker" />
  
    <category term="sqlite" />
  
    <category term="golang" />
  
    <summary type="html">In my previous post, I introduced Stevedore, my lightweight alternative to Kubernetes for small servers. Today, I want to dive into the architecture. How do we build a robust deployment system in Go without reinventing the wheel?</summary>
  
  </entry>
  
</feed>
